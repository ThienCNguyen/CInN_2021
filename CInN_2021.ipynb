{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CInN_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare environment"
      ],
      "metadata": {
        "id": "gRxVBvm6WaNW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKoaIEMk15MZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ba5579-2d71-4ec9-e57d-68da9675062f"
      },
      "source": [
        "!time pip install fairseq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 7.6MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.6)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.21)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Collecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.1.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=2491cba9a33908c899dfb1d064d391f09b9ccbcb200bde5d0966811f780fac24\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, sacrebleu, PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-0.10.2 hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.2.1 sacrebleu-1.5.0\n",
            "\n",
            "real\t0m7.094s\n",
            "user\t0m5.816s\n",
            "sys\t0m0.876s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "ROOTDIR='/content/drive/MyDrive/nmt_models/CInN_2021'\n",
        "os.chdir(ROOTDIR)"
      ],
      "metadata": {
        "id": "fdOmRXLAdwOw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vvdCqGB5HdE4",
        "outputId": "82b535a1-81da-4b43-f3e8-c949f06a8149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/nmt_models/CInN_2021'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment translation unit systems"
      ],
      "metadata": {
        "id": "Q898vMm-YGky"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGceEjID1-w7"
      },
      "source": [
        "DIR=os.path.join(ROOTDIR,'wovi2suben')\n",
        "if not os.path.exists(DIR):\n",
        "    os.mkdir(DIR)\n",
        "DATDIR=os.path.join(ROOTDIR,'datasets')\n",
        "SRC='wovi'\n",
        "TRG='suben'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKVDzRwD5EmG"
      },
      "source": [
        "!time fairseq-preprocess --source-lang $SRC --target-lang $TRG \\\n",
        "    --trainpref $DATDIR/train --validpref $DATDIR/dev --testpref $DATDIR/test \\\n",
        "    --joined-dictionary \\\n",
        "    --destdir $DIR/binarized\n",
        "!time fairseq-train \\\n",
        "    $DIR/binarized \\\n",
        "    --arch transformer --share-all-embeddings \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n",
        "    --lr 0.0002 --lr-scheduler inverse_sqrt \\\n",
        "    --dropout 0.3 \\\n",
        "    --max-tokens 3200 --label-smoothing 0.1 \\\n",
        "    --save-dir $DIR/checkpoints --log-interval 1000 --max-update 10000 \\\n",
        "    --keep-interval-updates -1 --save-interval-updates 0 \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --fp16\n",
        "!time fairseq-generate \\\n",
        "    $DIR/binarized --gen-subset test \\\n",
        "    --source-lang $SRC --target-lang $TRG \\\n",
        "    --path $DIR/checkpoints/checkpoint_best.pt --beam 5 --nbest 1 \\\n",
        "    --fp16 > $DIR/result.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r3WmsAc7I6Y"
      },
      "source": [
        "def extract(result, predict,start=9):\n",
        "    result = open(result, 'r').read().splitlines()\n",
        "    predict = open(predict, 'w')\n",
        "    select = result[start:-3:5]\n",
        "    select = [s.split('\\t') for s in select]\n",
        "    select = [(int(s[0].split('-')[-1]), s[-1]) for s in select]\n",
        "    select = sorted(select)\n",
        "    for s in select:\n",
        "        predict.write(s[-1] + '\\n')\n",
        "    predict.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHl5_rSO7fuh"
      },
      "source": [
        "def join_pieces(filename, new_filename, type='bpe'):\n",
        "    \"\"\"\n",
        "    function joins pieces to form words\n",
        "    input: pred_trgs, list of list of pieces\n",
        "    input: new_filename, str\n",
        "\n",
        "    \"\"\"\n",
        "    pred_trgs = [line.split(' ') for line in open(filename, 'r').read().splitlines()]\n",
        "    lines = [' '.join(line).replace('@@ ','') for line in pred_trgs]\n",
        "    open(new_filename, 'w').write('\\n'.join(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMyBezqxi74Z"
      },
      "source": [
        "#!cat $DIR/predict.wovi | sed 's/_/ /g' > $DIR/predict.vi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUNPbm4APGVm"
      },
      "source": [
        "!chmod +x /content/drive/MyDrive/nmt_models/CInN_2021/mosesdecoder/scripts/generic/multi-bleu.perl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twky3Ea2isG5"
      },
      "source": [
        "!/content/drive/MyDrive/nmt_models/CInN_2021/mosesdecoder/scripts/generic/multi-bleu.perl -lc $DATDIR/test.$TRGX < $DIRT/predict.$TRGX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha11XJ_OjGwf",
        "outputId": "0ddbbbf1-ef84-4561-c2eb-a81b1b628fd8"
      },
      "source": [
        "import os\n",
        "\n",
        "ROOTDIR='/content/drive/MyDrive/nmt_models/CInN_2021'\n",
        "DATDIR='/content/drive/MyDrive/nmt_models/CInN_2021/datasets'\n",
        "\n",
        "comb = []\n",
        "for src in ['vsub']:\n",
        "    for trg in ['en', 'suben']:\n",
        "        comb.append(f'{src}2{trg}')\n",
        "        comb.append(f'{trg}2{src}')\n",
        "\n",
        "for com in comb:\n",
        "    DIRT = os.path.join(ROOTDIR, com)\n",
        "    print(DIRT)\n",
        "    if 'predict.en' in os.listdir(DIRT):\n",
        "        TRGX = 'en'\n",
        "    else:\n",
        "        TRGX = 'vi'\n",
        "    !/content/drive/MyDrive/nmt_models/CInN_2021/mosesdecoder/scripts/generic/multi-bleu.perl -lc $DATDIR/test.$TRGX < $DIRT/predict.$TRGX\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en\n",
            "BLEU = 13.77, 46.0/18.8/9.0/4.6 (BP=1.000, ratio=1.063, hyp_len=33501, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub\n",
            "BLEU = 14.32, 45.4/20.7/10.3/5.4 (BP=0.948, ratio=0.950, hyp_len=41105, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben\n",
            "BLEU = 13.38, 47.3/18.7/8.7/4.2 (BP=1.000, ratio=1.004, hyp_len=31628, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub\n",
            "BLEU = 12.90, 45.6/20.0/9.6/4.8 (BP=0.900, ratio=0.905, hyp_len=39161, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr-Ry-3Jk_lU"
      },
      "source": [
        "\n",
        "Vi-En w2w: BLEU = 13.62, 45.6/18.8/9.0/4.5 (BP=1.000, ratio=1.100, hyp_len=34664, ref_len=31513)\n",
        "\n",
        "Vi-En c2w: BLEU = 14.40, 48.2/19.9/9.6/4.8 (BP=0.992, ratio=0.992, hyp_len=31252, ref_len=31513)\n",
        "\n",
        "Vi-En s2s: BLEU = 13.42, 48.9/19.4/9.1/4.4 (BP=0.959, ratio=0.960, hyp_len=30251, ref_len=31513)\n",
        "\n",
        "En-Vi w2c: BLEU = 13.38, 44.7/19.9/9.6/4.9 (BP=0.938, ratio=0.940, hyp_len=40683, ref_len=43286)\n",
        "\n",
        "En-Vi l2c: BLEU = 14.62, 46.1/21.1/10.3/5.3 (BP=0.964, ratio=0.964, hyp_len=41740, ref_len=43286)\n",
        "\n",
        "En-Vi w2w: BLEU = 20.98, 51.1/27.1/15.3/9.1 (BP=1.000, ratio=1.001, hyp_len=43327, ref_len=43286)\n",
        "\n",
        "En-Vi s2s: BLEU = 19.68, 53.8/28.4/15.8/9.2 (BP=0.907, ratio=0.911, hyp_len=39426, ref_len=43286)\n",
        "\n",
        "En-Vi l2w: BLEU = 23.07, 55.0/30.5/17.7/10.8 (BP=0.969, ratio=0.969, hyp_len=41965, ref_len=43286)\n",
        "\n",
        "suben-wovi: BLEU = 23.07, 55.0/30.5/17.7/10.8 (BP=0.969, ratio=0.969, hyp_len=41965, ref_len=43286)\n",
        "\n",
        "wovi2suben: BLEU = 11.54, 45.4/16.6/7.3/3.5 (BP=0.982, ratio=0.982, hyp_len=30957, ref_len=31513)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kRSOCUwAFkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362296a0-3ea7-43ef-9eb6-2f11dfe856bc"
      },
      "source": [
        "import os\n",
        "\n",
        "ROOTDIR='/content/drive/MyDrive/nmt_models/CInN_2021'\n",
        "DATDIR='/content/drive/MyDrive/nmt_models/CInN_2021/datasets'\n",
        "\n",
        "comb = []\n",
        "for src in ['vsub']:\n",
        "    for trg in ['en', 'suben']:\n",
        "        comb.append(f'{src}2{trg}')\n",
        "        comb.append(f'{trg}2{src}')\n",
        "\n",
        "for com in comb:\n",
        "    DIRT = os.path.join(ROOTDIR, com)\n",
        "    print(DIRT)\n",
        "    if os.path.exists(DIRT):\n",
        "        print(f'{src}2{trg}')\n",
        "        #!/content/drive/MyDrive/nmt_models/CInN_2021/mosesdecoder/scripts/generic/multi-bleu.perl -lc $DATDIR/test.en < $DIRT/predict.en\n",
        "    else:\n",
        "        SRC, TRG = com.split('2')\n",
        "        DIR = DIRT\n",
        "        os.mkdir(DIR)\n",
        "        !time fairseq-preprocess --source-lang $SRC --target-lang $TRG \\\n",
        "            --trainpref $DATDIR/train --validpref $DATDIR/dev --testpref $DATDIR/test \\\n",
        "            --joined-dictionary \\\n",
        "            --destdir $DIR/binarized\n",
        "        !time fairseq-train \\\n",
        "            $DIR/binarized \\\n",
        "            --arch transformer --share-all-embeddings \\\n",
        "            --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \\\n",
        "            --lr 0.0002 --lr-scheduler inverse_sqrt \\\n",
        "            --dropout 0.3 \\\n",
        "            --max-tokens 3200 --label-smoothing 0.1 \\\n",
        "            --save-dir $DIR/checkpoints --log-interval 1000 --max-update 10000 \\\n",
        "            --keep-interval-updates -1 --save-interval-updates 0 \\\n",
        "            --criterion label_smoothed_cross_entropy \\\n",
        "            --fp16\n",
        "        !time fairseq-generate \\\n",
        "            $DIR/binarized --gen-subset test \\\n",
        "            --source-lang $SRC --target-lang $TRG \\\n",
        "            --path $DIR/checkpoints/checkpoint_best.pt --beam 5 --nbest 1 \\\n",
        "            --fp16 > $DIR/result.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en\n",
            "2021-02-24 08:53:48 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='vsub', srcdict=None, target_lang='en', task='translation', tensorboard_logdir=None, testpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train', user_dir=None, validpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev', workers=1)\n",
            "2021-02-24 08:53:56 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 08:54:06 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.vsub: 42026 sents, 1164786 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 08:54:06 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 08:54:07 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.vsub: 1482 sents, 39469 tokens, 0.0355% replaced by <unk>\n",
            "2021-02-24 08:54:07 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 08:54:07 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.vsub: 1527 sents, 47028 tokens, 0.0702% replaced by <unk>\n",
            "2021-02-24 08:54:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 08:54:15 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.en: 42026 sents, 848482 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 08:54:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 08:54:16 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.en: 1482 sents, 27797 tokens, 3.53% replaced by <unk>\n",
            "2021-02-24 08:54:16 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 08:54:17 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.en: 1527 sents, 33040 tokens, 3.62% replaced by <unk>\n",
            "2021-02-24 08:54:17 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized\n",
            "\n",
            "real\t0m34.162s\n",
            "user\t0m27.106s\n",
            "sys\t0m0.494s\n",
            "2021-02-24 08:54:19 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n",
            "2021-02-24 08:54:19 | INFO | fairseq.tasks.translation | [vsub] dictionary: 43624 types\n",
            "2021-02-24 08:54:19 | INFO | fairseq.tasks.translation | [en] dictionary: 43624 types\n",
            "2021-02-24 08:54:19 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized/valid.vsub-en.vsub\n",
            "2021-02-24 08:54:19 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized/valid.vsub-en.en\n",
            "2021-02-24 08:54:19 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized valid vsub-en 1482 examples\n",
            "2021-02-24 08:54:20 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(43624, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(43624, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=43624, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-02-24 08:54:21 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-02-24 08:54:21 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
            "2021-02-24 08:54:21 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
            "2021-02-24 08:54:21 | INFO | fairseq_cli.train | num. model params: 66473984 (num. trained: 66473984)\n",
            "2021-02-24 08:54:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-02-24 08:54:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-02-24 08:54:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 08:54:30 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-02-24 08:54:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 08:54:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-02-24 08:54:30 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n",
            "2021-02-24 08:54:30 | INFO | fairseq.trainer | no existing checkpoint found /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint_last.pt\n",
            "2021-02-24 08:54:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-02-24 08:54:30 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized/train.vsub-en.vsub\n",
            "2021-02-24 08:54:30 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized/train.vsub-en.en\n",
            "2021-02-24 08:54:30 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/binarized train vsub-en 42026 examples\n",
            "2021-02-24 08:54:30 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "epoch 001:   0% 0/395 [00:00<?, ?it/s]2021-02-24 08:54:30 | INFO | fairseq.trainer | begin training epoch 1\n",
            "epoch 001:   8% 32/395 [00:07<01:22,  4.38it/s]2021-02-24 08:54:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
            "epoch 001: 100% 394/395 [01:28<00:00,  4.51it/s]2021-02-24 08:55:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 11.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.94it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 08:56:01 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.467 | nll_loss 10.893 | ppl 1901.17 | wps 24313.9 | wpb 1853.1 | bsz 98.8 | num_updates 394\n",
            "2021-02-24 08:56:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 08:56:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint1.pt (epoch 1 @ 394 updates, score 11.467) (writing took 48.34243182299997 seconds)\n",
            "2021-02-24 08:56:49 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-02-24 08:56:49 | INFO | train | epoch 001 | loss 13.295 | nll_loss 12.969 | ppl 8015.35 | wps 6112.1 | ups 2.85 | wpb 2148.1 | bsz 106.1 | num_updates 394 | lr 1.97e-05 | gnorm 3.095 | loss_scale 64 | train_wall 88 | wall 139\n",
            "epoch 002:   0% 0/395 [00:00<?, ?it/s]2021-02-24 08:56:49 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  89% 353/395 [01:22<00:09,  4.65it/s]2021-02-24 08:58:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
            "epoch 002: 100% 394/395 [01:32<00:00,  4.56it/s]2021-02-24 08:58:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 08:58:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.521 | nll_loss 9.693 | ppl 827.47 | wps 24076.9 | wpb 1853.1 | bsz 98.8 | num_updates 788 | best_loss 10.521\n",
            "2021-02-24 08:58:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 08:59:11 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint2.pt (epoch 2 @ 788 updates, score 10.521) (writing took 48.77259502900006 seconds)\n",
            "2021-02-24 08:59:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-02-24 08:59:11 | INFO | train | epoch 002 | loss 10.805 | nll_loss 10.107 | ppl 1102.56 | wps 5944.1 | ups 2.76 | wpb 2150.1 | bsz 105.3 | num_updates 788 | lr 3.94e-05 | gnorm 1.975 | loss_scale 32 | train_wall 91 | wall 281\n",
            "epoch 003:   0% 0/395 [00:00<?, ?it/s]2021-02-24 08:59:12 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 394/395 [01:31<00:00,  4.73it/s, loss=11.696, nll_loss=11.12, ppl=2226.34, wps=6470, ups=3.02, wpb=2146, bsz=105.3, num_updates=1000, lr=5e-05, gnorm=2.401, loss_scale=32, train_wall=228, wall=332]2021-02-24 09:00:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   7% 1/15 [00:00<00:01,  7.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  8.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 11.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 13.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:00:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.068 | nll_loss 9.163 | ppl 573.18 | wps 24274.6 | wpb 1853.1 | bsz 98.8 | num_updates 1183 | best_loss 10.068\n",
            "2021-02-24 09:00:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:01:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint3.pt (epoch 3 @ 1183 updates, score 10.068) (writing took 48.396619913999984 seconds)\n",
            "2021-02-24 09:01:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-02-24 09:01:33 | INFO | train | epoch 003 | loss 10.269 | nll_loss 9.439 | ppl 694.32 | wps 5981.1 | ups 2.78 | wpb 2148.1 | bsz 106.4 | num_updates 1183 | lr 5.915e-05 | gnorm 1.952 | loss_scale 32 | train_wall 91 | wall 423\n",
            "epoch 004:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:01:33 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 394/395 [01:31<00:00,  4.47it/s]2021-02-24 09:03:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 2/15 [00:00<00:01,  7.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  8.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 6/15 [00:00<00:00,  9.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 8/15 [00:00<00:00, 10.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 10.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 12/15 [00:01<00:00, 11.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 14/15 [00:01<00:00, 12.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:03:06 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.766 | nll_loss 8.851 | ppl 461.78 | wps 23824.1 | wpb 1853.1 | bsz 98.8 | num_updates 1578 | best_loss 9.766\n",
            "2021-02-24 09:03:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:03:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint4.pt (epoch 4 @ 1578 updates, score 9.766) (writing took 49.36616009199997 seconds)\n",
            "2021-02-24 09:03:56 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-02-24 09:03:56 | INFO | train | epoch 004 | loss 9.862 | nll_loss 8.976 | ppl 503.61 | wps 5965.6 | ups 2.78 | wpb 2148.1 | bsz 106.4 | num_updates 1578 | lr 7.89e-05 | gnorm 1.956 | loss_scale 32 | train_wall 90 | wall 565\n",
            "epoch 005:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:03:56 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 394/395 [01:31<00:00,  4.38it/s]2021-02-24 09:05:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.39it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:05:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.539 | nll_loss 8.587 | ppl 384.67 | wps 24149.7 | wpb 1853.1 | bsz 98.8 | num_updates 1973 | best_loss 9.539\n",
            "2021-02-24 09:05:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:06:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint5.pt (epoch 5 @ 1973 updates, score 9.539) (writing took 47.68708732499999 seconds)\n",
            "2021-02-24 09:06:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-02-24 09:06:16 | INFO | train | epoch 005 | loss 9.471 | nll_loss 8.538 | ppl 371.73 | wps 6022.1 | ups 2.8 | wpb 2148.1 | bsz 106.4 | num_updates 1973 | lr 9.865e-05 | gnorm 1.904 | loss_scale 32 | train_wall 90 | wall 706\n",
            "epoch 006:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:06:17 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 394/395 [01:31<00:00,  4.44it/s, loss=9.743, nll_loss=8.844, ppl=459.49, wps=5637.2, ups=2.62, wpb=2150.6, bsz=107.2, num_updates=2000, lr=0.0001, gnorm=1.947, loss_scale=32, train_wall=228, wall=713]2021-02-24 09:07:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.27it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.11it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.75it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.70it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:07:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.128 | nll_loss 8.123 | ppl 278.87 | wps 23785 | wpb 1853.1 | bsz 98.8 | num_updates 2368 | best_loss 9.128\n",
            "2021-02-24 09:07:50 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:08:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint6.pt (epoch 6 @ 2368 updates, score 9.128) (writing took 48.16006771399998 seconds)\n",
            "2021-02-24 09:08:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-02-24 09:08:38 | INFO | train | epoch 006 | loss 9.113 | nll_loss 8.134 | ppl 280.89 | wps 5988.2 | ups 2.79 | wpb 2148.1 | bsz 106.4 | num_updates 2368 | lr 0.0001184 | gnorm 1.876 | loss_scale 32 | train_wall 91 | wall 848\n",
            "epoch 007:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:08:38 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 394/395 [01:31<00:00,  4.42it/s]2021-02-24 09:10:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.23it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.24it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.23it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.76it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.47it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:10:11 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.844 | nll_loss 7.802 | ppl 223.11 | wps 24194.9 | wpb 1853.1 | bsz 98.8 | num_updates 2763 | best_loss 8.844\n",
            "2021-02-24 09:10:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:11:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint7.pt (epoch 7 @ 2763 updates, score 8.844) (writing took 49.40892238200013 seconds)\n",
            "2021-02-24 09:11:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-02-24 09:11:01 | INFO | train | epoch 007 | loss 8.812 | nll_loss 7.791 | ppl 221.44 | wps 5950.3 | ups 2.77 | wpb 2148.1 | bsz 106.4 | num_updates 2763 | lr 0.00013815 | gnorm 1.804 | loss_scale 32 | train_wall 90 | wall 990\n",
            "epoch 008:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:11:01 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 394/395 [01:31<00:00,  4.51it/s, loss=8.85, nll_loss=7.835, ppl=228.31, wps=6444.4, ups=3, wpb=2148.2, bsz=106.6, num_updates=3000, lr=0.00015, gnorm=1.809, loss_scale=32, train_wall=229, wall=1047]2021-02-24 09:12:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.28it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.25it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.15it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.79it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.50it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:12:34 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.704 | nll_loss 7.634 | ppl 198.61 | wps 24104.3 | wpb 1853.1 | bsz 98.8 | num_updates 3158 | best_loss 8.704\n",
            "2021-02-24 09:12:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:13:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint8.pt (epoch 8 @ 3158 updates, score 8.704) (writing took 45.050478083000144 seconds)\n",
            "2021-02-24 09:13:19 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-02-24 09:13:19 | INFO | train | epoch 008 | loss 8.523 | nll_loss 7.462 | ppl 176.34 | wps 6127.6 | ups 2.85 | wpb 2148.1 | bsz 106.4 | num_updates 3158 | lr 0.0001579 | gnorm 1.76 | loss_scale 32 | train_wall 91 | wall 1129\n",
            "epoch 009:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:13:19 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 394/395 [01:31<00:00,  4.58it/s]2021-02-24 09:14:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.80it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.75it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.78it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.79it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.51it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.27it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:14:53 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.374 | nll_loss 7.248 | ppl 151.98 | wps 23896.6 | wpb 1853.1 | bsz 98.8 | num_updates 3553 | best_loss 8.374\n",
            "2021-02-24 09:14:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:15:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint9.pt (epoch 9 @ 3553 updates, score 8.374) (writing took 46.69796417099997 seconds)\n",
            "2021-02-24 09:15:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-02-24 09:15:39 | INFO | train | epoch 009 | loss 8.237 | nll_loss 7.136 | ppl 140.68 | wps 6061.7 | ups 2.82 | wpb 2148.1 | bsz 106.4 | num_updates 3553 | lr 0.00017765 | gnorm 1.769 | loss_scale 32 | train_wall 90 | wall 1269\n",
            "epoch 010:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:15:39 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 394/395 [01:31<00:00,  4.61it/s]2021-02-24 09:17:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.09it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  13% 2/15 [00:00<00:01,  6.82it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  7.85it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.84it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 8/15 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 10.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 12/15 [00:01<00:00, 11.23it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  93% 14/15 [00:01<00:00, 11.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:17:12 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.119 | nll_loss 6.938 | ppl 122.65 | wps 23759 | wpb 1853.1 | bsz 98.8 | num_updates 3948 | best_loss 8.119\n",
            "2021-02-24 09:17:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:18:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint10.pt (epoch 10 @ 3948 updates, score 8.119) (writing took 47.54581284599999 seconds)\n",
            "2021-02-24 09:18:00 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-02-24 09:18:00 | INFO | train | epoch 010 | loss 7.921 | nll_loss 6.776 | ppl 109.58 | wps 6038.1 | ups 2.81 | wpb 2148.1 | bsz 106.4 | num_updates 3948 | lr 0.0001974 | gnorm 1.779 | loss_scale 32 | train_wall 90 | wall 1409\n",
            "epoch 011:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:18:00 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 394/395 [01:31<00:00,  4.50it/s, loss=8.127, nll_loss=7.01, ppl=128.93, wps=5713.9, ups=2.66, wpb=2149, bsz=106.6, num_updates=4000, lr=0.0002, gnorm=1.785, loss_scale=32, train_wall=229, wall=1423]2021-02-24 09:19:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.07it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:19:33 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.804 | nll_loss 6.597 | ppl 96.81 | wps 23709.6 | wpb 1853.1 | bsz 98.8 | num_updates 4343 | best_loss 7.804\n",
            "2021-02-24 09:19:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:20:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint11.pt (epoch 11 @ 4343 updates, score 7.804) (writing took 42.59358155200016 seconds)\n",
            "2021-02-24 09:20:16 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2021-02-24 09:20:16 | INFO | train | epoch 011 | loss 7.576 | nll_loss 6.384 | ppl 83.51 | wps 6238.1 | ups 2.9 | wpb 2148.1 | bsz 106.4 | num_updates 4343 | lr 0.00019194 | gnorm 1.742 | loss_scale 32 | train_wall 90 | wall 1545\n",
            "epoch 012:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:20:16 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 394/395 [01:31<00:00,  4.64it/s]2021-02-24 09:21:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.73it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.69it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.74it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.77it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.44it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.51it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:21:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.666 | nll_loss 6.425 | ppl 85.9 | wps 23905.8 | wpb 1853.1 | bsz 98.8 | num_updates 4738 | best_loss 7.666\n",
            "2021-02-24 09:21:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:22:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint12.pt (epoch 12 @ 4738 updates, score 7.666) (writing took 47.264652733000275 seconds)\n",
            "2021-02-24 09:22:36 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2021-02-24 09:22:36 | INFO | train | epoch 012 | loss 7.248 | nll_loss 6.011 | ppl 64.49 | wps 6044.4 | ups 2.81 | wpb 2148.1 | bsz 106.4 | num_updates 4738 | lr 0.000183765 | gnorm 1.79 | loss_scale 32 | train_wall 90 | wall 1686\n",
            "epoch 013:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:22:36 | INFO | fairseq.trainer | begin training epoch 13\n",
            "epoch 013: 100% 394/395 [01:31<00:00,  4.72it/s, loss=7.284, nll_loss=6.052, ppl=66.34, wps=6612.5, ups=3.08, wpb=2145.5, bsz=105.5, num_updates=5000, lr=0.000178885, gnorm=1.762, loss_scale=32, train_wall=228, wall=1747]2021-02-24 09:24:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.74it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.70it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.65it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.72it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.48it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 11.07it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.73it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:24:09 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.422 | nll_loss 6.142 | ppl 70.62 | wps 24112.4 | wpb 1853.1 | bsz 98.8 | num_updates 5133 | best_loss 7.422\n",
            "2021-02-24 09:24:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:24:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint13.pt (epoch 13 @ 5133 updates, score 7.422) (writing took 48.837861342999986 seconds)\n",
            "2021-02-24 09:24:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2021-02-24 09:24:58 | INFO | train | epoch 013 | loss 6.962 | nll_loss 5.685 | ppl 51.44 | wps 5983.6 | ups 2.79 | wpb 2148.1 | bsz 106.4 | num_updates 5133 | lr 0.000176553 | gnorm 1.783 | loss_scale 32 | train_wall 90 | wall 1828\n",
            "epoch 014:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:24:58 | INFO | fairseq.trainer | begin training epoch 14\n",
            "epoch 014: 100% 394/395 [01:31<00:00,  4.60it/s]2021-02-24 09:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.58it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.37it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.36it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.44it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.20it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.84it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.52it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:26:31 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.294 | nll_loss 5.97 | ppl 62.68 | wps 23598.8 | wpb 1853.1 | bsz 98.8 | num_updates 5528 | best_loss 7.294\n",
            "2021-02-24 09:26:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:27:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint14.pt (epoch 14 @ 5528 updates, score 7.294) (writing took 46.84441469000012 seconds)\n",
            "2021-02-24 09:27:18 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2021-02-24 09:27:18 | INFO | train | epoch 014 | loss 6.683 | nll_loss 5.367 | ppl 41.27 | wps 6061.2 | ups 2.82 | wpb 2148.1 | bsz 106.4 | num_updates 5528 | lr 0.000170128 | gnorm 1.779 | loss_scale 32 | train_wall 90 | wall 1968\n",
            "epoch 015:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:27:18 | INFO | fairseq.trainer | begin training epoch 15\n",
            "epoch 015: 100% 394/395 [01:31<00:00,  4.20it/s]2021-02-24 09:28:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.46it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.51it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.48it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.59it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.35it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.10it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.39it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:28:52 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.135 | nll_loss 5.798 | ppl 55.65 | wps 24149 | wpb 1853.1 | bsz 98.8 | num_updates 5923 | best_loss 7.135\n",
            "2021-02-24 09:28:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:29:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint15.pt (epoch 15 @ 5923 updates, score 7.135) (writing took 44.362926805999905 seconds)\n",
            "2021-02-24 09:29:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2021-02-24 09:29:36 | INFO | train | epoch 015 | loss 6.453 | nll_loss 5.105 | ppl 34.4 | wps 6148.4 | ups 2.86 | wpb 2148.1 | bsz 106.4 | num_updates 5923 | lr 0.000164357 | gnorm 1.827 | loss_scale 32 | train_wall 91 | wall 2106\n",
            "epoch 016:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:29:36 | INFO | fairseq.trainer | begin training epoch 16\n",
            "epoch 016: 100% 394/395 [01:31<00:00,  4.67it/s, loss=6.591, nll_loss=5.263, ppl=38.39, wps=5694.9, ups=2.65, wpb=2148.8, bsz=106.4, num_updates=6000, lr=0.000163299, gnorm=1.8, loss_scale=32, train_wall=229, wall=2125]2021-02-24 09:31:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.16it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.10it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.04it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.17it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.05it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.63it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.34it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.56it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:31:09 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.003 | nll_loss 5.647 | ppl 50.1 | wps 23665.7 | wpb 1853.1 | bsz 98.8 | num_updates 6318 | best_loss 7.003\n",
            "2021-02-24 09:31:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:31:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint16.pt (epoch 16 @ 6318 updates, score 7.003) (writing took 48.46198944800017 seconds)\n",
            "2021-02-24 09:31:57 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2021-02-24 09:31:57 | INFO | train | epoch 016 | loss 6.216 | nll_loss 4.835 | ppl 28.53 | wps 5992.5 | ups 2.79 | wpb 2148.1 | bsz 106.4 | num_updates 6318 | lr 0.000159137 | gnorm 1.793 | loss_scale 32 | train_wall 90 | wall 2247\n",
            "epoch 017:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:31:58 | INFO | fairseq.trainer | begin training epoch 17\n",
            "epoch 017: 100% 394/395 [01:31<00:00,  4.41it/s]2021-02-24 09:33:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.39it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.38it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.53it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.27it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.05it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.38it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:33:31 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.96 | nll_loss 5.57 | ppl 47.52 | wps 23867.3 | wpb 1853.1 | bsz 98.8 | num_updates 6713 | best_loss 6.96\n",
            "2021-02-24 09:33:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:34:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint17.pt (epoch 17 @ 6713 updates, score 6.96) (writing took 47.33060042900024 seconds)\n",
            "2021-02-24 09:34:18 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2021-02-24 09:34:18 | INFO | train | epoch 017 | loss 6.013 | nll_loss 4.602 | ppl 24.28 | wps 6038.6 | ups 2.81 | wpb 2148.1 | bsz 106.4 | num_updates 6713 | lr 0.000154384 | gnorm 1.805 | loss_scale 32 | train_wall 90 | wall 2388\n",
            "epoch 018:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:34:18 | INFO | fairseq.trainer | begin training epoch 18\n",
            "epoch 018: 100% 394/395 [01:31<00:00,  4.48it/s, loss=6.02, nll_loss=4.609, ppl=24.41, wps=6508.6, ups=3.03, wpb=2150.9, bsz=107.1, num_updates=7000, lr=0.000151186, gnorm=1.812, loss_scale=32, train_wall=228, wall=2455]2021-02-24 09:35:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.55it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.52it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.45it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.42it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.19it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.79it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.51it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:35:51 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.853 | nll_loss 5.435 | ppl 43.27 | wps 23766.2 | wpb 1853.1 | bsz 98.8 | num_updates 7108 | best_loss 6.853\n",
            "2021-02-24 09:35:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:36:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint18.pt (epoch 18 @ 7108 updates, score 6.853) (writing took 48.40951694100022 seconds)\n",
            "2021-02-24 09:36:40 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2021-02-24 09:36:40 | INFO | train | epoch 018 | loss 5.828 | nll_loss 4.389 | ppl 20.96 | wps 5991.4 | ups 2.79 | wpb 2148.1 | bsz 106.4 | num_updates 7108 | lr 0.000150033 | gnorm 1.845 | loss_scale 32 | train_wall 90 | wall 2529\n",
            "epoch 019:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:36:40 | INFO | fairseq.trainer | begin training epoch 19\n",
            "epoch 019: 100% 394/395 [01:31<00:00,  4.57it/s]2021-02-24 09:38:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.45it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.45it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.45it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.54it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.31it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.87it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.56it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:38:13 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.801 | nll_loss 5.358 | ppl 41 | wps 24115 | wpb 1853.1 | bsz 98.8 | num_updates 7503 | best_loss 6.801\n",
            "2021-02-24 09:38:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:38:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint19.pt (epoch 19 @ 7503 updates, score 6.801) (writing took 42.9947986279999 seconds)\n",
            "2021-02-24 09:38:56 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2021-02-24 09:38:56 | INFO | train | epoch 019 | loss 5.652 | nll_loss 4.187 | ppl 18.21 | wps 6240.7 | ups 2.91 | wpb 2148.1 | bsz 106.4 | num_updates 7503 | lr 0.00014603 | gnorm 1.815 | loss_scale 32 | train_wall 90 | wall 2665\n",
            "epoch 020:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:38:56 | INFO | fairseq.trainer | begin training epoch 20\n",
            "epoch 020: 100% 394/395 [01:31<00:00,  4.56it/s]2021-02-24 09:40:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.16it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.11it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.10it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.20it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.05it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.65it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.38it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:40:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.714 | nll_loss 5.257 | ppl 38.25 | wps 23807.3 | wpb 1853.1 | bsz 98.8 | num_updates 7898 | best_loss 6.714\n",
            "2021-02-24 09:40:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:41:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint20.pt (epoch 20 @ 7898 updates, score 6.714) (writing took 46.583679864999795 seconds)\n",
            "2021-02-24 09:41:15 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2021-02-24 09:41:15 | INFO | train | epoch 020 | loss 5.499 | nll_loss 4.01 | ppl 16.12 | wps 6064.7 | ups 2.82 | wpb 2148.1 | bsz 106.4 | num_updates 7898 | lr 0.000142332 | gnorm 1.841 | loss_scale 32 | train_wall 90 | wall 2805\n",
            "epoch 021:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:41:16 | INFO | fairseq.trainer | begin training epoch 21\n",
            "epoch 021: 100% 394/395 [01:31<00:00,  4.39it/s, loss=5.579, nll_loss=4.103, ppl=17.19, wps=5725.5, ups=2.67, wpb=2148.3, bsz=106, num_updates=8000, lr=0.000141421, gnorm=1.83, loss_scale=32, train_wall=229, wall=2830]2021-02-24 09:42:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.41it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.44it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.52it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.64it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.35it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.13it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.44it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:42:49 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.64 | nll_loss 5.191 | ppl 36.54 | wps 24237.8 | wpb 1853.1 | bsz 98.8 | num_updates 8293 | best_loss 6.64\n",
            "2021-02-24 09:42:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:43:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint21.pt (epoch 21 @ 8293 updates, score 6.64) (writing took 47.93604250599992 seconds)\n",
            "2021-02-24 09:43:37 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2021-02-24 09:43:37 | INFO | train | epoch 021 | loss 5.347 | nll_loss 3.836 | ppl 14.28 | wps 6011.6 | ups 2.8 | wpb 2148.1 | bsz 106.4 | num_updates 8293 | lr 0.000138901 | gnorm 1.807 | loss_scale 32 | train_wall 90 | wall 2946\n",
            "epoch 022:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:43:37 | INFO | fairseq.trainer | begin training epoch 22\n",
            "epoch 022: 100% 394/395 [01:31<00:00,  4.56it/s]2021-02-24 09:45:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.64it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.69it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.74it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.44it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.21it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.53it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:45:09 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.639 | nll_loss 5.173 | ppl 36.08 | wps 24055.4 | wpb 1853.1 | bsz 98.8 | num_updates 8688 | best_loss 6.639\n",
            "2021-02-24 09:45:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:45:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint22.pt (epoch 22 @ 8688 updates, score 6.639) (writing took 46.67813714100066 seconds)\n",
            "2021-02-24 09:45:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2021-02-24 09:45:56 | INFO | train | epoch 022 | loss 5.21 | nll_loss 3.678 | ppl 12.8 | wps 6080.1 | ups 2.83 | wpb 2148.1 | bsz 106.4 | num_updates 8688 | lr 0.000135706 | gnorm 1.802 | loss_scale 32 | train_wall 90 | wall 3086\n",
            "epoch 023:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:45:56 | INFO | fairseq.trainer | begin training epoch 23\n",
            "epoch 023: 100% 394/395 [01:31<00:00,  4.66it/s, loss=5.213, nll_loss=3.681, ppl=12.82, wps=6525.3, ups=3.04, wpb=2145.6, bsz=106.6, num_updates=9000, lr=0.000133333, gnorm=1.812, loss_scale=32, train_wall=228, wall=3159]2021-02-24 09:47:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.34it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  13% 2/15 [00:00<00:01,  6.99it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  7.97it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  40% 6/15 [00:00<00:01,  8.96it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  53% 8/15 [00:00<00:00,  9.93it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 10.61it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  80% 12/15 [00:01<00:00, 11.27it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  93% 14/15 [00:01<00:00, 12.02it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:47:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.615 | nll_loss 5.17 | ppl 36.01 | wps 23613.5 | wpb 1853.1 | bsz 98.8 | num_updates 9083 | best_loss 6.615\n",
            "2021-02-24 09:47:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:48:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint23.pt (epoch 23 @ 9083 updates, score 6.615) (writing took 49.53077590800058 seconds)\n",
            "2021-02-24 09:48:19 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2021-02-24 09:48:19 | INFO | train | epoch 023 | loss 5.092 | nll_loss 3.542 | ppl 11.65 | wps 5939.1 | ups 2.76 | wpb 2148.1 | bsz 106.4 | num_updates 9083 | lr 0.000132723 | gnorm 1.845 | loss_scale 32 | train_wall 91 | wall 3229\n",
            "epoch 024:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:48:19 | INFO | fairseq.trainer | begin training epoch 24\n",
            "epoch 024: 100% 394/395 [01:31<00:00,  4.65it/s]2021-02-24 09:49:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.86it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.76it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  27% 4/15 [00:00<00:01,  8.31it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  40% 6/15 [00:00<00:00,  9.25it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  53% 8/15 [00:00<00:00, 10.20it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  67% 10/15 [00:00<00:00, 10.86it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  80% 12/15 [00:01<00:00, 11.51it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  93% 14/15 [00:01<00:00, 12.14it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:49:52 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.568 | nll_loss 5.096 | ppl 34.2 | wps 23538.2 | wpb 1853.1 | bsz 98.8 | num_updates 9478 | best_loss 6.568\n",
            "2021-02-24 09:49:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:50:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint24.pt (epoch 24 @ 9478 updates, score 6.568) (writing took 42.67421163800009 seconds)\n",
            "2021-02-24 09:50:35 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2021-02-24 09:50:35 | INFO | train | epoch 024 | loss 4.97 | nll_loss 3.401 | ppl 10.56 | wps 6234.7 | ups 2.9 | wpb 2148.1 | bsz 106.4 | num_updates 9478 | lr 0.000129928 | gnorm 1.81 | loss_scale 32 | train_wall 91 | wall 3365\n",
            "epoch 025:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:50:35 | INFO | fairseq.trainer | begin training epoch 25\n",
            "epoch 025: 100% 394/395 [01:32<00:00,  4.41it/s]2021-02-24 09:52:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  5.24it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  6.24it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  7.27it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  60% 9/15 [00:00<00:00,  9.43it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 10.19it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 10.98it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.33it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:52:09 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.551 | nll_loss 5.065 | ppl 33.47 | wps 24013.4 | wpb 1853.1 | bsz 98.8 | num_updates 9873 | best_loss 6.551\n",
            "2021-02-24 09:52:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:52:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint25.pt (epoch 25 @ 9873 updates, score 6.551) (writing took 44.00103767900055 seconds)\n",
            "2021-02-24 09:52:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2021-02-24 09:52:53 | INFO | train | epoch 025 | loss 4.869 | nll_loss 3.283 | ppl 9.73 | wps 6148.9 | ups 2.86 | wpb 2148.1 | bsz 106.4 | num_updates 9873 | lr 0.000127302 | gnorm 1.841 | loss_scale 32 | train_wall 91 | wall 3503\n",
            "epoch 026:   0% 0/395 [00:00<?, ?it/s]2021-02-24 09:52:53 | INFO | fairseq.trainer | begin training epoch 26\n",
            "epoch 026:  32% 126/395 [00:30<01:02,  4.28it/s]2021-02-24 09:53:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:   7% 1/15 [00:00<00:02,  6.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  20% 3/15 [00:00<00:01,  7.69it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  33% 5/15 [00:00<00:01,  8.68it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  47% 7/15 [00:00<00:00,  9.73it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  60% 9/15 [00:00<00:00, 10.52it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  73% 11/15 [00:00<00:00, 11.09it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  87% 13/15 [00:01<00:00, 11.73it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 15/15 [00:01<00:00, 12.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:53:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.609 | nll_loss 5.131 | ppl 35.04 | wps 24000.2 | wpb 1853.1 | bsz 98.8 | num_updates 10000 | best_loss 6.551\n",
            "2021-02-24 09:53:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:53:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2en/checkpoints/checkpoint_last.pt (epoch 26 @ 10000 updates, score 6.609) (writing took 5.114479013000164 seconds)\n",
            "2021-02-24 09:53:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2021-02-24 09:53:30 | INFO | train | epoch 026 | loss 4.706 | nll_loss 3.098 | ppl 8.57 | wps 7301.8 | ups 3.44 | wpb 2120.4 | bsz 109.7 | num_updates 10000 | lr 0.000126491 | gnorm 1.782 | loss_scale 32 | train_wall 30 | wall 3540\n",
            "2021-02-24 09:53:30 | INFO | fairseq_cli.train | done training in 3539.5 seconds\n",
            "\n",
            "real\t59m14.303s\n",
            "user\t31m48.056s\n",
            "sys\t13m11.416s\n",
            "\n",
            "real\t1m9.452s\n",
            "user\t0m29.162s\n",
            "sys\t0m7.202s\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub\n",
            "2021-02-24 09:54:43 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict=None, target_lang='vsub', task='translation', tensorboard_logdir=None, testpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train', user_dir=None, validpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev', workers=1)\n",
            "2021-02-24 09:54:54 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 09:55:01 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.en: 42026 sents, 848482 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 09:55:01 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 09:55:02 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.en: 1482 sents, 27797 tokens, 3.53% replaced by <unk>\n",
            "2021-02-24 09:55:02 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43624 types\n",
            "2021-02-24 09:55:04 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.en: 1527 sents, 33040 tokens, 3.62% replaced by <unk>\n",
            "2021-02-24 09:55:04 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 09:55:14 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.vsub: 42026 sents, 1164786 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 09:55:14 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 09:55:15 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.vsub: 1482 sents, 39469 tokens, 0.0355% replaced by <unk>\n",
            "2021-02-24 09:55:15 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 43624 types\n",
            "2021-02-24 09:55:16 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.vsub: 1527 sents, 47028 tokens, 0.0702% replaced by <unk>\n",
            "2021-02-24 09:55:16 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized\n",
            "\n",
            "real\t0m34.641s\n",
            "user\t0m27.393s\n",
            "sys\t0m0.426s\n",
            "2021-02-24 09:55:18 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n",
            "2021-02-24 09:55:18 | INFO | fairseq.tasks.translation | [en] dictionary: 43624 types\n",
            "2021-02-24 09:55:18 | INFO | fairseq.tasks.translation | [vsub] dictionary: 43624 types\n",
            "2021-02-24 09:55:18 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized/valid.en-vsub.en\n",
            "2021-02-24 09:55:18 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized/valid.en-vsub.vsub\n",
            "2021-02-24 09:55:18 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized valid en-vsub 1482 examples\n",
            "2021-02-24 09:55:19 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(43624, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(43624, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=43624, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-02-24 09:55:19 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-02-24 09:55:19 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
            "2021-02-24 09:55:19 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
            "2021-02-24 09:55:19 | INFO | fairseq_cli.train | num. model params: 66473984 (num. trained: 66473984)\n",
            "2021-02-24 09:55:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-02-24 09:55:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-02-24 09:55:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 09:55:22 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-02-24 09:55:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 09:55:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-02-24 09:55:22 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n",
            "2021-02-24 09:55:22 | INFO | fairseq.trainer | no existing checkpoint found /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint_last.pt\n",
            "2021-02-24 09:55:22 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-02-24 09:55:22 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized/train.en-vsub.en\n",
            "2021-02-24 09:55:22 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized/train.en-vsub.vsub\n",
            "2021-02-24 09:55:22 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/binarized train en-vsub 42026 examples\n",
            "2021-02-24 09:55:22 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "epoch 001:   0% 0/420 [00:00<?, ?it/s]2021-02-24 09:55:22 | INFO | fairseq.trainer | begin training epoch 1\n",
            "epoch 001:   9% 39/420 [00:09<01:27,  4.34it/s]2021-02-24 09:55:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
            "epoch 001: 100% 419/420 [01:38<00:00,  4.25it/s]2021-02-24 09:57:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.12it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:57:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.35 | nll_loss 10.73 | ppl 1698.89 | wps 28638.5 | wpb 1973.5 | bsz 74.1 | num_updates 419\n",
            "2021-02-24 09:57:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 09:57:54 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint1.pt (epoch 1 @ 419 updates, score 11.35) (writing took 51.819548612999824 seconds)\n",
            "2021-02-24 09:57:54 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-02-24 09:57:54 | INFO | train | epoch 001 | loss 13.529 | nll_loss 13.229 | ppl 9598.25 | wps 7651.3 | ups 2.76 | wpb 2774.7 | bsz 99.4 | num_updates 419 | lr 2.095e-05 | gnorm 2.285 | loss_scale 64 | train_wall 97 | wall 152\n",
            "epoch 002:   0% 0/420 [00:00<?, ?it/s]2021-02-24 09:57:54 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  42% 176/420 [00:43<00:57,  4.25it/s]2021-02-24 09:58:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
            "epoch 002: 100% 419/420 [01:40<00:00,  4.49it/s]2021-02-24 09:59:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 09:59:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.359 | nll_loss 9.44 | ppl 694.35 | wps 28488.8 | wpb 1973.5 | bsz 74.1 | num_updates 838 | best_loss 10.359\n",
            "2021-02-24 09:59:36 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:00:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint2.pt (epoch 2 @ 838 updates, score 10.359) (writing took 47.62665572500009 seconds)\n",
            "2021-02-24 10:00:24 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-02-24 10:00:24 | INFO | train | epoch 002 | loss 10.64 | nll_loss 9.859 | ppl 928.76 | wps 7778.2 | ups 2.8 | wpb 2777.6 | bsz 99.8 | num_updates 838 | lr 4.19e-05 | gnorm 1.643 | loss_scale 32 | train_wall 99 | wall 302\n",
            "epoch 003:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:00:24 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 419/420 [01:40<00:00,  4.51it/s, loss=11.787, nll_loss=11.191, ppl=2337.98, wps=8108.4, ups=2.92, wpb=2774.4, bsz=99.7, num_updates=1000, lr=5e-05, gnorm=1.945, loss_scale=32, train_wall=236, wall=342]2021-02-24 10:02:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.22it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.00it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:02:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.65 | nll_loss 8.617 | ppl 392.76 | wps 28543.5 | wpb 1973.5 | bsz 74.1 | num_updates 1258 | best_loss 9.65\n",
            "2021-02-24 10:02:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:02:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint3.pt (epoch 3 @ 1258 updates, score 9.65) (writing took 49.830766422000124 seconds)\n",
            "2021-02-24 10:02:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-02-24 10:02:56 | INFO | train | epoch 003 | loss 10.054 | nll_loss 9.142 | ppl 564.93 | wps 7646.2 | ups 2.76 | wpb 2773.3 | bsz 100.1 | num_updates 1258 | lr 6.29e-05 | gnorm 2.028 | loss_scale 32 | train_wall 99 | wall 454\n",
            "epoch 004:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:02:56 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 419/420 [01:40<00:00,  4.69it/s]2021-02-24 10:04:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  6.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:04:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.992 | nll_loss 7.872 | ppl 234.23 | wps 28502.5 | wpb 1973.5 | bsz 74.1 | num_updates 1678 | best_loss 8.992\n",
            "2021-02-24 10:04:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:05:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint4.pt (epoch 4 @ 1678 updates, score 8.992) (writing took 48.73728451700026 seconds)\n",
            "2021-02-24 10:05:27 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-02-24 10:05:27 | INFO | train | epoch 004 | loss 9.338 | nll_loss 8.335 | ppl 322.91 | wps 7735.1 | ups 2.79 | wpb 2773.3 | bsz 100.1 | num_updates 1678 | lr 8.39e-05 | gnorm 2.263 | loss_scale 32 | train_wall 98 | wall 605\n",
            "epoch 005:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:05:27 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 419/420 [01:39<00:00,  4.32it/s, loss=9.323, nll_loss=8.318, ppl=319.19, wps=8174.4, ups=2.94, wpb=2777, bsz=99.9, num_updates=2000, lr=0.0001, gnorm=2.232, loss_scale=32, train_wall=234, wall=682]2021-02-24 10:07:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.12it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:07:08 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.533 | nll_loss 7.392 | ppl 167.97 | wps 28821.4 | wpb 1973.5 | bsz 74.1 | num_updates 2098 | best_loss 8.533\n",
            "2021-02-24 10:07:08 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:07:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint5.pt (epoch 5 @ 2098 updates, score 8.533) (writing took 47.407393344999946 seconds)\n",
            "2021-02-24 10:07:56 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-02-24 10:07:56 | INFO | train | epoch 005 | loss 8.765 | nll_loss 7.689 | ppl 206.29 | wps 7814.7 | ups 2.82 | wpb 2773.3 | bsz 100.1 | num_updates 2098 | lr 0.0001049 | gnorm 2.242 | loss_scale 32 | train_wall 98 | wall 754\n",
            "epoch 006:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:07:56 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 419/420 [01:39<00:00,  4.36it/s]2021-02-24 10:09:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.08it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.15it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.17it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.02it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:09:37 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.165 | nll_loss 6.936 | ppl 122.48 | wps 28531.9 | wpb 1973.5 | bsz 74.1 | num_updates 2518 | best_loss 8.165\n",
            "2021-02-24 10:09:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:10:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint6.pt (epoch 6 @ 2518 updates, score 8.165) (writing took 47.831886791000215 seconds)\n",
            "2021-02-24 10:10:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-02-24 10:10:25 | INFO | train | epoch 006 | loss 8.342 | nll_loss 7.212 | ppl 148.22 | wps 7810.8 | ups 2.82 | wpb 2773.3 | bsz 100.1 | num_updates 2518 | lr 0.0001259 | gnorm 2.122 | loss_scale 32 | train_wall 98 | wall 903\n",
            "epoch 007:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:10:25 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 419/420 [01:39<00:00,  4.56it/s]2021-02-24 10:12:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.21it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.83it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.91it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.00it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.05it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.51it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.00it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:12:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.909 | nll_loss 6.663 | ppl 101.35 | wps 28765.4 | wpb 1973.5 | bsz 74.1 | num_updates 2938 | best_loss 7.909\n",
            "2021-02-24 10:12:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:12:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint7.pt (epoch 7 @ 2938 updates, score 7.909) (writing took 45.67797633800001 seconds)\n",
            "2021-02-24 10:12:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-02-24 10:12:52 | INFO | train | epoch 007 | loss 8.001 | nll_loss 6.826 | ppl 113.48 | wps 7920.6 | ups 2.86 | wpb 2773.3 | bsz 100.1 | num_updates 2938 | lr 0.0001469 | gnorm 2.052 | loss_scale 32 | train_wall 98 | wall 1050\n",
            "epoch 008:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:12:52 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 419/420 [01:40<00:00,  4.47it/s, loss=8.184, nll_loss=7.033, ppl=130.97, wps=7213.1, ups=2.6, wpb=2776, bsz=101, num_updates=3000, lr=0.00015, gnorm=2.081, loss_scale=32, train_wall=235, wall=1067]2021-02-24 10:14:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 1/20 [00:00<00:02,  6.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.52it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.70it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.80it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.71it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.69it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.99it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.28it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:14:34 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.676 | nll_loss 6.402 | ppl 84.55 | wps 28546.9 | wpb 1973.5 | bsz 74.1 | num_updates 3358 | best_loss 7.676\n",
            "2021-02-24 10:14:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:15:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint8.pt (epoch 8 @ 3358 updates, score 7.676) (writing took 47.919572464999874 seconds)\n",
            "2021-02-24 10:15:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-02-24 10:15:22 | INFO | train | epoch 008 | loss 7.7 | nll_loss 6.487 | ppl 89.71 | wps 7752.2 | ups 2.8 | wpb 2773.3 | bsz 100.1 | num_updates 3358 | lr 0.0001679 | gnorm 1.924 | loss_scale 32 | train_wall 99 | wall 1200\n",
            "epoch 009:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:15:22 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 419/420 [01:39<00:00,  4.38it/s]2021-02-24 10:17:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  6.10it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.47it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.70it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.64it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.66it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.82it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.23it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:17:04 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.514 | nll_loss 6.193 | ppl 73.16 | wps 28918.8 | wpb 1973.5 | bsz 74.1 | num_updates 3778 | best_loss 7.514\n",
            "2021-02-24 10:17:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:17:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint9.pt (epoch 9 @ 3778 updates, score 7.514) (writing took 46.103722781000215 seconds)\n",
            "2021-02-24 10:17:50 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-02-24 10:17:50 | INFO | train | epoch 009 | loss 7.452 | nll_loss 6.206 | ppl 73.82 | wps 7886.6 | ups 2.84 | wpb 2773.3 | bsz 100.1 | num_updates 3778 | lr 0.0001889 | gnorm 1.886 | loss_scale 32 | train_wall 98 | wall 1348\n",
            "epoch 010:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:17:50 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 419/420 [01:39<00:00,  4.46it/s, loss=7.488, nll_loss=6.247, ppl=75.93, wps=8269.6, ups=2.98, wpb=2770.7, bsz=99.3, num_updates=4000, lr=0.0002, gnorm=1.885, loss_scale=32, train_wall=234, wall=1402]2021-02-24 10:19:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.77it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.98it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.23it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.26it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.68it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:19:31 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.364 | nll_loss 6.018 | ppl 64.8 | wps 28889 | wpb 1973.5 | bsz 74.1 | num_updates 4198 | best_loss 7.364\n",
            "2021-02-24 10:19:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:20:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint10.pt (epoch 10 @ 4198 updates, score 7.364) (writing took 49.08135380300064 seconds)\n",
            "2021-02-24 10:20:20 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-02-24 10:20:20 | INFO | train | epoch 010 | loss 7.215 | nll_loss 5.937 | ppl 61.26 | wps 7729.5 | ups 2.79 | wpb 2773.3 | bsz 100.1 | num_updates 4198 | lr 0.000195227 | gnorm 1.836 | loss_scale 32 | train_wall 98 | wall 1499\n",
            "epoch 011:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:20:20 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 419/420 [01:39<00:00,  4.43it/s]2021-02-24 10:22:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.46it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.70it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 13/20 [00:00<00:00, 12.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.65it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.33it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:22:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.202 | nll_loss 5.836 | ppl 57.12 | wps 29414.8 | wpb 1973.5 | bsz 74.1 | num_updates 4618 | best_loss 7.202\n",
            "2021-02-24 10:22:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:22:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint11.pt (epoch 11 @ 4618 updates, score 7.202) (writing took 42.87873119300002 seconds)\n",
            "2021-02-24 10:22:45 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2021-02-24 10:22:45 | INFO | train | epoch 011 | loss 6.978 | nll_loss 5.668 | ppl 50.86 | wps 8071.8 | ups 2.91 | wpb 2773.3 | bsz 100.1 | num_updates 4618 | lr 0.000186137 | gnorm 1.74 | loss_scale 32 | train_wall 98 | wall 1643\n",
            "epoch 012:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:22:45 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 419/420 [01:39<00:00,  4.52it/s, loss=6.936, nll_loss=5.621, ppl=49.21, wps=8369.9, ups=3.02, wpb=2774.8, bsz=100.3, num_updates=5000, lr=0.000178885, gnorm=1.747, loss_scale=32, train_wall=233, wall=1734]2021-02-24 10:24:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.25it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.56it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.83it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 10.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.07it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.14it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.56it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:24:26 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.179 | nll_loss 5.823 | ppl 56.61 | wps 28571.9 | wpb 1973.5 | bsz 74.1 | num_updates 5038 | best_loss 7.179\n",
            "2021-02-24 10:24:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:25:12 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint12.pt (epoch 12 @ 5038 updates, score 7.179) (writing took 45.92132029500044 seconds)\n",
            "2021-02-24 10:25:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2021-02-24 10:25:12 | INFO | train | epoch 012 | loss 6.753 | nll_loss 5.414 | ppl 42.64 | wps 7931 | ups 2.86 | wpb 2773.3 | bsz 100.1 | num_updates 5038 | lr 0.00017821 | gnorm 1.711 | loss_scale 32 | train_wall 98 | wall 1790\n",
            "epoch 013:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:25:12 | INFO | fairseq.trainer | begin training epoch 13\n",
            "epoch 013: 100% 419/420 [01:40<00:00,  4.38it/s]2021-02-24 10:26:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.54it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.78it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.80it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.01it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.09it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.15it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.24it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.30it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.64it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:26:54 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.019 | nll_loss 5.631 | ppl 49.56 | wps 28674.2 | wpb 1973.5 | bsz 74.1 | num_updates 5458 | best_loss 7.019\n",
            "2021-02-24 10:26:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:27:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint13.pt (epoch 13 @ 5458 updates, score 7.019) (writing took 45.32575066499976 seconds)\n",
            "2021-02-24 10:27:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2021-02-24 10:27:39 | INFO | train | epoch 013 | loss 6.571 | nll_loss 5.207 | ppl 36.94 | wps 7905.6 | ups 2.85 | wpb 2773.3 | bsz 100.1 | num_updates 5458 | lr 0.000171216 | gnorm 1.684 | loss_scale 32 | train_wall 99 | wall 1937\n",
            "epoch 014:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:27:39 | INFO | fairseq.trainer | begin training epoch 14\n",
            "epoch 014: 100% 419/420 [01:40<00:00,  4.55it/s]2021-02-24 10:29:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.14it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.31it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  25% 5/20 [00:00<00:02,  7.44it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.69it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.85it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 10.91it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.00it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.05it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.39it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.91it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:29:21 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.822 | nll_loss 5.397 | ppl 42.14 | wps 28801.6 | wpb 1973.5 | bsz 74.1 | num_updates 5878 | best_loss 6.822\n",
            "2021-02-24 10:29:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:30:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint14.pt (epoch 14 @ 5878 updates, score 6.822) (writing took 44.658090630000515 seconds)\n",
            "2021-02-24 10:30:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2021-02-24 10:30:05 | INFO | train | epoch 014 | loss 6.367 | nll_loss 4.975 | ppl 31.44 | wps 7952.1 | ups 2.87 | wpb 2773.3 | bsz 100.1 | num_updates 5878 | lr 0.000164985 | gnorm 1.667 | loss_scale 32 | train_wall 99 | wall 2084\n",
            "epoch 015:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:30:05 | INFO | fairseq.trainer | begin training epoch 15\n",
            "epoch 015: 100% 419/420 [01:40<00:00,  4.25it/s, loss=6.446, nll_loss=5.065, ppl=33.47, wps=7259.4, ups=2.63, wpb=2764.6, bsz=99.5, num_updates=6000, lr=0.000163299, gnorm=1.704, loss_scale=32, train_wall=236, wall=2114]2021-02-24 10:31:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.33it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.55it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.74it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.95it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.08it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.05it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.12it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.18it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.55it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.01it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:31:48 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.729 | nll_loss 5.284 | ppl 38.95 | wps 28836.3 | wpb 1973.5 | bsz 74.1 | num_updates 6298 | best_loss 6.729\n",
            "2021-02-24 10:31:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:32:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint15.pt (epoch 15 @ 6298 updates, score 6.729) (writing took 50.02849944300033 seconds)\n",
            "2021-02-24 10:32:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2021-02-24 10:32:38 | INFO | train | epoch 015 | loss 6.171 | nll_loss 4.751 | ppl 26.93 | wps 7653 | ups 2.76 | wpb 2773.3 | bsz 100.1 | num_updates 6298 | lr 0.000159389 | gnorm 1.728 | loss_scale 32 | train_wall 99 | wall 2236\n",
            "epoch 016:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:32:38 | INFO | fairseq.trainer | begin training epoch 16\n",
            "epoch 016: 100% 419/420 [01:40<00:00,  4.41it/s]2021-02-24 10:34:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.61it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  10% 2/20 [00:00<00:02,  6.36it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  20% 4/20 [00:00<00:02,  7.57it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  30% 6/20 [00:00<00:01,  8.80it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  40% 8/20 [00:00<00:01,  9.92it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 10/20 [00:00<00:00, 10.82it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  60% 12/20 [00:00<00:00, 11.88it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  70% 14/20 [00:01<00:00, 12.86it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  80% 16/20 [00:01<00:00, 13.55it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  90% 18/20 [00:01<00:00, 13.87it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 20/20 [00:01<00:00, 14.57it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:34:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.648 | nll_loss 5.194 | ppl 36.61 | wps 27981.9 | wpb 1973.5 | bsz 74.1 | num_updates 6718 | best_loss 6.648\n",
            "2021-02-24 10:34:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:35:07 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint16.pt (epoch 16 @ 6718 updates, score 6.648) (writing took 47.128714396999385 seconds)\n",
            "2021-02-24 10:35:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2021-02-24 10:35:07 | INFO | train | epoch 016 | loss 5.996 | nll_loss 4.552 | ppl 23.45 | wps 7777.8 | ups 2.8 | wpb 2773.3 | bsz 100.1 | num_updates 6718 | lr 0.000154326 | gnorm 1.724 | loss_scale 32 | train_wall 99 | wall 2386\n",
            "epoch 017:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:35:07 | INFO | fairseq.trainer | begin training epoch 17\n",
            "epoch 017: 100% 419/420 [01:40<00:00,  4.21it/s, loss=5.998, nll_loss=4.554, ppl=23.49, wps=8187.9, ups=2.94, wpb=2783, bsz=101.4, num_updates=7000, lr=0.000151186, gnorm=1.725, loss_scale=32, train_wall=236, wall=2454]2021-02-24 10:36:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.76it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.94it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.15it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.36it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.38it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.40it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.37it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.34it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.69it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.14it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:36:50 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.554 | nll_loss 5.057 | ppl 33.29 | wps 28686.1 | wpb 1973.5 | bsz 74.1 | num_updates 7138 | best_loss 6.554\n",
            "2021-02-24 10:36:50 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:37:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint17.pt (epoch 17 @ 7138 updates, score 6.554) (writing took 49.16419986799974 seconds)\n",
            "2021-02-24 10:37:39 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2021-02-24 10:37:39 | INFO | train | epoch 017 | loss 5.83 | nll_loss 4.362 | ppl 20.57 | wps 7692.6 | ups 2.77 | wpb 2773.3 | bsz 100.1 | num_updates 7138 | lr 0.000149717 | gnorm 1.751 | loss_scale 32 | train_wall 99 | wall 2537\n",
            "epoch 018:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:37:39 | INFO | fairseq.trainer | begin training epoch 18\n",
            "epoch 018: 100% 419/420 [01:40<00:00,  4.42it/s]2021-02-24 10:39:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.41it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  10% 2/20 [00:00<00:02,  6.26it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  20% 4/20 [00:00<00:02,  7.52it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  30% 6/20 [00:00<00:01,  8.74it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  40% 8/20 [00:00<00:01,  9.86it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 10/20 [00:00<00:00, 10.73it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  60% 12/20 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  70% 14/20 [00:01<00:00, 12.83it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  80% 16/20 [00:01<00:00, 13.57it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  90% 18/20 [00:01<00:00, 13.92it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 20/20 [00:01<00:00, 14.65it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:39:21 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.467 | nll_loss 4.943 | ppl 30.76 | wps 28332 | wpb 1973.5 | bsz 74.1 | num_updates 7558 | best_loss 6.467\n",
            "2021-02-24 10:39:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:40:07 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint18.pt (epoch 18 @ 7558 updates, score 6.467) (writing took 46.340272098999776 seconds)\n",
            "2021-02-24 10:40:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2021-02-24 10:40:07 | INFO | train | epoch 018 | loss 5.681 | nll_loss 4.192 | ppl 18.27 | wps 7846.8 | ups 2.83 | wpb 2773.3 | bsz 100.1 | num_updates 7558 | lr 0.000145498 | gnorm 1.795 | loss_scale 32 | train_wall 99 | wall 2685\n",
            "epoch 019:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:40:07 | INFO | fairseq.trainer | begin training epoch 19\n",
            "epoch 019: 100% 419/420 [01:40<00:00,  4.48it/s]2021-02-24 10:41:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.14it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.31it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  25% 5/20 [00:00<00:02,  7.40it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.67it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.81it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 10.91it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.03it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.01it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.44it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:41:49 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.453 | nll_loss 4.935 | ppl 30.6 | wps 28615.9 | wpb 1973.5 | bsz 74.1 | num_updates 7978 | best_loss 6.453\n",
            "2021-02-24 10:41:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:42:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint19.pt (epoch 19 @ 7978 updates, score 6.453) (writing took 47.127820632000294 seconds)\n",
            "2021-02-24 10:42:36 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2021-02-24 10:42:36 | INFO | train | epoch 019 | loss 5.542 | nll_loss 4.032 | ppl 16.35 | wps 7813.7 | ups 2.82 | wpb 2773.3 | bsz 100.1 | num_updates 7978 | lr 0.000141616 | gnorm 1.808 | loss_scale 32 | train_wall 99 | wall 2834\n",
            "epoch 020:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:42:36 | INFO | fairseq.trainer | begin training epoch 20\n",
            "epoch 020: 100% 419/420 [01:40<00:00,  4.40it/s, loss=5.636, nll_loss=4.14, ppl=17.63, wps=7176.4, ups=2.59, wpb=2770.7, bsz=99.3, num_updates=8000, lr=0.000141421, gnorm=1.79, loss_scale=32, train_wall=234, wall=2840]2021-02-24 10:44:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.18it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.40it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.70it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.90it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.96it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.03it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.14it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.15it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.50it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:44:18 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.43 | nll_loss 4.901 | ppl 29.88 | wps 29121.2 | wpb 1973.5 | bsz 74.1 | num_updates 8398 | best_loss 6.43\n",
            "2021-02-24 10:44:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:45:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint20.pt (epoch 20 @ 8398 updates, score 6.43) (writing took 45.552204438000444 seconds)\n",
            "2021-02-24 10:45:04 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2021-02-24 10:45:04 | INFO | train | epoch 020 | loss 5.418 | nll_loss 3.89 | ppl 14.83 | wps 7905.7 | ups 2.85 | wpb 2773.3 | bsz 100.1 | num_updates 8398 | lr 0.00013803 | gnorm 1.845 | loss_scale 32 | train_wall 99 | wall 2982\n",
            "epoch 021:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:45:04 | INFO | fairseq.trainer | begin training epoch 21\n",
            "epoch 021: 100% 419/420 [01:40<00:00,  4.44it/s]2021-02-24 10:46:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.65it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.95it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.02it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.07it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.14it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.18it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.27it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.29it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.63it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:46:46 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.346 | nll_loss 4.804 | ppl 27.93 | wps 28521.2 | wpb 1973.5 | bsz 74.1 | num_updates 8818 | best_loss 6.346\n",
            "2021-02-24 10:46:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:47:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint21.pt (epoch 21 @ 8818 updates, score 6.346) (writing took 46.52334769499976 seconds)\n",
            "2021-02-24 10:47:32 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2021-02-24 10:47:32 | INFO | train | epoch 021 | loss 5.296 | nll_loss 3.749 | ppl 13.45 | wps 7828.6 | ups 2.82 | wpb 2773.3 | bsz 100.1 | num_updates 8818 | lr 0.000134702 | gnorm 1.847 | loss_scale 32 | train_wall 99 | wall 3131\n",
            "epoch 022:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:47:32 | INFO | fairseq.trainer | begin training epoch 22\n",
            "epoch 022: 100% 419/420 [01:40<00:00,  4.39it/s, loss=5.322, nll_loss=3.78, ppl=13.74, wps=8249.3, ups=2.99, wpb=2761.2, bsz=99.1, num_updates=9000, lr=0.000133333, gnorm=1.851, loss_scale=32, train_wall=236, wall=3175]2021-02-24 10:49:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  6.10it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.09it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.28it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.49it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.50it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.50it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.41it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.46it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.64it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.03it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:49:14 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.311 | nll_loss 4.733 | ppl 26.6 | wps 28080.3 | wpb 1973.5 | bsz 74.1 | num_updates 9238 | best_loss 6.311\n",
            "2021-02-24 10:49:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:50:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint22.pt (epoch 22 @ 9238 updates, score 6.311) (writing took 46.41491263699936 seconds)\n",
            "2021-02-24 10:50:01 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2021-02-24 10:50:01 | INFO | train | epoch 022 | loss 5.182 | nll_loss 3.619 | ppl 12.29 | wps 7847.5 | ups 2.83 | wpb 2773.3 | bsz 100.1 | num_updates 9238 | lr 0.000131605 | gnorm 1.869 | loss_scale 32 | train_wall 99 | wall 3279\n",
            "epoch 023:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:50:01 | INFO | fairseq.trainer | begin training epoch 23\n",
            "epoch 023: 100% 419/420 [01:39<00:00,  4.62it/s]2021-02-24 10:51:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.92it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  7.09it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  8.35it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  9.54it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  45% 9/20 [00:00<00:01, 10.59it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.62it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.59it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.86it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 14.27it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:51:42 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.349 | nll_loss 4.799 | ppl 27.84 | wps 29124.2 | wpb 1973.5 | bsz 74.1 | num_updates 9658 | best_loss 6.311\n",
            "2021-02-24 10:51:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:52:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint23.pt (epoch 23 @ 9658 updates, score 6.349) (writing took 19.421642567000163 seconds)\n",
            "2021-02-24 10:52:02 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2021-02-24 10:52:02 | INFO | train | epoch 023 | loss 5.075 | nll_loss 3.496 | ppl 11.28 | wps 9642.5 | ups 3.48 | wpb 2773.3 | bsz 100.1 | num_updates 9658 | lr 0.000128711 | gnorm 1.887 | loss_scale 32 | train_wall 98 | wall 3400\n",
            "epoch 024:   0% 0/420 [00:00<?, ?it/s]2021-02-24 10:52:02 | INFO | fairseq.trainer | begin training epoch 24\n",
            "epoch 024:  81% 341/420 [01:23<00:18,  4.23it/s]2021-02-24 10:53:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   5% 1/20 [00:00<00:03,  5.26it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  15% 3/20 [00:00<00:02,  6.48it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  25% 5/20 [00:00<00:01,  7.66it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  35% 7/20 [00:00<00:01,  8.85it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  45% 9/20 [00:00<00:01,  9.95it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  55% 11/20 [00:00<00:00, 10.91it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  65% 13/20 [00:01<00:00, 12.04it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  75% 15/20 [00:01<00:00, 13.13it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  85% 17/20 [00:01<00:00, 13.52it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  95% 19/20 [00:01<00:00, 13.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:53:26 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.284 | nll_loss 4.703 | ppl 26.04 | wps 28651.1 | wpb 1973.5 | bsz 74.1 | num_updates 10000 | best_loss 6.284\n",
            "2021-02-24 10:53:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:53:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vsub/checkpoints/checkpoint_best.pt (epoch 24 @ 10000 updates, score 6.284) (writing took 24.043344556001102 seconds)\n",
            "2021-02-24 10:53:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2021-02-24 10:53:50 | INFO | train | epoch 024 | loss 4.966 | nll_loss 3.371 | ppl 10.35 | wps 8659.7 | ups 3.14 | wpb 2756.5 | bsz 100.4 | num_updates 10000 | lr 0.000126491 | gnorm 1.906 | loss_scale 32 | train_wall 82 | wall 3509\n",
            "2021-02-24 10:53:50 | INFO | fairseq_cli.train | done training in 3508.5 seconds\n",
            "\n",
            "real\t58m40.144s\n",
            "user\t32m30.777s\n",
            "sys\t13m18.998s\n",
            "\n",
            "real\t1m7.602s\n",
            "user\t0m36.904s\n",
            "sys\t0m7.728s\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben\n",
            "2021-02-24 10:55:06 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='vsub', srcdict=None, target_lang='suben', task='translation', tensorboard_logdir=None, testpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train', user_dir=None, validpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev', workers=1)\n",
            "2021-02-24 10:55:16 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 10:55:27 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.vsub: 42026 sents, 1164786 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 10:55:27 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 10:55:28 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.vsub: 1482 sents, 39469 tokens, 0.0304% replaced by <unk>\n",
            "2021-02-24 10:55:28 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 10:55:29 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.vsub: 1527 sents, 47028 tokens, 0.0638% replaced by <unk>\n",
            "2021-02-24 10:55:29 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 10:55:38 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.suben: 42026 sents, 981713 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 10:55:38 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 10:55:39 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.suben: 1482 sents, 33337 tokens, 0.003% replaced by <unk>\n",
            "2021-02-24 10:55:39 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 10:55:40 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.suben: 1527 sents, 40254 tokens, 0.0174% replaced by <unk>\n",
            "2021-02-24 10:55:40 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized\n",
            "\n",
            "real\t0m35.820s\n",
            "user\t0m29.677s\n",
            "sys\t0m0.442s\n",
            "2021-02-24 10:55:42 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n",
            "2021-02-24 10:55:42 | INFO | fairseq.tasks.translation | [vsub] dictionary: 17280 types\n",
            "2021-02-24 10:55:42 | INFO | fairseq.tasks.translation | [suben] dictionary: 17280 types\n",
            "2021-02-24 10:55:42 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized/valid.vsub-suben.vsub\n",
            "2021-02-24 10:55:42 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized/valid.vsub-suben.suben\n",
            "2021-02-24 10:55:42 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized valid vsub-suben 1482 examples\n",
            "2021-02-24 10:55:43 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(17280, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(17280, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=17280, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-02-24 10:55:43 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-02-24 10:55:43 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
            "2021-02-24 10:55:43 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
            "2021-02-24 10:55:43 | INFO | fairseq_cli.train | num. model params: 52985856 (num. trained: 52985856)\n",
            "2021-02-24 10:55:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-02-24 10:55:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-02-24 10:55:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 10:55:45 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-02-24 10:55:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 10:55:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-02-24 10:55:45 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n",
            "2021-02-24 10:55:45 | INFO | fairseq.trainer | no existing checkpoint found /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint_last.pt\n",
            "2021-02-24 10:55:45 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-02-24 10:55:45 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized/train.vsub-suben.vsub\n",
            "2021-02-24 10:55:45 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized/train.vsub-suben.suben\n",
            "2021-02-24 10:55:45 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/binarized train vsub-suben 42026 examples\n",
            "2021-02-24 10:55:45 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "epoch 001:   0% 0/415 [00:00<?, ?it/s]2021-02-24 10:55:45 | INFO | fairseq.trainer | begin training epoch 1\n",
            "epoch 001:   4% 17/415 [00:03<01:22,  4.80it/s]2021-02-24 10:55:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
            "epoch 001: 100% 414/415 [01:23<00:00,  5.20it/s]2021-02-24 10:57:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:57:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.036 | nll_loss 10.542 | ppl 1491.28 | wps 29400.3 | wpb 1852.1 | bsz 82.3 | num_updates 414\n",
            "2021-02-24 10:57:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:57:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint1.pt (epoch 1 @ 414 updates, score 11.036) (writing took 27.892540328000905 seconds)\n",
            "2021-02-24 10:57:38 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-02-24 10:57:38 | INFO | train | epoch 001 | loss 12.362 | nll_loss 12.076 | ppl 4318.61 | wps 8716.3 | ups 3.69 | wpb 2364.2 | bsz 100.8 | num_updates 414 | lr 2.07e-05 | gnorm 2.499 | loss_scale 64 | train_wall 82 | wall 113\n",
            "epoch 002:   0% 0/415 [00:00<?, ?it/s]2021-02-24 10:57:38 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  63% 261/415 [00:56<00:30,  5.11it/s]2021-02-24 10:58:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
            "epoch 002: 100% 414/415 [01:27<00:00,  5.11it/s]2021-02-24 10:59:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  7.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  9.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 10:59:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.491 | nll_loss 9.851 | ppl 923.45 | wps 29045.9 | wpb 1852.1 | bsz 82.3 | num_updates 828 | best_loss 10.491\n",
            "2021-02-24 10:59:07 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 10:59:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint2.pt (epoch 2 @ 828 updates, score 10.491) (writing took 22.74889702300061 seconds)\n",
            "2021-02-24 10:59:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-02-24 10:59:29 | INFO | train | epoch 002 | loss 10.67 | nll_loss 10.108 | ppl 1103.77 | wps 8764.3 | ups 3.71 | wpb 2365.5 | bsz 100.7 | num_updates 828 | lr 4.14e-05 | gnorm 1.721 | loss_scale 32 | train_wall 86 | wall 224\n",
            "epoch 003:   0% 0/415 [00:00<?, ?it/s]2021-02-24 10:59:30 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 414/415 [01:27<00:00,  5.22it/s, loss=11.322, nll_loss=10.863, ppl=1861.98, wps=9003.1, ups=3.8, wpb=2371.2, bsz=100.8, num_updates=1000, lr=5e-05, gnorm=2.043, loss_scale=32, train_wall=206, wall=264]2021-02-24 11:00:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:00:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.207 | nll_loss 9.512 | ppl 730.08 | wps 29713.5 | wpb 1852.1 | bsz 82.3 | num_updates 1243 | best_loss 10.207\n",
            "2021-02-24 11:00:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:01:21 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint3.pt (epoch 3 @ 1243 updates, score 10.207) (writing took 22.397362636000253 seconds)\n",
            "2021-02-24 11:01:21 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-02-24 11:01:21 | INFO | train | epoch 003 | loss 10.315 | nll_loss 9.676 | ppl 818.16 | wps 8782.3 | ups 3.71 | wpb 2365.6 | bsz 101.3 | num_updates 1243 | lr 6.215e-05 | gnorm 1.757 | loss_scale 32 | train_wall 86 | wall 336\n",
            "epoch 004:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:01:21 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 414/415 [01:27<00:00,  5.02it/s]2021-02-24 11:02:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  4.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  5.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  7.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  8.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 10.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  78% 14/18 [00:01<00:00, 12.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 13.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 14.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:02:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.806 | nll_loss 9.067 | ppl 536.4 | wps 29588 | wpb 1852.1 | bsz 82.3 | num_updates 1658 | best_loss 9.806\n",
            "2021-02-24 11:02:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:03:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint4.pt (epoch 4 @ 1658 updates, score 9.806) (writing took 27.39021535000029 seconds)\n",
            "2021-02-24 11:03:18 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-02-24 11:03:18 | INFO | train | epoch 004 | loss 9.928 | nll_loss 9.235 | ppl 602.59 | wps 8403 | ups 3.55 | wpb 2365.6 | bsz 101.3 | num_updates 1658 | lr 8.29e-05 | gnorm 1.774 | loss_scale 32 | train_wall 86 | wall 453\n",
            "epoch 005:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:03:18 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 414/415 [01:26<00:00,  5.20it/s, loss=9.873, nll_loss=9.173, ppl=577.07, wps=9026.6, ups=3.82, wpb=2363.1, bsz=101.4, num_updates=2000, lr=0.0001, gnorm=1.78, loss_scale=32, train_wall=205, wall=525]2021-02-24 11:04:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:04:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.377 | nll_loss 8.583 | ppl 383.48 | wps 28562.8 | wpb 1852.1 | bsz 82.3 | num_updates 2073 | best_loss 9.377\n",
            "2021-02-24 11:04:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint5.pt (epoch 5 @ 2073 updates, score 9.377) (writing took 22.205441359999895 seconds)\n",
            "2021-02-24 11:05:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-02-24 11:05:09 | INFO | train | epoch 005 | loss 9.512 | nll_loss 8.76 | ppl 433.61 | wps 8870.6 | ups 3.75 | wpb 2365.6 | bsz 101.3 | num_updates 2073 | lr 0.00010365 | gnorm 1.797 | loss_scale 32 | train_wall 85 | wall 564\n",
            "epoch 006:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:05:09 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 414/415 [01:27<00:00,  5.04it/s]2021-02-24 11:06:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.99it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.22it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 11.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 12.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.38it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.08it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 14.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:06:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.987 | nll_loss 8.118 | ppl 277.85 | wps 29602.4 | wpb 1852.1 | bsz 82.3 | num_updates 2488 | best_loss 8.987\n",
            "2021-02-24 11:06:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:07:06 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint6.pt (epoch 6 @ 2488 updates, score 8.987) (writing took 27.958772169999065 seconds)\n",
            "2021-02-24 11:07:06 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-02-24 11:07:06 | INFO | train | epoch 006 | loss 9.119 | nll_loss 8.309 | ppl 317.22 | wps 8391.1 | ups 3.55 | wpb 2365.6 | bsz 101.3 | num_updates 2488 | lr 0.0001244 | gnorm 1.751 | loss_scale 32 | train_wall 86 | wall 681\n",
            "epoch 007:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:07:06 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 414/415 [01:26<00:00,  5.21it/s]2021-02-24 11:08:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.50it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  7.73it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 10.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.05it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.03it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 13.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:08:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.71 | nll_loss 7.807 | ppl 223.95 | wps 29283.2 | wpb 1852.1 | bsz 82.3 | num_updates 2903 | best_loss 8.71\n",
            "2021-02-24 11:08:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:09:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint7.pt (epoch 7 @ 2903 updates, score 8.71) (writing took 27.97059659000115 seconds)\n",
            "2021-02-24 11:09:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-02-24 11:09:02 | INFO | train | epoch 007 | loss 8.784 | nll_loss 7.922 | ppl 242.6 | wps 8461.1 | ups 3.58 | wpb 2365.6 | bsz 101.3 | num_updates 2903 | lr 0.00014515 | gnorm 1.709 | loss_scale 32 | train_wall 85 | wall 797\n",
            "epoch 008:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:09:02 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 414/415 [01:26<00:00,  5.21it/s, loss=8.94, nll_loss=8.103, ppl=274.92, wps=8012.1, ups=3.4, wpb=2358.7, bsz=101.8, num_updates=3000, lr=0.00015, gnorm=1.735, loss_scale=32, train_wall=207, wall=820]2021-02-24 11:10:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.05it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 11.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 12.46it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.38it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.11it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 14.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:10:30 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.588 | nll_loss 7.653 | ppl 201.27 | wps 29965.2 | wpb 1852.1 | bsz 82.3 | num_updates 3318 | best_loss 8.588\n",
            "2021-02-24 11:10:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:10:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint8.pt (epoch 8 @ 3318 updates, score 8.588) (writing took 22.0431776610003 seconds)\n",
            "2021-02-24 11:10:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-02-24 11:10:52 | INFO | train | epoch 008 | loss 8.496 | nll_loss 7.59 | ppl 192.7 | wps 8905.5 | ups 3.76 | wpb 2365.6 | bsz 101.3 | num_updates 3318 | lr 0.0001659 | gnorm 1.629 | loss_scale 32 | train_wall 85 | wall 907\n",
            "epoch 009:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:10:52 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 414/415 [01:26<00:00,  5.19it/s]2021-02-24 11:12:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.69it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.49it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.27it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.63it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.23it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:12:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.312 | nll_loss 7.336 | ppl 161.59 | wps 29960.7 | wpb 1852.1 | bsz 82.3 | num_updates 3733 | best_loss 8.312\n",
            "2021-02-24 11:12:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:12:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint9.pt (epoch 9 @ 3733 updates, score 8.312) (writing took 27.34649856500073 seconds)\n",
            "2021-02-24 11:12:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-02-24 11:12:48 | INFO | train | epoch 009 | loss 8.258 | nll_loss 7.313 | ppl 159.04 | wps 8489.1 | ups 3.59 | wpb 2365.6 | bsz 101.3 | num_updates 3733 | lr 0.00018665 | gnorm 1.631 | loss_scale 32 | train_wall 85 | wall 1023\n",
            "epoch 010:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:12:48 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 414/415 [01:26<00:00,  5.22it/s, loss=8.259, nll_loss=7.315, ppl=159.21, wps=9087.6, ups=3.84, wpb=2363.9, bsz=100.7, num_updates=4000, lr=0.0002, gnorm=1.614, loss_scale=32, train_wall=204, wall=1080]2021-02-24 11:14:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.88it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.17it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:14:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.051 | nll_loss 7.024 | ppl 130.16 | wps 30451.1 | wpb 1852.1 | bsz 82.3 | num_updates 4148 | best_loss 8.051\n",
            "2021-02-24 11:14:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:14:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint10.pt (epoch 10 @ 4148 updates, score 8.051) (writing took 27.524573403999966 seconds)\n",
            "2021-02-24 11:14:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-02-24 11:14:44 | INFO | train | epoch 010 | loss 7.959 | nll_loss 6.967 | ppl 125.11 | wps 8456.8 | ups 3.57 | wpb 2365.6 | bsz 101.3 | num_updates 4148 | lr 0.0001964 | gnorm 1.596 | loss_scale 32 | train_wall 85 | wall 1139\n",
            "epoch 011:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:14:44 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 414/415 [01:26<00:00,  5.14it/s]2021-02-24 11:16:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.83it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  8.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  9.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.69it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 12.39it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 13.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 14.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.68it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 15.01it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:16:12 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.789 | nll_loss 6.699 | ppl 103.92 | wps 30256.6 | wpb 1852.1 | bsz 82.3 | num_updates 4563 | best_loss 7.789\n",
            "2021-02-24 11:16:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:16:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint11.pt (epoch 11 @ 4563 updates, score 7.789) (writing took 27.93942953199985 seconds)\n",
            "2021-02-24 11:16:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2021-02-24 11:16:40 | INFO | train | epoch 011 | loss 7.646 | nll_loss 6.606 | ppl 97.4 | wps 8432.5 | ups 3.56 | wpb 2365.6 | bsz 101.3 | num_updates 4563 | lr 0.000187256 | gnorm 1.619 | loss_scale 32 | train_wall 85 | wall 1255\n",
            "epoch 012:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:16:40 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 414/415 [01:26<00:00,  4.92it/s]2021-02-24 11:18:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.30it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  7.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.35it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.58it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.66it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:18:08 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.554 | nll_loss 6.426 | ppl 85.98 | wps 29143.9 | wpb 1852.1 | bsz 82.3 | num_updates 4978 | best_loss 7.554\n",
            "2021-02-24 11:18:08 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:18:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint12.pt (epoch 12 @ 4978 updates, score 7.554) (writing took 27.75041297700045 seconds)\n",
            "2021-02-24 11:18:36 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2021-02-24 11:18:36 | INFO | train | epoch 012 | loss 7.332 | nll_loss 6.242 | ppl 75.71 | wps 8475.9 | ups 3.58 | wpb 2365.6 | bsz 101.3 | num_updates 4978 | lr 0.00017928 | gnorm 1.63 | loss_scale 32 | train_wall 85 | wall 1371\n",
            "epoch 013:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:18:36 | INFO | fairseq.trainer | begin training epoch 13\n",
            "epoch 013: 100% 414/415 [01:26<00:00,  5.22it/s, loss=7.545, nll_loss=6.489, ppl=89.81, wps=7966.5, ups=3.36, wpb=2369.9, bsz=101.3, num_updates=5000, lr=0.000178885, gnorm=1.624, loss_scale=32, train_wall=206, wall=1377]2021-02-24 11:20:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.84it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.10it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.38it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.81it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 11.50it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 12.78it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.66it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.41it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 15.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:20:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.365 | nll_loss 6.2 | ppl 73.53 | wps 30518.8 | wpb 1852.1 | bsz 82.3 | num_updates 5393 | best_loss 7.365\n",
            "2021-02-24 11:20:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:20:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint13.pt (epoch 13 @ 5393 updates, score 7.365) (writing took 27.632402777999232 seconds)\n",
            "2021-02-24 11:20:31 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2021-02-24 11:20:31 | INFO | train | epoch 013 | loss 7.054 | nll_loss 5.92 | ppl 60.55 | wps 8516.7 | ups 3.6 | wpb 2365.6 | bsz 101.3 | num_updates 5393 | lr 0.000172244 | gnorm 1.663 | loss_scale 32 | train_wall 85 | wall 1486\n",
            "epoch 014:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:20:31 | INFO | fairseq.trainer | begin training epoch 14\n",
            "epoch 014: 100% 414/415 [01:25<00:00,  5.32it/s]2021-02-24 11:21:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.54it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  7.87it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  9.12it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.49it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 12.12it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 13.17it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.98it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.62it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 15.39it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:21:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.22 | nll_loss 6.021 | ppl 64.96 | wps 30598.9 | wpb 1852.1 | bsz 82.3 | num_updates 5808 | best_loss 7.22\n",
            "2021-02-24 11:21:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:22:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint14.pt (epoch 14 @ 5808 updates, score 7.22) (writing took 26.191163665000204 seconds)\n",
            "2021-02-24 11:22:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2021-02-24 11:22:24 | INFO | train | epoch 014 | loss 6.804 | nll_loss 5.631 | ppl 49.55 | wps 8671.5 | ups 3.67 | wpb 2365.6 | bsz 101.3 | num_updates 5808 | lr 0.000165977 | gnorm 1.709 | loss_scale 32 | train_wall 84 | wall 1599\n",
            "epoch 015:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:22:25 | INFO | fairseq.trainer | begin training epoch 15\n",
            "epoch 015: 100% 414/415 [01:25<00:00,  5.31it/s, loss=6.864, nll_loss=5.7, ppl=51.99, wps=8992.4, ups=3.8, wpb=2367.4, bsz=101.4, num_updates=6000, lr=0.000163299, gnorm=1.703, loss_scale=32, train_wall=203, wall=1641]2021-02-24 11:23:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.48it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.81it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.03it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.40it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 11.10it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 12.41it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.35it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.08it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 14.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:23:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.106 | nll_loss 5.872 | ppl 58.56 | wps 30442.4 | wpb 1852.1 | bsz 82.3 | num_updates 6223 | best_loss 7.106\n",
            "2021-02-24 11:23:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:24:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint15.pt (epoch 15 @ 6223 updates, score 7.106) (writing took 27.63795804700021 seconds)\n",
            "2021-02-24 11:24:19 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2021-02-24 11:24:19 | INFO | train | epoch 015 | loss 6.58 | nll_loss 5.369 | ppl 41.33 | wps 8558.6 | ups 3.62 | wpb 2365.6 | bsz 101.3 | num_updates 6223 | lr 0.000160347 | gnorm 1.754 | loss_scale 32 | train_wall 84 | wall 1714\n",
            "epoch 016:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:24:19 | INFO | fairseq.trainer | begin training epoch 16\n",
            "epoch 016: 100% 414/415 [01:25<00:00,  5.28it/s]2021-02-24 11:25:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.47it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  7.66it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.85it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.28it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.42it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.72it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.69it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.25it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.72it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:25:46 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.936 | nll_loss 5.656 | ppl 50.42 | wps 29252.7 | wpb 1852.1 | bsz 82.3 | num_updates 6638 | best_loss 6.936\n",
            "2021-02-24 11:25:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:26:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint16.pt (epoch 16 @ 6638 updates, score 6.936) (writing took 27.862177893999615 seconds)\n",
            "2021-02-24 11:26:14 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2021-02-24 11:26:14 | INFO | train | epoch 016 | loss 6.353 | nll_loss 5.105 | ppl 34.42 | wps 8525.3 | ups 3.6 | wpb 2365.6 | bsz 101.3 | num_updates 6638 | lr 0.000155253 | gnorm 1.752 | loss_scale 32 | train_wall 84 | wall 1829\n",
            "epoch 017:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:26:14 | INFO | fairseq.trainer | begin training epoch 17\n",
            "epoch 017: 100% 414/415 [01:25<00:00,  5.16it/s, loss=6.32, nll_loss=5.068, ppl=33.54, wps=8936.7, ups=3.79, wpb=2361.1, bsz=101.3, num_updates=7000, lr=0.000151186, gnorm=1.75, loss_scale=32, train_wall=202, wall=1905]2021-02-24 11:27:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.03it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.29it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  7.52it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  8.97it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 10.74it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.16it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.28it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 13.94it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:27:42 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.818 | nll_loss 5.522 | ppl 45.97 | wps 30503.7 | wpb 1852.1 | bsz 82.3 | num_updates 7053 | best_loss 6.818\n",
            "2021-02-24 11:27:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:28:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint17.pt (epoch 17 @ 7053 updates, score 6.818) (writing took 27.431234990999656 seconds)\n",
            "2021-02-24 11:28:09 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2021-02-24 11:28:09 | INFO | train | epoch 017 | loss 6.149 | nll_loss 4.867 | ppl 29.19 | wps 8541.5 | ups 3.61 | wpb 2365.6 | bsz 101.3 | num_updates 7053 | lr 0.000150617 | gnorm 1.76 | loss_scale 32 | train_wall 85 | wall 1944\n",
            "epoch 018:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:28:09 | INFO | fairseq.trainer | begin training epoch 18\n",
            "epoch 018: 100% 414/415 [01:25<00:00,  5.02it/s]2021-02-24 11:29:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.51it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.73it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.03it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.47it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.22it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.53it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.58it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.14it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.65it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:29:37 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.708 | nll_loss 5.378 | ppl 41.58 | wps 30155.6 | wpb 1852.1 | bsz 82.3 | num_updates 7468 | best_loss 6.708\n",
            "2021-02-24 11:29:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:30:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint18.pt (epoch 18 @ 7468 updates, score 6.708) (writing took 27.597171634000915 seconds)\n",
            "2021-02-24 11:30:04 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2021-02-24 11:30:04 | INFO | train | epoch 018 | loss 5.96 | nll_loss 4.646 | ppl 25.04 | wps 8526.5 | ups 3.6 | wpb 2365.6 | bsz 101.3 | num_updates 7468 | lr 0.000146372 | gnorm 1.786 | loss_scale 32 | train_wall 85 | wall 2059\n",
            "epoch 019:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:30:04 | INFO | fairseq.trainer | begin training epoch 19\n",
            "epoch 019: 100% 414/415 [01:25<00:00,  5.22it/s]2021-02-24 11:31:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  7.00it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  8.12it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  9.39it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.69it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 12.25it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 13.45it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 14.32it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.77it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 15.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:31:31 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.619 | nll_loss 5.268 | ppl 38.54 | wps 29814.2 | wpb 1852.1 | bsz 82.3 | num_updates 7883 | best_loss 6.619\n",
            "2021-02-24 11:31:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:31:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint19.pt (epoch 19 @ 7883 updates, score 6.619) (writing took 27.689014057999884 seconds)\n",
            "2021-02-24 11:31:59 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2021-02-24 11:31:59 | INFO | train | epoch 019 | loss 5.792 | nll_loss 4.45 | ppl 21.85 | wps 8551.4 | ups 3.61 | wpb 2365.6 | bsz 101.3 | num_updates 7883 | lr 0.000142467 | gnorm 1.816 | loss_scale 32 | train_wall 84 | wall 2174\n",
            "epoch 020:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:31:59 | INFO | fairseq.trainer | begin training epoch 20\n",
            "epoch 020: 100% 414/415 [01:26<00:00,  5.43it/s, loss=5.86, nll_loss=4.53, ppl=23.1, wps=8009.5, ups=3.38, wpb=2369.9, bsz=101.2, num_updates=8000, lr=0.000141421, gnorm=1.798, loss_scale=32, train_wall=205, wall=2201]2021-02-24 11:33:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.29it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.54it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  7.50it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  8.95it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 10.65it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 11.99it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  78% 14/18 [00:01<00:00, 12.99it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 13.77it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 14.62it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:33:27 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.544 | nll_loss 5.185 | ppl 36.39 | wps 29212.8 | wpb 1852.1 | bsz 82.3 | num_updates 8298 | best_loss 6.544\n",
            "2021-02-24 11:33:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:33:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint20.pt (epoch 20 @ 8298 updates, score 6.544) (writing took 28.01924247900024 seconds)\n",
            "2021-02-24 11:33:55 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2021-02-24 11:33:55 | INFO | train | epoch 020 | loss 5.631 | nll_loss 4.261 | ppl 19.17 | wps 8491.5 | ups 3.59 | wpb 2365.6 | bsz 101.3 | num_updates 8298 | lr 0.000138859 | gnorm 1.84 | loss_scale 32 | train_wall 85 | wall 2290\n",
            "epoch 021:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:33:55 | INFO | fairseq.trainer | begin training epoch 21\n",
            "epoch 021: 100% 414/415 [01:25<00:00,  5.38it/s]2021-02-24 11:35:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.23it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  17% 3/18 [00:00<00:01,  7.58it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.80it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  39% 7/18 [00:00<00:01, 10.18it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.92it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 13.14it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 14.07it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.59it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:35:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.474 | nll_loss 5.079 | ppl 33.81 | wps 30527.4 | wpb 1852.1 | bsz 82.3 | num_updates 8713 | best_loss 6.474\n",
            "2021-02-24 11:35:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:35:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint21.pt (epoch 21 @ 8713 updates, score 6.474) (writing took 27.59084632200029 seconds)\n",
            "2021-02-24 11:35:50 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2021-02-24 11:35:50 | INFO | train | epoch 021 | loss 5.475 | nll_loss 4.078 | ppl 16.89 | wps 8536.3 | ups 3.61 | wpb 2365.6 | bsz 101.3 | num_updates 8713 | lr 0.000135511 | gnorm 1.822 | loss_scale 32 | train_wall 84 | wall 2405\n",
            "epoch 022:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:35:50 | INFO | fairseq.trainer | begin training epoch 22\n",
            "epoch 022: 100% 414/415 [01:26<00:00,  4.96it/s, loss=5.48, nll_loss=4.084, ppl=16.96, wps=8970, ups=3.78, wpb=2376.1, bsz=101.2, num_updates=9000, lr=0.000133333, gnorm=1.827, loss_scale=32, train_wall=203, wall=2466]2021-02-24 11:37:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  6.12it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.28it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.59it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.94it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  67% 12/18 [00:00<00:00, 12.78it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  78% 14/18 [00:00<00:00, 13.68it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 14.33it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 15.18it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:37:17 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.423 | nll_loss 5.006 | ppl 32.13 | wps 29950.7 | wpb 1852.1 | bsz 82.3 | num_updates 9128 | best_loss 6.423\n",
            "2021-02-24 11:37:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:37:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint22.pt (epoch 22 @ 9128 updates, score 6.423) (writing took 27.67917937799939 seconds)\n",
            "2021-02-24 11:37:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2021-02-24 11:37:45 | INFO | train | epoch 022 | loss 5.334 | nll_loss 3.913 | ppl 15.06 | wps 8509.5 | ups 3.6 | wpb 2365.6 | bsz 101.3 | num_updates 9128 | lr 0.000132395 | gnorm 1.833 | loss_scale 32 | train_wall 85 | wall 2520\n",
            "epoch 023:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:37:45 | INFO | fairseq.trainer | begin training epoch 23\n",
            "epoch 023: 100% 414/415 [01:27<00:00,  5.17it/s]2021-02-24 11:39:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   6% 1/18 [00:00<00:02,  5.87it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  7.12it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  8.33it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.72it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 11.47it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.80it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.74it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 14.24it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:39:14 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.375 | nll_loss 4.958 | ppl 31.07 | wps 29996.5 | wpb 1852.1 | bsz 82.3 | num_updates 9543 | best_loss 6.375\n",
            "2021-02-24 11:39:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:39:42 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint23.pt (epoch 23 @ 9543 updates, score 6.375) (writing took 27.660670192999532 seconds)\n",
            "2021-02-24 11:39:42 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2021-02-24 11:39:42 | INFO | train | epoch 023 | loss 5.198 | nll_loss 3.753 | ppl 13.48 | wps 8423 | ups 3.56 | wpb 2365.6 | bsz 101.3 | num_updates 9543 | lr 0.000129484 | gnorm 1.816 | loss_scale 32 | train_wall 86 | wall 2637\n",
            "epoch 024:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:39:42 | INFO | fairseq.trainer | begin training epoch 24\n",
            "epoch 024: 100% 414/415 [01:26<00:00,  5.22it/s]2021-02-24 11:41:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  5.32it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  17% 3/18 [00:00<00:02,  6.39it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  28% 5/18 [00:00<00:01,  7.72it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  39% 7/18 [00:00<00:01,  9.16it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 9/18 [00:00<00:00, 10.75it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  61% 11/18 [00:00<00:00, 12.13it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  72% 13/18 [00:00<00:00, 13.21it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  83% 15/18 [00:01<00:00, 13.90it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  94% 17/18 [00:01<00:00, 14.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:41:10 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.325 | nll_loss 4.893 | ppl 29.72 | wps 29455.7 | wpb 1852.1 | bsz 82.3 | num_updates 9958 | best_loss 6.325\n",
            "2021-02-24 11:41:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:41:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint24.pt (epoch 24 @ 9958 updates, score 6.325) (writing took 26.262792051998986 seconds)\n",
            "2021-02-24 11:41:36 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2021-02-24 11:41:36 | INFO | train | epoch 024 | loss 5.077 | nll_loss 3.611 | ppl 12.22 | wps 8601.1 | ups 3.64 | wpb 2365.6 | bsz 101.3 | num_updates 9958 | lr 0.000126758 | gnorm 1.824 | loss_scale 32 | train_wall 85 | wall 2751\n",
            "epoch 025:   0% 0/415 [00:00<?, ?it/s]2021-02-24 11:41:36 | INFO | fairseq.trainer | begin training epoch 25\n",
            "epoch 025:  10% 41/415 [00:11<01:24,  4.41it/s]2021-02-24 11:41:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   6% 1/18 [00:00<00:03,  4.58it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  11% 2/18 [00:00<00:03,  5.16it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  22% 4/18 [00:00<00:02,  6.03it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 6/18 [00:00<00:01,  7.32it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  44% 8/18 [00:00<00:01,  8.80it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  56% 10/18 [00:00<00:00, 10.15it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  67% 12/18 [00:01<00:00, 11.48it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  78% 14/18 [00:01<00:00, 12.27it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  89% 16/18 [00:01<00:00, 12.93it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 18/18 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:41:49 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.336 | nll_loss 4.898 | ppl 29.81 | wps 25684.1 | wpb 1852.1 | bsz 82.3 | num_updates 10000 | best_loss 6.325\n",
            "2021-02-24 11:41:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:41:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/vsub2suben/checkpoints/checkpoint_last.pt (epoch 25 @ 10000 updates, score 6.336) (writing took 4.164694094999504 seconds)\n",
            "2021-02-24 11:41:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2021-02-24 11:41:53 | INFO | train | epoch 025 | loss 4.976 | nll_loss 3.495 | ppl 11.28 | wps 5785.1 | ups 2.43 | wpb 2378.2 | bsz 96.4 | num_updates 10000 | lr 0.000126491 | gnorm 1.868 | loss_scale 32 | train_wall 11 | wall 2768\n",
            "2021-02-24 11:41:53 | INFO | fairseq_cli.train | done training in 2767.9 seconds\n",
            "\n",
            "real\t46m14.980s\n",
            "user\t29m6.853s\n",
            "sys\t10m34.139s\n",
            "\n",
            "real\t1m1.502s\n",
            "user\t0m30.284s\n",
            "sys\t0m5.764s\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub\n",
            "2021-02-24 11:42:58 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='suben', srcdict=None, target_lang='vsub', task='translation', tensorboard_logdir=None, testpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train', user_dir=None, validpref='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev', workers=1)\n",
            "2021-02-24 11:43:09 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 11:43:17 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.suben: 42026 sents, 981713 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 11:43:17 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 11:43:18 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.suben: 1482 sents, 33337 tokens, 0.003% replaced by <unk>\n",
            "2021-02-24 11:43:18 | INFO | fairseq_cli.preprocess | [suben] Dictionary: 17280 types\n",
            "2021-02-24 11:43:19 | INFO | fairseq_cli.preprocess | [suben] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.suben: 1527 sents, 40254 tokens, 0.0174% replaced by <unk>\n",
            "2021-02-24 11:43:19 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 11:43:30 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/train.vsub: 42026 sents, 1164786 tokens, 0.0% replaced by <unk>\n",
            "2021-02-24 11:43:30 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 11:43:31 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/dev.vsub: 1482 sents, 39469 tokens, 0.0304% replaced by <unk>\n",
            "2021-02-24 11:43:31 | INFO | fairseq_cli.preprocess | [vsub] Dictionary: 17280 types\n",
            "2021-02-24 11:43:32 | INFO | fairseq_cli.preprocess | [vsub] /content/drive/MyDrive/nmt_models/Unchosen_2_2021/datasets/test.vsub: 1527 sents, 47028 tokens, 0.0638% replaced by <unk>\n",
            "2021-02-24 11:43:32 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized\n",
            "\n",
            "real\t0m34.772s\n",
            "user\t0m28.544s\n",
            "sys\t0m0.447s\n",
            "2021-02-24 11:43:33 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=1000, lr=[0.0002], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=10000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')\n",
            "2021-02-24 11:43:33 | INFO | fairseq.tasks.translation | [suben] dictionary: 17280 types\n",
            "2021-02-24 11:43:33 | INFO | fairseq.tasks.translation | [vsub] dictionary: 17280 types\n",
            "2021-02-24 11:43:33 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized/valid.suben-vsub.suben\n",
            "2021-02-24 11:43:33 | INFO | fairseq.data.data_utils | loaded 1482 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized/valid.suben-vsub.vsub\n",
            "2021-02-24 11:43:33 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized valid suben-vsub 1482 examples\n",
            "2021-02-24 11:43:34 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(17280, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(17280, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=17280, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-02-24 11:43:34 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-02-24 11:43:34 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
            "2021-02-24 11:43:34 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
            "2021-02-24 11:43:34 | INFO | fairseq_cli.train | num. model params: 52985856 (num. trained: 52985856)\n",
            "2021-02-24 11:43:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-02-24 11:43:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-02-24 11:43:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 11:43:37 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-02-24 11:43:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-02-24 11:43:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-02-24 11:43:37 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None\n",
            "2021-02-24 11:43:37 | INFO | fairseq.trainer | no existing checkpoint found /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint_last.pt\n",
            "2021-02-24 11:43:37 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-02-24 11:43:37 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized/train.suben-vsub.suben\n",
            "2021-02-24 11:43:37 | INFO | fairseq.data.data_utils | loaded 42026 examples from: /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized/train.suben-vsub.vsub\n",
            "2021-02-24 11:43:37 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/binarized train suben-vsub 42026 examples\n",
            "2021-02-24 11:43:37 | INFO | fairseq.trainer | NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster\n",
            "epoch 001:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:43:37 | INFO | fairseq.trainer | begin training epoch 1\n",
            "epoch 001:   9% 39/424 [00:08<01:19,  4.85it/s]2021-02-24 11:43:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
            "epoch 001: 100% 423/424 [01:25<00:00,  5.15it/s]2021-02-24 11:45:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:45:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.729 | nll_loss 10.198 | ppl 1174.6 | wps 32521.1 | wpb 1879.5 | bsz 70.6 | num_updates 423\n",
            "2021-02-24 11:45:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:45:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint1.pt (epoch 1 @ 423 updates, score 10.729) (writing took 29.138195399000324 seconds)\n",
            "2021-02-24 11:45:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-02-24 11:45:33 | INFO | train | epoch 001 | loss 12.467 | nll_loss 12.197 | ppl 4694.83 | wps 10023.3 | ups 3.65 | wpb 2750.5 | bsz 98.7 | num_updates 423 | lr 2.115e-05 | gnorm 2.133 | loss_scale 64 | train_wall 84 | wall 116\n",
            "epoch 002:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:45:33 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  40% 168/424 [00:36<00:52,  4.91it/s]2021-02-24 11:46:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
            "epoch 002:  42% 178/424 [00:38<00:47,  5.14it/s]2021-02-24 11:46:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
            "epoch 002: 100% 423/424 [01:27<00:00,  4.93it/s]2021-02-24 11:47:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 15/21 [00:00<00:00, 15.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.72it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:47:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.107 | nll_loss 9.364 | ppl 658.88 | wps 33424.8 | wpb 1879.5 | bsz 70.6 | num_updates 845 | best_loss 10.107\n",
            "2021-02-24 11:47:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:47:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint2.pt (epoch 2 @ 845 updates, score 10.107) (writing took 27.827988274000745 seconds)\n",
            "2021-02-24 11:47:30 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-02-24 11:47:30 | INFO | train | epoch 002 | loss 10.346 | nll_loss 9.705 | ppl 834.88 | wps 9941.1 | ups 3.61 | wpb 2753.4 | bsz 98.6 | num_updates 845 | lr 4.225e-05 | gnorm 1.647 | loss_scale 16 | train_wall 86 | wall 233\n",
            "epoch 003:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:47:30 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 423/424 [01:29<00:00,  5.11it/s, loss=11.2, nll_loss=10.704, ppl=1667.98, wps=10216, ups=3.72, wpb=2747.9, bsz=98.5, num_updates=1000, lr=5e-05, gnorm=1.889, loss_scale=16, train_wall=205, wall=269]2021-02-24 11:49:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:49:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.347 | nll_loss 8.494 | ppl 360.42 | wps 31823.2 | wpb 1879.5 | bsz 70.6 | num_updates 1269 | best_loss 9.347\n",
            "2021-02-24 11:49:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:49:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint3.pt (epoch 3 @ 1269 updates, score 9.347) (writing took 27.692173789999288 seconds)\n",
            "2021-02-24 11:49:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-02-24 11:49:29 | INFO | train | epoch 003 | loss 9.83 | nll_loss 9.084 | ppl 542.61 | wps 9755.2 | ups 3.55 | wpb 2747.1 | bsz 99.1 | num_updates 1269 | lr 6.345e-05 | gnorm 2.107 | loss_scale 16 | train_wall 88 | wall 353\n",
            "epoch 004:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:49:30 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 423/424 [01:26<00:00,  4.90it/s]2021-02-24 11:50:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 1/21 [00:00<00:04,  4.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 3/21 [00:00<00:03,  5.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  8.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:50:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.725 | nll_loss 7.759 | ppl 216.66 | wps 32854.7 | wpb 1879.5 | bsz 70.6 | num_updates 1693 | best_loss 8.725\n",
            "2021-02-24 11:50:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:51:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint4.pt (epoch 4 @ 1693 updates, score 8.725) (writing took 26.158076493000408 seconds)\n",
            "2021-02-24 11:51:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-02-24 11:51:24 | INFO | train | epoch 004 | loss 9.104 | nll_loss 8.26 | ppl 306.61 | wps 10162.8 | ups 3.7 | wpb 2747.1 | bsz 99.1 | num_updates 1693 | lr 8.465e-05 | gnorm 2.243 | loss_scale 16 | train_wall 85 | wall 467\n",
            "epoch 005:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:51:24 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 423/424 [01:29<00:00,  5.18it/s, loss=9.118, nll_loss=8.275, ppl=309.72, wps=10393.2, ups=3.78, wpb=2750.6, bsz=99.2, num_updates=2000, lr=0.0001, gnorm=2.235, loss_scale=16, train_wall=204, wall=534]2021-02-24 11:52:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 15.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.57it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:52:56 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.32 | nll_loss 7.307 | ppl 158.33 | wps 32116.7 | wpb 1879.5 | bsz 70.6 | num_updates 2117 | best_loss 8.32\n",
            "2021-02-24 11:52:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:53:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint5.pt (epoch 5 @ 2117 updates, score 8.32) (writing took 27.422913334999976 seconds)\n",
            "2021-02-24 11:53:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-02-24 11:53:23 | INFO | train | epoch 005 | loss 8.546 | nll_loss 7.624 | ppl 197.26 | wps 9797.6 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 2117 | lr 0.00010585 | gnorm 2.201 | loss_scale 16 | train_wall 88 | wall 586\n",
            "epoch 006:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:53:23 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 423/424 [01:28<00:00,  5.19it/s]2021-02-24 11:54:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.07it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.06it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 15/21 [00:00<00:00, 15.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.42it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:54:54 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.999 | nll_loss 6.917 | ppl 120.87 | wps 32783.2 | wpb 1879.5 | bsz 70.6 | num_updates 2541 | best_loss 7.999\n",
            "2021-02-24 11:54:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:55:21 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint6.pt (epoch 6 @ 2541 updates, score 7.999) (writing took 27.760650160000296 seconds)\n",
            "2021-02-24 11:55:21 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-02-24 11:55:21 | INFO | train | epoch 006 | loss 8.154 | nll_loss 7.175 | ppl 144.54 | wps 9848.8 | ups 3.59 | wpb 2747.1 | bsz 99.1 | num_updates 2541 | lr 0.00012705 | gnorm 2.166 | loss_scale 16 | train_wall 87 | wall 704\n",
            "epoch 007:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:55:21 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 423/424 [01:29<00:00,  5.04it/s]2021-02-24 11:56:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.08it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.21it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.69it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.05it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.70it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.28it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:56:52 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.731 | nll_loss 6.602 | ppl 97.13 | wps 32266.4 | wpb 1879.5 | bsz 70.6 | num_updates 2965 | best_loss 7.731\n",
            "2021-02-24 11:56:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:57:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint7.pt (epoch 7 @ 2965 updates, score 7.731) (writing took 27.810901640001248 seconds)\n",
            "2021-02-24 11:57:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-02-24 11:57:20 | INFO | train | epoch 007 | loss 7.818 | nll_loss 6.79 | ppl 110.7 | wps 9806 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 2965 | lr 0.00014825 | gnorm 1.966 | loss_scale 16 | train_wall 87 | wall 823\n",
            "epoch 008:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:57:20 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 423/424 [01:29<00:00,  5.34it/s, loss=8.014, nll_loss=7.016, ppl=129.41, wps=9179.6, ups=3.34, wpb=2745.1, bsz=99.3, num_updates=3000, lr=0.00015, gnorm=2.073, loss_scale=16, train_wall=207, wall=833]2021-02-24 11:58:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.83it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.88it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.21it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.73it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.05it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.30it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.46it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 11:58:51 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.501 | nll_loss 6.335 | ppl 80.73 | wps 31738.2 | wpb 1879.5 | bsz 70.6 | num_updates 3389 | best_loss 7.501\n",
            "2021-02-24 11:58:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 11:59:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint8.pt (epoch 8 @ 3389 updates, score 7.501) (writing took 27.904180077001 seconds)\n",
            "2021-02-24 11:59:19 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-02-24 11:59:19 | INFO | train | epoch 008 | loss 7.537 | nll_loss 6.469 | ppl 88.6 | wps 9808.5 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 3389 | lr 0.00016945 | gnorm 1.94 | loss_scale 16 | train_wall 87 | wall 942\n",
            "epoch 009:   0% 0/424 [00:00<?, ?it/s]2021-02-24 11:59:19 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 423/424 [01:29<00:00,  5.34it/s]2021-02-24 12:00:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.30it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.41it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.74it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.05it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.43it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.77it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.71it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:00:50 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.35 | nll_loss 6.181 | ppl 72.54 | wps 32167.6 | wpb 1879.5 | bsz 70.6 | num_updates 3813 | best_loss 7.35\n",
            "2021-02-24 12:00:50 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:01:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint9.pt (epoch 9 @ 3813 updates, score 7.35) (writing took 27.01131316899955 seconds)\n",
            "2021-02-24 12:01:17 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-02-24 12:01:17 | INFO | train | epoch 009 | loss 7.29 | nll_loss 6.184 | ppl 72.72 | wps 9888.7 | ups 3.6 | wpb 2747.1 | bsz 99.1 | num_updates 3813 | lr 0.00019065 | gnorm 1.844 | loss_scale 16 | train_wall 88 | wall 1060\n",
            "epoch 010:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:01:17 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 423/424 [01:28<00:00,  4.97it/s, loss=7.349, nll_loss=6.252, ppl=76.23, wps=10273.1, ups=3.73, wpb=2750.7, bsz=98.4, num_updates=4000, lr=0.0002, gnorm=1.866, loss_scale=16, train_wall=206, wall=1101]2021-02-24 12:02:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.99it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.80it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.07it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.86it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:02:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.17 | nll_loss 5.969 | ppl 62.64 | wps 32178.6 | wpb 1879.5 | bsz 70.6 | num_updates 4237 | best_loss 7.17\n",
            "2021-02-24 12:02:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:03:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint10.pt (epoch 10 @ 4237 updates, score 7.17) (writing took 27.669997428001807 seconds)\n",
            "2021-02-24 12:03:15 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-02-24 12:03:15 | INFO | train | epoch 010 | loss 7.064 | nll_loss 5.924 | ppl 60.7 | wps 9862.2 | ups 3.59 | wpb 2747.1 | bsz 99.1 | num_updates 4237 | lr 0.000194326 | gnorm 1.816 | loss_scale 16 | train_wall 87 | wall 1178\n",
            "epoch 011:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:03:15 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 423/424 [01:28<00:00,  4.88it/s]2021-02-24 12:04:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.07it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.43it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.22it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.88it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 15.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.12it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.45it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 21/21 [00:01<00:00, 17.36it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:04:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.047 | nll_loss 5.823 | ppl 56.59 | wps 31742.4 | wpb 1879.5 | bsz 70.6 | num_updates 4661 | best_loss 7.047\n",
            "2021-02-24 12:04:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:05:13 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint11.pt (epoch 11 @ 4661 updates, score 7.047) (writing took 27.83687973800079 seconds)\n",
            "2021-02-24 12:05:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2021-02-24 12:05:13 | INFO | train | epoch 011 | loss 6.837 | nll_loss 5.664 | ppl 50.7 | wps 9873.9 | ups 3.59 | wpb 2747.1 | bsz 99.1 | num_updates 4661 | lr 0.000185277 | gnorm 1.747 | loss_scale 16 | train_wall 87 | wall 1296\n",
            "epoch 012:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:05:13 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 423/424 [01:29<00:00,  5.18it/s, loss=6.813, nll_loss=5.635, ppl=49.71, wps=10260.7, ups=3.74, wpb=2746.7, bsz=100.5, num_updates=5000, lr=0.000178885, gnorm=1.755, loss_scale=16, train_wall=205, wall=1368]2021-02-24 12:06:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.14it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.27it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.74it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.01it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.29it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.69it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.77it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:06:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.935 | nll_loss 5.68 | ppl 51.28 | wps 31399.5 | wpb 1879.5 | bsz 70.6 | num_updates 5085 | best_loss 6.935\n",
            "2021-02-24 12:06:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:07:12 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint12.pt (epoch 12 @ 5085 updates, score 6.935) (writing took 27.772333236000122 seconds)\n",
            "2021-02-24 12:07:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2021-02-24 12:07:12 | INFO | train | epoch 012 | loss 6.631 | nll_loss 5.427 | ppl 43.01 | wps 9797.3 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 5085 | lr 0.000177384 | gnorm 1.679 | loss_scale 16 | train_wall 88 | wall 1415\n",
            "epoch 013:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:07:12 | INFO | fairseq.trainer | begin training epoch 13\n",
            "epoch 013: 100% 423/424 [01:29<00:00,  5.30it/s]2021-02-24 12:08:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.64it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.88it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.26it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.69it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.99it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.24it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.63it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.85it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.74it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.32it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:08:43 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.866 | nll_loss 5.602 | ppl 48.56 | wps 32408.9 | wpb 1879.5 | bsz 70.6 | num_updates 5509 | best_loss 6.866\n",
            "2021-02-24 12:08:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:09:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint13.pt (epoch 13 @ 5509 updates, score 6.866) (writing took 27.696470777000286 seconds)\n",
            "2021-02-24 12:09:10 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2021-02-24 12:09:10 | INFO | train | epoch 013 | loss 6.452 | nll_loss 5.221 | ppl 37.29 | wps 9811.9 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 5509 | lr 0.000170421 | gnorm 1.678 | loss_scale 16 | train_wall 88 | wall 1533\n",
            "epoch 014:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:09:10 | INFO | fairseq.trainer | begin training epoch 14\n",
            "epoch 014: 100% 423/424 [01:28<00:00,  5.31it/s]2021-02-24 12:10:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.93it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.25it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.59it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.04it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.33it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.61it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.11it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 15.36it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.32it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:10:40 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.772 | nll_loss 5.492 | ppl 45.02 | wps 33048.5 | wpb 1879.5 | bsz 70.6 | num_updates 5933 | best_loss 6.772\n",
            "2021-02-24 12:10:40 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:11:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint14.pt (epoch 14 @ 5933 updates, score 6.772) (writing took 27.494833983999342 seconds)\n",
            "2021-02-24 12:11:08 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2021-02-24 12:11:08 | INFO | train | epoch 014 | loss 6.285 | nll_loss 5.029 | ppl 32.65 | wps 9895.3 | ups 3.6 | wpb 2747.1 | bsz 99.1 | num_updates 5933 | lr 0.000164219 | gnorm 1.647 | loss_scale 16 | train_wall 87 | wall 1651\n",
            "epoch 015:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:11:08 | INFO | fairseq.trainer | begin training epoch 15\n",
            "epoch 015: 100% 423/424 [01:28<00:00,  5.09it/s, loss=6.379, nll_loss=5.136, ppl=35.17, wps=9173.3, ups=3.34, wpb=2744.6, bsz=98, num_updates=6000, lr=0.000163299, gnorm=1.663, loss_scale=16, train_wall=207, wall=1668]2021-02-24 12:12:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.11it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.41it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.69it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.14it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.38it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.21it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.60it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.66it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:12:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.616 | nll_loss 5.301 | ppl 39.43 | wps 32463.1 | wpb 1879.5 | bsz 70.6 | num_updates 6357 | best_loss 6.616\n",
            "2021-02-24 12:12:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:13:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint15.pt (epoch 15 @ 6357 updates, score 6.616) (writing took 24.665844823999578 seconds)\n",
            "2021-02-24 12:13:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2021-02-24 12:13:03 | INFO | train | epoch 015 | loss 6.112 | nll_loss 4.829 | ppl 28.42 | wps 10170.1 | ups 3.7 | wpb 2747.1 | bsz 99.1 | num_updates 6357 | lr 0.000158648 | gnorm 1.675 | loss_scale 16 | train_wall 87 | wall 1766\n",
            "epoch 016:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:13:03 | INFO | fairseq.trainer | begin training epoch 16\n",
            "epoch 016: 100% 423/424 [01:29<00:00,  5.09it/s]2021-02-24 12:14:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.29it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.25it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.71it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.20it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.55it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.14it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.51it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.54it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 15.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:14:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.566 | nll_loss 5.246 | ppl 37.95 | wps 30897 | wpb 1879.5 | bsz 70.6 | num_updates 6781 | best_loss 6.566\n",
            "2021-02-24 12:14:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:14:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint16.pt (epoch 16 @ 6781 updates, score 6.566) (writing took 25.083122353000363 seconds)\n",
            "2021-02-24 12:14:59 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2021-02-24 12:14:59 | INFO | train | epoch 016 | loss 5.926 | nll_loss 4.615 | ppl 24.51 | wps 9990.8 | ups 3.64 | wpb 2747.1 | bsz 99.1 | num_updates 6781 | lr 0.000153608 | gnorm 1.673 | loss_scale 16 | train_wall 88 | wall 1882\n",
            "epoch 017:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:14:59 | INFO | fairseq.trainer | begin training epoch 17\n",
            "epoch 017: 100% 423/424 [01:28<00:00,  5.11it/s, loss=5.953, nll_loss=4.646, ppl=25.04, wps=10466.1, ups=3.81, wpb=2747.8, bsz=100, num_updates=7000, lr=0.000151186, gnorm=1.675, loss_scale=16, train_wall=206, wall=1930]2021-02-24 12:16:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.54it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.89it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.23it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.56it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.81it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.08it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.60it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.93it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.91it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.46it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:16:30 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.432 | nll_loss 5.065 | ppl 33.48 | wps 32526.3 | wpb 1879.5 | bsz 70.6 | num_updates 7205 | best_loss 6.432\n",
            "2021-02-24 12:16:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:16:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint17.pt (epoch 17 @ 7205 updates, score 6.432) (writing took 26.25865974199951 seconds)\n",
            "2021-02-24 12:16:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2021-02-24 12:16:56 | INFO | train | epoch 017 | loss 5.777 | nll_loss 4.442 | ppl 21.73 | wps 9960.1 | ups 3.63 | wpb 2747.1 | bsz 99.1 | num_updates 7205 | lr 0.000149019 | gnorm 1.703 | loss_scale 16 | train_wall 87 | wall 1999\n",
            "epoch 018:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:16:56 | INFO | fairseq.trainer | begin training epoch 18\n",
            "epoch 018: 100% 423/424 [01:29<00:00,  5.13it/s]2021-02-24 12:18:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.44it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.79it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  9.27it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.50it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.66it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.88it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.34it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  71% 15/21 [00:00<00:00, 15.55it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.40it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:18:27 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.357 | nll_loss 4.966 | ppl 31.25 | wps 32718.9 | wpb 1879.5 | bsz 70.6 | num_updates 7629 | best_loss 6.357\n",
            "2021-02-24 12:18:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:18:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint18.pt (epoch 18 @ 7629 updates, score 6.357) (writing took 27.84519759199975 seconds)\n",
            "2021-02-24 12:18:55 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2021-02-24 12:18:55 | INFO | train | epoch 018 | loss 5.63 | nll_loss 4.272 | ppl 19.32 | wps 9810.7 | ups 3.57 | wpb 2747.1 | bsz 99.1 | num_updates 7629 | lr 0.000144819 | gnorm 1.744 | loss_scale 16 | train_wall 88 | wall 2118\n",
            "epoch 019:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:18:55 | INFO | fairseq.trainer | begin training epoch 19\n",
            "epoch 019: 100% 423/424 [01:28<00:00,  5.25it/s, loss=5.601, nll_loss=4.238, ppl=18.87, wps=10314, ups=3.75, wpb=2747.7, bsz=99.4, num_updates=8000, lr=0.000141421, gnorm=1.735, loss_scale=16, train_wall=205, wall=2197]2021-02-24 12:20:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.16it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.60it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.90it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.07it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.29it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.57it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.97it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 15.19it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.10it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:20:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.352 | nll_loss 4.971 | ppl 31.35 | wps 32371.5 | wpb 1879.5 | bsz 70.6 | num_updates 8053 | best_loss 6.352\n",
            "2021-02-24 12:20:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:20:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint19.pt (epoch 19 @ 8053 updates, score 6.352) (writing took 27.848820467999758 seconds)\n",
            "2021-02-24 12:20:53 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2021-02-24 12:20:53 | INFO | train | epoch 019 | loss 5.496 | nll_loss 4.117 | ppl 17.35 | wps 9828.8 | ups 3.58 | wpb 2747.1 | bsz 99.1 | num_updates 8053 | lr 0.000140955 | gnorm 1.75 | loss_scale 16 | train_wall 87 | wall 2236\n",
            "epoch 020:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:20:53 | INFO | fairseq.trainer | begin training epoch 20\n",
            "epoch 020: 100% 423/424 [01:29<00:00,  4.99it/s]2021-02-24 12:22:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   5% 1/21 [00:00<00:04,  4.89it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  14% 3/21 [00:00<00:03,  5.90it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.34it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  8.71it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  43% 9/21 [00:00<00:01,  9.96it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 11.22it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 12.84it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.21it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.36it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:22:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.252 | nll_loss 4.838 | ppl 28.59 | wps 31218.9 | wpb 1879.5 | bsz 70.6 | num_updates 8477 | best_loss 6.252\n",
            "2021-02-24 12:22:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:22:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint20.pt (epoch 20 @ 8477 updates, score 6.252) (writing took 25.84657449899896 seconds)\n",
            "2021-02-24 12:22:50 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2021-02-24 12:22:50 | INFO | train | epoch 020 | loss 5.376 | nll_loss 3.978 | ppl 15.75 | wps 9992.3 | ups 3.64 | wpb 2747.1 | bsz 99.1 | num_updates 8477 | lr 0.000137385 | gnorm 1.786 | loss_scale 16 | train_wall 87 | wall 2353\n",
            "epoch 021:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:22:50 | INFO | fairseq.trainer | begin training epoch 21\n",
            "epoch 021: 100% 423/424 [01:28<00:00,  5.01it/s]2021-02-24 12:24:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.69it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.82it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.08it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.35it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.70it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.08it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.59it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.85it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.72it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:24:20 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.197 | nll_loss 4.772 | ppl 27.32 | wps 31329 | wpb 1879.5 | bsz 70.6 | num_updates 8901 | best_loss 6.197\n",
            "2021-02-24 12:24:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:24:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint21.pt (epoch 21 @ 8901 updates, score 6.197) (writing took 27.751780899998266 seconds)\n",
            "2021-02-24 12:24:48 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2021-02-24 12:24:48 | INFO | train | epoch 021 | loss 5.274 | nll_loss 3.859 | ppl 14.51 | wps 9847.9 | ups 3.58 | wpb 2747.1 | bsz 99.1 | num_updates 8901 | lr 0.000134073 | gnorm 1.822 | loss_scale 16 | train_wall 87 | wall 2471\n",
            "epoch 022:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:24:48 | INFO | fairseq.trainer | begin training epoch 22\n",
            "epoch 022: 100% 423/424 [01:28<00:00,  5.07it/s, loss=5.326, nll_loss=3.919, ppl=15.12, wps=9200.4, ups=3.35, wpb=2748.5, bsz=98.7, num_updates=9000, lr=0.000133333, gnorm=1.801, loss_scale=16, train_wall=208, wall=2495]2021-02-24 12:26:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.52it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.98it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  9.47it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.88it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  43% 9/21 [00:00<00:00, 12.07it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 13.25it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.67it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  71% 15/21 [00:00<00:00, 15.72it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.32it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.54it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:26:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.178 | nll_loss 4.739 | ppl 26.7 | wps 33390.8 | wpb 1879.5 | bsz 70.6 | num_updates 9325 | best_loss 6.178\n",
            "2021-02-24 12:26:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:26:46 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint22.pt (epoch 22 @ 9325 updates, score 6.178) (writing took 27.549665964999804 seconds)\n",
            "2021-02-24 12:26:46 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2021-02-24 12:26:46 | INFO | train | epoch 022 | loss 5.148 | nll_loss 3.713 | ppl 13.11 | wps 9857.3 | ups 3.59 | wpb 2747.1 | bsz 99.1 | num_updates 9325 | lr 0.000130989 | gnorm 1.774 | loss_scale 16 | train_wall 87 | wall 2589\n",
            "epoch 023:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:26:46 | INFO | fairseq.trainer | begin training epoch 23\n",
            "epoch 023: 100% 423/424 [01:28<00:00,  5.07it/s]2021-02-24 12:28:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  5.27it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  6.43it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  24% 5/21 [00:00<00:02,  7.66it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 7/21 [00:00<00:01,  9.02it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 10.41it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 13.38it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  71% 15/21 [00:01<00:00, 14.71it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 15.69it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.25it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:28:16 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.139 | nll_loss 4.693 | ppl 25.87 | wps 31593.2 | wpb 1879.5 | bsz 70.6 | num_updates 9749 | best_loss 6.139\n",
            "2021-02-24 12:28:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:28:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint23.pt (epoch 23 @ 9749 updates, score 6.139) (writing took 27.67121552900062 seconds)\n",
            "2021-02-24 12:28:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2021-02-24 12:28:44 | INFO | train | epoch 023 | loss 5.046 | nll_loss 3.594 | ppl 12.07 | wps 9887.1 | ups 3.6 | wpb 2747.1 | bsz 99.1 | num_updates 9749 | lr 0.000128109 | gnorm 1.802 | loss_scale 16 | train_wall 87 | wall 2707\n",
            "epoch 024:   0% 0/424 [00:00<?, ?it/s]2021-02-24 12:28:44 | INFO | fairseq.trainer | begin training epoch 24\n",
            "epoch 024:  59% 250/424 [00:53<00:34,  4.98it/s]2021-02-24 12:29:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   5% 1/21 [00:00<00:03,  6.16it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  14% 3/21 [00:00<00:02,  7.53it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  24% 5/21 [00:00<00:01,  8.80it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 7/21 [00:00<00:01, 10.21it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  43% 9/21 [00:00<00:01, 11.43it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  52% 11/21 [00:00<00:00, 12.70it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  62% 13/21 [00:00<00:00, 14.15it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  71% 15/21 [00:00<00:00, 15.43it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  81% 17/21 [00:01<00:00, 16.18it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  90% 19/21 [00:01<00:00, 16.56it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-02-24 12:29:39 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.133 | nll_loss 4.679 | ppl 25.62 | wps 32301.1 | wpb 1879.5 | bsz 70.6 | num_updates 10000 | best_loss 6.133\n",
            "2021-02-24 12:29:39 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-02-24 12:29:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vsub/checkpoints/checkpoint_best.pt (epoch 24 @ 10000 updates, score 6.133) (writing took 9.055915320999702 seconds)\n",
            "2021-02-24 12:29:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2021-02-24 12:29:48 | INFO | train | epoch 024 | loss 4.938 | nll_loss 3.469 | ppl 11.07 | wps 10735.5 | ups 3.92 | wpb 2735.6 | bsz 97.3 | num_updates 10000 | lr 0.000126491 | gnorm 1.851 | loss_scale 16 | train_wall 52 | wall 2771\n",
            "2021-02-24 12:29:48 | INFO | fairseq_cli.train | done training in 2771.1 seconds\n",
            "\n",
            "real\t46m18.599s\n",
            "user\t29m35.137s\n",
            "sys\t10m18.988s\n",
            "\n",
            "real\t0m48.144s\n",
            "user\t0m25.769s\n",
            "sys\t0m6.061s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTHRlE4HpZvY"
      },
      "source": [
        "#Extract prediction from the output and evaluate prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ROOTDIR='/content/drive/MyDrive/nmt_models/CInN_2021'\n",
        "DATDIR='/content/drive/MyDrive/nmt_models/CInN_2021/datasets'\n",
        "\n",
        "comb = []\n",
        "for src in ['wovi', 'vi', 'subvi']:\n",
        "    for trg in ['en', 'suben']:\n",
        "        comb.append(f'{src}2{trg}')\n",
        "        comb.append(f'{trg}2{src}')\n",
        "\n",
        "comb = sorted(comb)"
      ],
      "metadata": {
        "id": "2IcJ2WFziCQW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLEU"
      ],
      "metadata": {
        "id": "wqCJUjzEZNuq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXQ8zbYVpi7Y",
        "outputId": "942f89ea-0d7f-418f-d9e2-3ad56955f29d"
      },
      "source": [
        "for com in comb:\n",
        "    DIRT = os.path.join(ROOTDIR, com)\n",
        "    print(DIRT)\n",
        "    result_file = os.path.join(DIRT, 'result.txt')\n",
        "    if os.path.exists(result_file):\n",
        "        _, TRG = com.split('2')\n",
        "        extract(result_file, os.path.join(DIRT,f'predict.{TRG}'))\n",
        "        if TRG is 'subvi':\n",
        "            join_pieces(os.path.join(DIRT, 'predict.subvi'), os.path.join(DIRT, 'predict.wovi'))\n",
        "        if os.path.exists(os.path.join(DIRT, 'predict.wovi')):\n",
        "            !cat $DIRT/predict.wovi | sed 's/_/ /g' > $DIRT/predict.vi\n",
        "        if TRG is 'suben':\n",
        "            join_pieces(os.path.join(DIRT, 'predict.suben'), os.path.join(DIRT, 'predict.en'))\n",
        "        print(f'-----{com}------')\n",
        "        TRGX = TRG[-2:]\n",
        "        !/content/drive/MyDrive/nmt_models/CInN_2021/mosesdecoder/scripts/generic/multi-bleu.perl -lc $DATDIR/test.$TRGX < $DIRT/predict.$TRGX\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2subvi\n",
            "-----en2subvi------\n",
            "BLEU = 18.95, 49.8/25.4/13.8/7.8 (BP=0.986, ratio=0.986, hyp_len=42690, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2vi\n",
            "-----en2vi------\n",
            "BLEU = 13.38, 44.7/19.9/9.6/4.9 (BP=0.938, ratio=0.940, hyp_len=40683, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/en2wovi\n",
            "-----en2wovi------\n",
            "BLEU = 20.98, 51.1/27.1/15.3/9.1 (BP=1.000, ratio=1.001, hyp_len=43327, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2subvi\n",
            "-----suben2subvi------\n",
            "BLEU = 19.68, 53.8/28.4/15.8/9.2 (BP=0.907, ratio=0.911, hyp_len=39426, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2vi\n",
            "-----suben2vi------\n",
            "BLEU = 10.22, 43.5/17.5/7.7/3.5 (BP=0.851, ratio=0.861, hyp_len=37285, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/suben2wovi\n",
            "-----suben2wovi------\n",
            "BLEU = 18.77, 53.1/27.7/15.1/8.6 (BP=0.899, ratio=0.904, hyp_len=39127, ref_len=43286)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/subvi2en\n",
            "-----subvi2en------\n",
            "BLEU = 13.40, 48.1/18.9/8.8/4.3 (BP=0.986, ratio=0.987, hyp_len=31089, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/subvi2suben\n",
            "-----subvi2suben------\n",
            "BLEU = 13.42, 48.9/19.4/9.1/4.4 (BP=0.959, ratio=0.960, hyp_len=30251, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vi2en\n",
            "-----vi2en------\n",
            "BLEU = 14.40, 48.2/19.9/9.6/4.8 (BP=0.992, ratio=0.992, hyp_len=31252, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/vi2suben\n",
            "-----vi2suben------\n",
            "BLEU = 12.42, 45.7/17.5/7.9/3.8 (BP=1.000, ratio=1.002, hyp_len=31571, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/wovi2en\n",
            "-----wovi2en------\n",
            "BLEU = 13.62, 45.6/18.8/9.0/4.5 (BP=1.000, ratio=1.100, hyp_len=34664, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "/content/drive/MyDrive/nmt_models/Unchosen_2_2021/wovi2suben\n",
            "-----wovi2suben------\n",
            "BLEU = 11.54, 45.4/16.6/7.3/3.5 (BP=0.982, ratio=0.982, hyp_len=30957, ref_len=31513)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TER"
      ],
      "metadata": {
        "id": "TtGiN1HzZSUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convert to TER format"
      ],
      "metadata": {
        "id": "TiaL9dfTJeKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ter(filename):\n",
        "    \"\"\"\n",
        "    converts a file with filename into a format acceptable by tercom\n",
        "    output will have the name = filename + \".ter\"\n",
        "    \"\"\"\n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    lines = [line + f\"  ({str(i)})\" for i, line in enumerate(lines)]\n",
        "    ofile = open(filename + \".ter\", 'w')\n",
        "    for line in lines:\n",
        "        ofile.write(line + '\\n')\n",
        "    ofile.close()"
      ],
      "metadata": {
        "id": "rXBndt-CL-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References"
      ],
      "metadata": {
        "id": "RCvhUXtK9EVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATDIR='datasets'\n",
        "for ext in ['en', 'vi']:\n",
        "    filename = f\"{DATDIR}/test.{ext}\"\n",
        "    convert_ter(filename)"
      ],
      "metadata": {
        "id": "DxGCXNZSK0kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head $DATDIR/test.vi.ter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne4hv4nFNltR",
        "outputId": "078e7e36-8733-4eb3-f31f-e1fec46c9a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bệnh An-dai-mơ  (0)\n",
            "chúng ta ai cũng có đôi lúc quên thứ này thứ nọ .  (1)\n",
            "có thể bạn quên gởi thiệp mừng sinh nhật của ai đó hoặc quên trả lại sách quá hạn cho thư viện .  (2)\n",
            "cuộc sống chúng ta phải có lúc quên và triệu chứng này thường hay xảy ra nhiều hơn khi con người có tuổi .  (3)\n",
            "chúng ta ai cũng có đôi lúc quên thứ này thứ nọ .  (4)\n",
            "có thể bạn quên gởi thiệp mừng sinh nhật của ai đó hoặc quên trả lại sách quá hạn cho thư viện .  (5)\n",
            "cuộc sống chúng ta phải có lúc quên và triệu chứng này thường hay xảy ra nhiều hơn khi con người có tuổi .  (6)\n",
            "nhưng bệnh An-dai-mơ , xảy ra ở một số người già , khác xa với chứng bệnh ngày nào cũng quên .  (7)\n",
            "đây là bệnh thường ảnh hưởng đến não , và dần dần nó làm cho người ta khó nhớ nhiều hơn thậm chí đến những chuyện dễ dàng , cơ bản nhất , chẳng hạn như cột dây giày ra làm sao .  (8)\n",
            "cuối cùng thì người bệnh có thể khó nhớ nổi tên hoặc khuôn mặt của người thân trong gia đình - hoặc thậm chí là không nhớ ra mình là ai nữa .  (9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head $DATDIR/test.en.ter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwd58EPmN8j1",
        "outputId": "ee419f18-0971-45a5-f245-05e1303ca772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alzheimer disease  (0)\n",
            "we all forget things once in a while .  (1)\n",
            "maybe you 've forgotten to send a card for someone 's birthday or to return an overdue library book .  (2)\n",
            "forgetting stuff is a part of life and it often becomes more common as people age .  (3)\n",
            "we all forget things once in a while .  (4)\n",
            "maybe you 've forgotten to send a card for someone 's birthday or to return an overdue library book .  (5)\n",
            "forgetting stuff is a part of life and it often becomes more common as people age .  (6)\n",
            "but Alzheimer disease , which affects some older people , is different from everyday forgetting .  (7)\n",
            "it is a condition that permanently affects the brain , and over time , makes it harder to remember even basic stuff , like how to tie a shoe .  (8)\n",
            "eventually , the person may have trouble remembering the names and faces of family members - or even who he or she is .  (9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis"
      ],
      "metadata": {
        "id": "5x3wOJs79I5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comb = []\n",
        "for src in ['wovi', 'vi', 'subvi']:\n",
        "    for trg in ['en', 'suben']:\n",
        "        comb.append(f'{src}2{trg}')\n",
        "        comb.append(f'{trg}2{src}')\n",
        "\n",
        "comb = sorted(comb)\n",
        "for com in comb:\n",
        "    print(com)\n",
        "    TRGX = com[-2:]\n",
        "    filename = f\"{com}/predict.{TRGX}\"\n",
        "    convert_ter(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9bir0AQJiS5",
        "outputId": "12fc6fd3-5fc0-4e48-db4c-97a0a55a4468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en2subvi\n",
            "en2vi\n",
            "en2wovi\n",
            "suben2subvi\n",
            "suben2vi\n",
            "suben2wovi\n",
            "subvi2en\n",
            "subvi2suben\n",
            "vi2en\n",
            "vi2suben\n",
            "wovi2en\n",
            "wovi2suben\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head en2subvi/predict.vi.ter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuaQjxBVPVhE",
        "outputId": "a79f5451-0a25-4d57-e1c9-b15782e6b1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bệnh Kawata  (0)\n",
            "tất cả chúng ta quên mọi thứ trong một lúc .  (1)\n",
            "có lẽ bạn đã quên lấy thẻ cho một người nào đó để trở về một quyển sách hay một quyển sách .  (2)\n",
            "quên là một phần của cuộc sống và thường trở nên phổ biến hơn khi mọi người trưởng thành .  (3)\n",
            "tất cả chúng ta quên mọi thứ trong một lúc .  (4)\n",
            "có lẽ bạn đã quên lấy thẻ cho một người nào đó để trở về một quyển sách hay một quyển sách .  (5)\n",
            "quên là một phần của cuộc sống và thường trở nên phổ biến hơn khi mọi người trưởng thành .  (6)\n",
            "nhưng bệnh Kronronen , bệnh này ảnh hưởng đến một số người lớn tuổi hơn , nhưng không phải mỗi ngày .  (7)\n",
            "bệnh này là bệnh ảnh hưởng đến não , và thời gian , làm cho việc đó càng khó mà nhớ đến những thứ cơ bản hơn , chẳng hạn như việc giày dép dép dép xôn , như thế nào .  (8)\n",
            "cuối cùng , người ta có thể gặp rắc rối về những cái tên và khuôn mặt của các thành viên trong gia đình - thậm chí là người ấy hay thậm chí là bé .  (9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluate with tercom"
      ],
      "metadata": {
        "id": "-BTAF_r987ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for com in comb:\n",
        "    print(com)\n",
        "    TRGX = com[-2:]\n",
        "    !java -jar tercom.7.25.jar -r \"$DATDIR\"/test.\"$TRGX\".ter -h \"$com\"/predict.\"$TRGX\".ter > \"$com\"/ter.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB4FqZFHZgrq",
        "outputId": "83ab2387-5d63-4c1a-ec04-f17f524d4c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en2subvi\n",
            "en2vi\n",
            "en2wovi\n",
            "suben2subvi\n",
            "suben2vi\n",
            "suben2wovi\n",
            "subvi2en\n",
            "subvi2suben\n",
            "vi2en\n",
            "vi2suben\n",
            "wovi2en\n",
            "wovi2suben\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for com in comb:\n",
        "    print(com)\n",
        "    TRGX = com[-2:]\n",
        "    !tail -4 \"$com\"/ter.txt | head -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjoJd8GTQhdJ",
        "outputId": "56a592f2-e69b-415b-bfbe-58ac1d3f756b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en2subvi\n",
            "Total TER: 0.6978930832139721 (30209.0/43286.0)\n",
            "en2vi\n",
            "Total TER: 0.7513514762278797 (32523.0/43286.0)\n",
            "en2wovi\n",
            "Total TER: 0.683107702259391 (29569.0/43286.0)\n",
            "suben2subvi\n",
            "Total TER: 0.6525897518828259 (28248.0/43286.0)\n",
            "suben2vi\n",
            "Total TER: 0.7705262671533522 (33353.0/43286.0)\n",
            "suben2wovi\n",
            "Total TER: 0.6584576999491752 (28502.0/43286.0)\n",
            "subvi2en\n",
            "Total TER: 0.7151017040586425 (22535.0/31513.0)\n",
            "subvi2suben\n",
            "Total TER: 0.706438612636055 (22262.0/31513.0)\n",
            "vi2en\n",
            "Total TER: 0.708945514549551 (22341.0/31513.0)\n",
            "vi2suben\n",
            "Total TER: 0.740583251356583 (23338.0/31513.0)\n",
            "wovi2en\n",
            "Total TER: 0.7640021578396218 (24076.0/31513.0)\n",
            "wovi2suben\n",
            "Total TER: 0.7484530193888237 (23586.0/31513.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CollGram"
      ],
      "metadata": {
        "id": "Si4y0NDr9V-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from itertools import chain\n",
        "from math import log, sqrt\n",
        "def collgram(reference_corpus, N, learner_corpus):\n",
        "    \"\"\"\n",
        "    computes MI score and t-score \n",
        "    input:\n",
        "    reference_corpus(str) - link to the reference corpus\n",
        "    N(int) - size of the reference corpus\n",
        "    learner_corpus(str) - link to the learner corpus\n",
        "    output:\n",
        "    scores of CollGram technique: MI score(float) and t-score(float)\n",
        "    \"\"\"\n",
        "    reference_corpus = open(reference_corpus, 'r').read()\n",
        "    learner_corpus = open(learner_corpus, 'r').read()\n",
        "    meaning_phrases = re.split(\"\\n|(?<!\\d)[\\?\\!,.](?!\\d)\", learner_corpus)\n",
        "    bigrams = [b for l in meaning_phrases for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "    dictionary = set(chain.from_iterable(bigrams))\n",
        "    counts = {token: reference_corpus.count(token) for token in dictionary}\n",
        "    Os = [reference_corpus.count(f\"{i} {j}\") for i,j in bigrams]\n",
        "    Es = [counts[i]*counts[j]/N for i,j in bigrams]\n",
        "    scores = [(log(O/E, 2), (O-E)/sqrt(O)) for O,E in zip(Os, Es) if O != 0 and E != 0]\n",
        "    meanMI = sum([score[0] for score in scores])/len(scores)\n",
        "    mean_t_score = sum([score[1] for score in scores])/len(scores)\n",
        "    return meanMI, mean_t_score"
      ],
      "metadata": {
        "id": "s_rR5Xza9Ysk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ns = dict()\n",
        "for ext in ['en', 'vi']:\n",
        "    reference_corpus = f\"{DATDIR}/train.{ext}\"\n",
        "    ref = open(reference_corpus, 'r').read()\n",
        "    N = len(re.findall(r'\\w+', ref))\n",
        "    Ns[ext] = N\n",
        "print(Ns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqnM-k0gg5q1",
        "outputId": "574ab4b8-086d-4674-e11b-d19e557dea65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'en': 724231, 'vi': 1016870}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for com in comb:\n",
        "    print(com)\n",
        "    TRGX = com[-2:]\n",
        "    reference_corpus = f\"{DATDIR}/train.{TRGX}\"\n",
        "    learner_corpus = f\"{com}/predict.{TRGX}\"\n",
        "    x = collgram(reference_corpus, Ns[TRGX], learner_corpus)\n",
        "    print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-sM55OJfxvK",
        "outputId": "258709b5-849a-4f09-9a43-c4cccb309094"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en2subvi\n",
            "(3.129081517944966, -18.451083929716795)\n",
            "en2vi\n",
            "(3.7010325131848165, -16.200660012402288)\n",
            "en2wovi\n",
            "(3.192994583836798, -14.276648951647095)\n",
            "suben2subvi\n",
            "(3.0802239506053124, -16.55942610116526)\n",
            "suben2vi\n",
            "(3.5691579008416285, -14.543163205972936)\n",
            "suben2wovi\n",
            "(3.1749692415049204, -18.433742414542586)\n",
            "subvi2en\n",
            "(1.0984190794823483, -55.53341489092003)\n",
            "subvi2suben\n",
            "(0.960115998342372, -70.5831189936073)\n",
            "vi2en\n",
            "(1.112369208137499, -60.700725964227445)\n",
            "vi2suben\n",
            "(1.023936791412845, -71.66690922653581)\n",
            "wovi2en\n",
            "(1.1099403356901925, -59.15740899245958)\n",
            "wovi2suben\n",
            "(1.1056679082333498, -60.73035430479805)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfw3ClWXlk1r"
      },
      "source": [
        "#Tokenize sentences into subtokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDcnLCWl2uuG",
        "outputId": "9f7b4319-fb5c-499f-f36e-0627b6d79195"
      },
      "source": [
        "!pip install subword-nmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRiS7xaL2z3K"
      },
      "source": [
        "!subword-nmt learn-bpe -s 10000 < $DATDIR/train.wovi > $DATDIR/vi.bpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNSfarFOK-da"
      },
      "source": [
        "!subword-nmt learn-bpe -s 10000 < $DATDIR/train.vi > $DATDIR/vsub.bpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xQ-lbcLIWH"
      },
      "source": [
        "!subword-nmt apply-bpe -c $DATDIR/vsub.bpe < $DATDIR/test.vi > $DATDIR/test.vsub\n",
        "!subword-nmt apply-bpe -c $DATDIR/vsub.bpe < $DATDIR/dev.vi > $DATDIR/dev.vsub\n",
        "!subword-nmt apply-bpe -c $DATDIR/vsub.bpe < $DATDIR/train.vi > $DATDIR/train.vsub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etDY5g0kPGaa",
        "outputId": "9f5c04a4-a58e-4403-9a71-fce49f1e0672"
      },
      "source": [
        "!head -50 $DATDIR/test.vsub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bệnh An@@ -@@ da@@ i-@@ mơ\n",
            "chúng ta ai cũng có đôi lúc quên thứ này thứ nọ .\n",
            "có thể bạn quên gởi thiệp mừng sinh nhật của ai đó hoặc quên trả lại sách quá hạn cho thư viện .\n",
            "cuộc sống chúng ta phải có lúc quên và triệu chứng này thường hay xảy ra nhiều hơn khi con người có tuổi .\n",
            "chúng ta ai cũng có đôi lúc quên thứ này thứ nọ .\n",
            "có thể bạn quên gởi thiệp mừng sinh nhật của ai đó hoặc quên trả lại sách quá hạn cho thư viện .\n",
            "cuộc sống chúng ta phải có lúc quên và triệu chứng này thường hay xảy ra nhiều hơn khi con người có tuổi .\n",
            "nhưng bệnh An@@ -@@ da@@ i-@@ mơ , xảy ra ở một số người già , khác xa với chứng bệnh ngày nào cũng quên .\n",
            "đây là bệnh thường ảnh hưởng đến não , và dần dần nó làm cho người ta khó nhớ nhiều hơn thậm chí đến những chuyện dễ dàng , cơ bản nhất , chẳng hạn như cột dây giày ra làm sao .\n",
            "cuối cùng thì người bệnh có thể khó nhớ nổi tên hoặc khuôn mặt của người thân trong gia đình - hoặc thậm chí là không nhớ ra mình là ai nữa .\n",
            "việc này có thể rất đáng buồn đối với bản thân bệnh nhân và cả gia đình .\n",
            "điều quan trọng nên biết là chứng bệnh này không xảy ra ở trẻ nhỏ .\n",
            "đối tượng thường mắc bệnh là người già trên 65 tuổi .\n",
            "các nhà nghiên cứu cho biết có nhiều loại thuốc dường như đã làm chậm lại quá trình diễn tiến bệnh .\n",
            "và người ta cũng hy vọng một ngày nào đó sẽ chữa lành .\n",
            "não bị ảnh hưởng gì ?\n",
            "có lẽ bạn cũng đã biết là não của mình hoạt động bằng cách gởi truyền tín hiệu .\n",
            "các chất truyền hoá học này được gọi là chất dẫn truyền thần kinh - giúp cho tế bào não liên hệ được với nhau .\n",
            "nhưng lượng chất dẫn truyền thần kinh này ở người bị bệnh An@@ -@@ da@@ i-@@ mơ đã bị giảm .\n",
            "người bệnh An@@ -@@ da@@ i-@@ mơ cũng lắng đọng nhiều chất ( prô-tê-in và chất xơ ) làm cho tế bào này không hoạt động tốt được .\n",
            "khi điều này xảy ra thì các tế bào không thể gởi tín hiệu đúng đến những bộ phận khác của não .\n",
            "dần dần , tế bào não của người bệnh An@@ -@@ da@@ i-@@ mơ cũng bắt đầu teo lại và hoại tử .\n",
            "nhiều công trình nghiên cứu vẫn đang thực hiện để tìm hiểu thêm về nguyên nhân của căn bệnh này .\n",
            "không phải chỉ một nguyên nhân gây bệnh .\n",
            "người già thường dễ bệnh hơn , và nguy cơ mắc bệnh càng cao khi càng lớn tuổi .\n",
            "chẳng hạn như , nguy cơ đối với người 85 tuổi cáo hơn so với người 65 tuổi .\n",
            "và phụ nữ cũng dễ mắc bệnh nhiều hơn nam giới .\n",
            "nhiều nhà nghiên cứu cũng cho rằng gien di truyền từ người thân trong gia đình có khả năng làm cho người ta mắc bệnh An@@ -@@ da@@ i-@@ mơ nhiều hơn .\n",
            "nhưng điều đó không đồng nghĩa với việc bất cứ ai có quan hệ với người bị An@@ -@@ da@@ i-@@ mơ đều sẽ mắc bệnh .\n",
            "nhiều yếu tố khác , cùng với yếu tố di truyền , cũng có thể làm cho người ta mắc bệnh .\n",
            "một vài người trong số đó bị cao huyết áp , cholesterol cao , bị hội chứng Down , hoặc bị chấn thương đầu .\n",
            "nhiều nhà nghiên cứu tin rằng việc luyện tập thể dục , ăn uống đầy đủ có lợi cho sức khỏe và có biện pháp giữ cho đầu óc mình hoạt bát ( chẳng hạn như chơi trò chơi ô chữ ) có thể giúp làm chậm lại cơn phát bệnh An@@ -@@ da@@ i-@@ mơ .\n",
            "làm sao người ta biết mình mắc bệnh An@@ -@@ da@@ i-@@ mơ ?\n",
            "dấu hiệu đầu tiên của bệnh An@@ -@@ da@@ i-@@ mơ là quên liên tục .\n",
            "điều này bắt đầu gây ảnh hưởng đến cuộc sống thường nhật của người bệnh .\n",
            "người ta có thể sẽ quên cửa hàng tạp hoá ở đâu hoặc tên của người thân trong gia đình và bạn bè của mình .\n",
            "giai đoạn này có thể kéo dài một thời gian hoặc tiến triển nhanh chóng , làm mất trí nhớ và chứng hay quên càng trầm trọng hơn .\n",
            "bác sĩ sẽ làm gì ?\n",
            "bác sĩ có thể rất khó chẩn đoán bệnh An@@ -@@ da@@ i-@@ mơ bởi nhiều triệu chứng của nó ( chẳng hạn như các vấn đề trí nhớ ) cũng có thể tương tự như những triệu chứng của nhiều chứng bệnh não khác .\n",
            "bác sĩ sẽ nói chuyện với bệnh nhân , tìm hiểu về bất cứ bệnh tật nào của người ấy , và sẽ khám bệnh cho họ .\n",
            "bác sĩ có thể đặt nhiều câu hỏi cho bệnh nhân hoặc yêu cầu bệnh nhân làm bài xét nghiệm viết để tìm hiểu xem trí nhớ của họ hoạt động tốt như thế nào .\n",
            "bác sĩ cũng có thể làm xét nghiệm y khoa ( chẳng hạn như chụp MRI hoặc chụp C@@ T ) để có được hình ảnh chi tiết của não .\n",
            "bác sĩ có thể nghiên cứu những hình ảnh này và dò tìm prô-tê-in và chất xơ lắng đọng – dấu hiệu điển hình của chứng bệnh An@@ -@@ da@@ i-@@ mơ .\n",
            "khi người bệnh được chẩn đoán mắc bệnh An@@ -@@ da@@ i-@@ mơ , bác sĩ có thể kê toa để giúp bệnh nhân nhớ và suy nghĩ được .\n",
            "bác sĩ cũng có thể cho bệnh nhân sử dụng thuốc để trị nhiều vấn đề khác , chẳng hạn như trầm cảm ( cảm giác buồn chán kéo dài một thời gian dài ) .\n",
            "tiếc là những thuốc này không thể chữa lành được bệnh mà chỉ giúp làm chậm quá trình tiến triển của bệnh .\n",
            "khi một người bạn yêu thương bị bệnh An@@ -@@ da@@ i-@@ mơ\n",
            "bạn có thể cảm thấy buồn phiền hay tức giận – hoặc cả hai – nếu một người nào đó mà mình thương yêu mắc bệnh An@@ -@@ da@@ i-@@ mơ .\n",
            "bạn cũng có thể thấy lo lắng , nhất là khi người ấy không nhớ được những điều quan trọng hoặc không còn có thể tự chăm sóc cho bản thân mình .\n",
            "bạn có thể không muốn đến thăm người bệnh , mặc dù bố hoặc mẹ mình muốn vậy .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs3lbMVz26uY"
      },
      "source": [
        "!subword-nmt apply-bpe -c $DATDIR/en.bpe < $DATDIR/test.en > $DATDIR/test.suben\n",
        "!subword-nmt apply-bpe -c $DATDIR/en.bpe < $DATDIR/dev.en > $DATDIR/dev.suben\n",
        "!subword-nmt apply-bpe -c $DATDIR/en.bpe < $DATDIR/train.en > $DATDIR/train.suben"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVWF3xI24_fp"
      },
      "source": [
        "!subword-nmt learn-bpe -s 10000 < $DATDIR/train.en > $DATDIR/en.bpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOkpTEbel0_R"
      },
      "source": [
        "#Extract feature of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_MacnYVDGU9",
        "outputId": "c7b06110-6bff-4c52-9f15-117cae962e26"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 25.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (53.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jDvuhsfDI1O",
        "outputId": "732cbfed-90b9-40cc-c218-6a1d717182f2"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 25.9MB/s]                    \n",
            "2021-02-03 07:03:02 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [00:04<00:00, 82.3MB/s]\n",
            "2021-02-03 07:03:12 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSEoA4lJDMIi",
        "outputId": "220cf378-8d70-404e-fa9a-b6756de7bb4a"
      },
      "source": [
        "import stanza,time\n",
        "import os\n",
        "\n",
        "def word2lemma(filename, ofilename):\n",
        "    nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma', tokenize_pretokenized=True)\n",
        "    isentences = open(filename, 'r').read()\n",
        "\n",
        "    out = open(ofilename, 'w')\n",
        "\n",
        "    doc = nlp(isentences)\n",
        "\n",
        "    for i, isentence in enumerate(doc.sentences):\n",
        "        osentence = ' '.join([word.lemma for word in isentence.words])\n",
        "        out.write(osentence + '\\n')\n",
        "    out.close()      \n",
        "    \n",
        "\n",
        "start_time = time.time()\n",
        "word2lemma(os.path.join(DATDIR,'test.en'), os.path.join(DATDIR,'test.leen'))\n",
        "word2lemma(os.path.join(DATDIR,'dev.en'), os.path.join(DATDIR,'dev.leen'))\n",
        "word2lemma(os.path.join(DATDIR,'train.en'), os.path.join(DATDIR,'train.leen'))\n",
        "end_time = time.time()        \n",
        "print(f'time in minutes: {end_time-start_time}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-03 07:03:32 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| lemma     | combined |\n",
            "========================\n",
            "\n",
            "2021-02-03 07:03:32 INFO: Use device: gpu\n",
            "2021-02-03 07:03:32 INFO: Loading: tokenize\n",
            "2021-02-03 07:03:32 INFO: Loading: lemma\n",
            "2021-02-03 07:03:37 INFO: Done loading processors!\n",
            "2021-02-03 07:03:40 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| lemma     | combined |\n",
            "========================\n",
            "\n",
            "2021-02-03 07:03:40 INFO: Use device: gpu\n",
            "2021-02-03 07:03:40 INFO: Loading: tokenize\n",
            "2021-02-03 07:03:40 INFO: Loading: lemma\n",
            "2021-02-03 07:03:40 INFO: Done loading processors!\n",
            "2021-02-03 07:03:43 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| lemma     | combined |\n",
            "========================\n",
            "\n",
            "2021-02-03 07:03:43 INFO: Use device: gpu\n",
            "2021-02-03 07:03:43 INFO: Loading: tokenize\n",
            "2021-02-03 07:03:43 INFO: Loading: lemma\n",
            "2021-02-03 07:03:43 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time in minutes: 74.23542642593384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0eTn8HDRrN",
        "outputId": "d15ea7d4-7739-4737-c6c9-839064616f7e"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (53.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZVjGuloDPG3",
        "outputId": "45b42f55-5496-4ae5-943b-51a1eb3832bb"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 19.6MB/s]                    \n",
            "2021-02-03 13:20:49 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [00:15<00:00, 26.7MB/s]\n",
            "2021-02-03 13:21:11 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jf2sMT2FUuG",
        "outputId": "f94d8bf9-b19c-48a1-9442-11612d875d8a"
      },
      "source": [
        "stanza.download('vi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 26.6MB/s]                    \n",
            "2021-02-03 13:29:29 INFO: Downloading default packages for language: vi (Vietnamese)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/vi/default.zip: 100%|██████████| 208M/208M [00:11<00:00, 17.8MB/s]\n",
            "2021-02-03 13:29:44 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psxiS9aTDi0E",
        "outputId": "180d14b5-edb3-4c98-92a2-e1fe86462caa"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
        "sentence = r\"it is impossible to understand China without knowing what Fenqing is and what role they play in the society in today 's China .\"\n",
        "doc = nlp(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-03 13:42:25 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2021-02-03 13:42:25 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| pos       | combined  |\n",
            "| lemma     | combined  |\n",
            "| depparse  | combined  |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-02-03 13:42:25 INFO: Use device: gpu\n",
            "2021-02-03 13:42:25 INFO: Loading: tokenize\n",
            "2021-02-03 13:42:25 INFO: Loading: pos\n",
            "2021-02-03 13:42:25 INFO: Loading: lemma\n",
            "2021-02-03 13:42:25 INFO: Loading: depparse\n",
            "2021-02-03 13:42:26 INFO: Loading: ner\n",
            "2021-02-03 13:42:26 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "UGaubFdlFkDw",
        "outputId": "bc51bd88-f975-4b01-b1ef-eb6ffdf73e5d"
      },
      "source": [
        "visentence = 'Fenqing là một từ tiếng Hoa mà nghĩa đen là \" thanh niên phẫn nộ \" .'\n",
        "vinlp = stanza.Pipeline(lang='vi', processors='tokenize,pos,lemma,depparse,ner')\n",
        "vidoc = vinlp(visentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-03 13:31:48 INFO: Loading these models for language: vi (Vietnamese):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | vtb      |\n",
            "| pos       | vtb      |\n",
            "| lemma     | identity |\n",
            "| depparse  | vtb      |\n",
            "| ner       | default  |\n",
            "========================\n",
            "\n",
            "2021-02-03 13:31:48 INFO: Use device: gpu\n",
            "2021-02-03 13:31:48 INFO: Loading: tokenize\n",
            "2021-02-03 13:31:48 INFO: Loading: pos\n",
            "2021-02-03 13:31:48 INFO: Loading: lemma\n",
            "2021-02-03 13:31:48 INFO: Loading: depparse\n",
            "2021-02-03 13:31:49 INFO: Loading: ner\n",
            "2021-02-03 13:31:49 ERROR: Cannot load model from /root/stanza_resources/vi/ner/default.pt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "UnsupportedProcessorError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                                                                                           use_gpu=self.use_gpu)\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_variant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/pipeline/ner_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, use_gpu)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 'charlm_backward_file': config.get('backward_charlm_path', None)}\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/models/ner/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, pretrain, model_file, use_cuda, train_classifier_only)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# load everything from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/models/ner/trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename, args)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/stanza_resources/vi/ner/default.pt'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnsupportedProcessorError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f9f49899185b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvisentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Fenqing là một từ tiếng Hoa mà nghĩa đen là \" thanh niên phẫn nộ \" .'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvinlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenize,pos,lemma,depparse,ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvidoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvinlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0;31m# user asked for a model which doesn't exist for this language?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mUnsupportedProcessorError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedProcessorError\u001b[0m: Processor ner is not known for language vi.  If you have created your own model, please specify the ner_model_path parameter when creating the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7xdgBX0BaFd"
      },
      "source": [
        "# !pip install stanza\n",
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
        "sentence = 'đầu tư vào giáo dục là vấn đề sống còn đối với mỹ latinh và caribê .'\n",
        "doc = nlp(sentence)\n",
        "\n",
        "def __words2factors(sentence, nlp=nlp):\n",
        "    doc = nlp(sentence)\n",
        "    result = [' '.join([word.lemma for sent in doc.sentences for word in sent.words]),\n",
        "            ' '.join([word.deprel for sent in doc.sentences for word in sent.words]),\n",
        "            ' '.join([word.upos for sent in doc.sentences for word in sent.words]),\n",
        "            ' '.join([word.ner for sent in doc.sentences for word in sent.words]),\n",
        "            ' '.join([word.feats if word.feats else \"_\" for sent in doc.sentences for word in sent.words]).replace('|', ',')]\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "end_time = time.time()        \n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'time in minutes: {epoch_mins}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGegQKkJm5HB"
      },
      "source": [
        "#Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv-OL4pRXH5j",
        "outputId": "7a38030a-679d-4158-9299-1fc0664de55e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XyOkBTFWzT5",
        "outputId": "4366fd26-695d-4b86-f911-cc3bab4f3ff1"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n",
        "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKvgh9mit8LL"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(ifname, ofilename):\n",
        "    ilines = open(ifname, 'r').read().splitlines()\n",
        "    olines = [[word for word in iline.split(' ') if word not in stopwords.words('english')] for iline in ilines]\n",
        "    olines = [' '.join(oline) for oline in olines]\n",
        "    ofile = open(ofilename, 'w')\n",
        "    for oline in olines:\n",
        "        ofile.write(oline + '\\n')\n",
        "    ofile.close()\n",
        "\n",
        "remove_stopwords(os.path.join(DATDIR,'test.leen'), os.path.join(DATDIR,'test.nsleen'))\n",
        "remove_stopwords(os.path.join(DATDIR,'dev.leen'), os.path.join(DATDIR,'dev.nsleen'))\n",
        "remove_stopwords(os.path.join(DATDIR,'train.leen'), os.path.join(DATDIR,'train.nsleen'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDfKaBKMclRH"
      },
      "source": [
        "Run locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2mZ2tCIwcnOO",
        "outputId": "d4b6a019-2b41-4496-9250-69fc2cabcf5c"
      },
      "source": [
        "folder = '/Volumes/nttrang/Writing/Complexity_2_2021/datasets'\n",
        "import os\n",
        "os.chdir(folder)\n",
        "srctest = 'test.vi'\n",
        "trgtest = 'test.en'\n",
        "prde1 = 'predict.en'\n",
        "pre_prde2 = 'test.wovi'\n",
        "prde2 = 'predict (1).en'\n",
        "prdev1 = 'predict.vi'\n",
        "prdev2 = 'predict.wovi'\n",
        "post_prdev2 = 'predict (1).vi'\n",
        "allfiles=['test.vi', 'test.en', 'predict.en', 'test.wovi', 'predict (1).en', 'predict.vi', 'predict.wovi', 'predict (1).vi']\n",
        "ps = [os.path.join(folder, file) for file in allfiles]\n",
        "def catl(stop=1, start=0, length=5, ps=ps):  \n",
        "    line_sets = [open(p, 'r').read().splitlines() for p in ps]\n",
        "\n",
        "    line_sets = [line_set for line_set in zip(*line_sets) if len(line_set[0].split(' '))==length]\n",
        "    for i in range(start, stop):\n",
        "        print(f'----------{i}----------')\n",
        "        print('\\n'.join(line_sets[i]))\n",
        "        print('         ')\n",
        "\n",
        "catl(stop=50, start=0, length=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------0----------\n",
            "công ty đã sẵn sàng cho ra mắt chip Atom \" Medfield \" nó sẽ bắt đầu xuất hiện trong điện thoại thông minh và máy tính bảng vào năm tới .\n",
            "the company is readying its \" Medfield \" Atom chips , which will begin appearing in smartphones and tablets next year .\n",
            "the company was ready to launch its \" Atom Atom Atom chip chip \" will start to appear in smartphones and tablet computers next year .\n",
            "công_ty đã sẵn_sàng cho ra_mắt chip Atom \" Medfield \" nó sẽ bắt_đầu xuất_hiện trong điện_thoại thông_minh và máy_tính bảng vào năm tới .\n",
            "the company is ready to launch its Atom chip , which would start to appear in the smartphone and tablet tablet tablet next year .\n",
            "công ty sản xuất máy tính bảng này là \" chip \" sẽ bắt đầu kích hoạt dòng điện thoại thông minh chạy Windows 8 và điện thoại thông minh năm nay .\n",
            "công_ty này đang mở_rộng các chip \" Atom \" của họ , sẽ bắt_đầu biến mất trong máy_tính bảng và máy_tính bảng trong năm tới .\n",
            "công ty này đang mở rộng các chip \" Atom \" của họ , sẽ bắt đầu biến mất trong máy tính bảng và máy tính bảng trong năm tới .\n",
            "         \n",
            "----------1----------\n",
            "Intel , một gã khổng lồ đến từ nền tảng máy tính x 86 và không gian máy chủ , nơi mà những con chip tiêu thụ nhiều năng lượng hơn .\n",
            "Intel is coming from the x86 PC and server spaces , where the chips consume much more energy .\n",
            "Intel , a giant giant giant giant , has come from a computer and space space , where chips are more energy .\n",
            "Intel , một gã khổng_lồ đến từ nền_tảng máy_tính x 86 và không_gian máy_chủ , nơi mà những con chip tiêu_thụ nhiều năng_lượng hơn .\n",
            "Intel , a huge giant giant giant , has come from the desktop platform , and server space space , where many of the chips have more energy on energy .\n",
            "Intel đang ra mắt bộ vi xử lí máy tính xách tay và máy tính bảng , máy tính bảng này có khả năng chi tiêu nhiều hơn .\n",
            "Intel đang ra_mắt từ các máy_chủ PC và các máy_chủ PC , nơi mà các chip tiêu_thụ điện_năng tiêu_thụ nhiều hơn .\n",
            "Intel đang ra mắt từ các máy chủ PC và các máy chủ PC , nơi mà các chip tiêu thụ điện năng tiêu thụ nhiều hơn .\n",
            "         \n",
            "----------2----------\n",
            "nhóm nghiên cứu thuộc Đại học Northwestern đã hỏi 686 phụ nữ đang dùng thuốc ức chế aromatase để điều trị ung thư vú nhạy với ét - xtrô – gien .\n",
            "----------the Northwestern University team questioned 686 women who were taking aromatase inhibitors as treatment for oestrogen-sensitive breast cancer .\n",
            "the team of the University of Cancer University asked women who were using estrogen therapy to treat breast cancer .\n",
            "nhóm nghiên_cứu thuộc Đại_học Northwestern đã hỏi 686 phụ_nữ đang dùng thuốc ức_chế aromatase để điều_trị ung_thư vú nhạy với ét - xtrô – gien .\n",
            "the team of the University of Toronto , who asked women who were using breast cancer to treat breast cancer therapy with breast cancer - showed the gene gene gene gene gene gene gene .\n",
            "nhóm nghiên cứu tại Đại học Toronto cho thấy phụ nữ đang bỏ thuốc chống ung thư dạ dày - ví dụ như thuốc chống ung thư dạ dày .\n",
            "nhóm nghiên_cứu tại Đại_học Oxford đã được phỏng_vấn với những phụ_nữ mắc chứng ung_thư vú khi điều_trị ung_thư vú .\n",
            "nhóm nghiên cứu tại Đại học Oxford đã được phỏng vấn với những phụ nữ mắc chứng ung thư vú khi điều trị ung thư vú .\n",
            "         \n",
            "----------3----------\n",
            "khoảng hai phần ba bệnh nhân ung thư vú nhạy với ét - xtrô – gien còn thuốc ức chế aromatase cho thấy làm giảm nguy cơ ung thư tái diễn .\n",
            "about two-thirds of breast cancers are oestrogen-sensitive , and aromatase inhibitors have been shown to reduce the risk of cancer recurring .\n",
            "about two percent of the patients with breast cancer - the gene also showed a decrease in the risk of cancer .\n",
            "khoảng hai_phần_ba bệnh_nhân ung_thư vú nhạy với ét - xtrô – gien còn thuốc ức_chế aromatase cho thấy làm giảm nguy_cơ ung_thư tái_diễn .\n",
            "about two-thirds of breast cancer patients with breast breast cancer - the gene also showed that smoking reduces the risk of cancer risk .\n",
            "khoảng hai phần trăm ung thư vú , và trà xanh có tác dụng làm giảm nguy cơ ung thư tuyến tiền liệt .\n",
            "khoảng hai_phần_ba trong số các ung_thư vú đang được điều_trị bằng polyphenol , và các chất ức_chế ức_chế sự giảm nguy_cơ ung_thư tuyến tuỵ .\n",
            "khoảng hai phần ba trong số các ung thư vú đang được điều trị bằng polyphenol , và các chất ức chế ức chế sự giảm nguy cơ ung thư tuyến tuỵ .\n",
            "         \n",
            "----------4----------\n",
            "ngoài việc sử dụng tầng hầm làm khoa ung thư , nó còn đang được dùng làm chỗ đỗ xe hơi và chứa thuốc men , ông Khan cho biết thêm .\n",
            "in addition to using the basement as an oncology department , it was also being used to park cars and house medical supplies , Khan said .\n",
            "in addition to the use of pancreatic cancer , it is also being used as a machine that contains over-the-counter cars and drugs , he added .\n",
            "ngoài việc sử_dụng tầng hầm làm khoa ung_thư , nó còn đang được dùng làm chỗ đỗ xe_hơi và chứa thuốc_men , ông Khan cho biết thêm .\n",
            "in addition to using a basement basement of the cancer , it is also used to make a car , where the car is using a car and drugs , he added .\n",
            "ngoài ra , sử dụng bộ đồ đạc như một bộ đồ đạc , nó cũng được sử dụng để sử dụng và xe hơi .\n",
            "ngoài_ra , trong đó việc sử_dụng tầng hầm như một tấm thảm , nó cũng được sử_dụng để được trang_trí trong căn nhà và vật_dụng vật_dụng , các vật_dụng được cho biết .\n",
            "ngoài ra , trong đó việc sử dụng tầng hầm như một tấm thảm , nó cũng được sử dụng để được trang trí trong căn nhà và vật dụng vật dụng , các vật dụng được cho biết .\n",
            "         \n",
            "----------5----------\n",
            "thiết bị này đi kèm với tất cả các tính năng mà bất kỳ chủ sở hữu máy tính bảng nào cũng mong muốn , và nó hoạt động cực tốt .\n",
            "the device comes with all the features any tablet owner would want , and it works exceptionally well .\n",
            "this device is accompanied with all the owners of any tablet that will want to expect , and it is extremely active .\n",
            "thiết_bị này đi kèm với tất_cả các tính_năng mà bất_kỳ chủ_sở_hữu máy_tính bảng nào cũng mong_muốn , và nó hoạt_động cực tốt .\n",
            "the device comes with all the features that all the owners of any tablet will want , and it works well .\n",
            "thiết bị này được thiết kế với máy tính bảng chạy Windows 8 , nó sẽ muốn cạnh tranh với nó .\n",
            "thiết_bị này đi kèm với tất_cả các tính_năng máy_tính bảng chạy trên máy_tính bảng sẽ muốn có được , và nó cũng sẽ hoạt_động tốt .\n",
            "thiết bị này đi kèm với tất cả các tính năng máy tính bảng chạy trên máy tính bảng sẽ muốn có được , và nó cũng sẽ hoạt động tốt .\n",
            "         \n",
            "----------6----------\n",
            "tuy nhiên , những ai muốn một điều gì đó khác hơn so với cảm giác giống hệ điều hành i OS cũ sẽ phải thất vọng bởi phần mềm này .\n",
            "but those who want something other than the same old iOS feel will be sorely disappointed by the software .\n",
            "however , those who want to do something more than the same thing that the old OS is expected to be disappointed by this software .\n",
            "tuy_nhiên , những_ai muốn một điều gì đó khác hơn so với cảm_giác giống hệ_điều_hành i OS cũ sẽ phải thất_vọng bởi phần_mềm này .\n",
            "however , those who want to do something better than the same OS OS would have to be disappointed by this software .\n",
            "nhưng những người muốn những người khác sẽ cảm thấy iOS 6 sẽ cảm thấy mạnh mẽ hơn bởi hệ điều hành mã nguồn mở .\n",
            "nhưng những người muốn cái gì đó giống như iOS so với iOS cũ sẽ cảm_thấy thất_vọng bởi phần_mềm này .\n",
            "nhưng những người muốn cái gì đó giống như iOS so với iOS cũ sẽ cảm thấy thất vọng bởi phần mềm này .\n",
            "         \n",
            "----------7----------\n",
            "mặc dù dự án có thể sẽ tiết kiệm được tiền bạc , nhưng Aftergood cảnh báo trước quy trình số hoá có thể sẽ làm tốn rất nhiều tiền . \"\n",
            "although the project may inevitably save money , Aftergood warned that the digitization process could prove costly .\n",
            "although the project may save money , money warned that the number of hormones may be expensive . \"\n",
            "mặc_dù dự_án có_thể sẽ tiết_kiệm được tiền_bạc , nhưng Aftergood cảnh_báo trước quy_trình số_hoá có_thể sẽ làm tốn rất nhiều tiền .\n",
            "although the project can save money , the governor warned that it may be too much expensive .\n",
            "mặc dù dự án dự án có thể tiết kiệm tiền tệ , nhưng nhà kinh tế học vẫn cảnh báo rằng việc thiếu tiền lương thực có thể gây khó khăn .\n",
            "mặc_dù dự_án này có_thể tiết_kiệm được tiền tiết_kiệm , nhưng người ta cảnh_báo rằng quá_trình này có_thể làm giảm bớt tốn_kém .\n",
            "mặc dù dự án này có thể tiết kiệm được tiền tiết kiệm , nhưng người ta cảnh báo rằng quá trình này có thể làm giảm bớt tốn kém .\n",
            "         \n",
            "----------8----------\n",
            "bộ trưởng Châu Âu của Hungary cho BBC biết nước bà sẵn sàng tham gia hiệp ước vì quốc hội đã cho phép - phủ nhận các tin đồn trước đây .\n",
            "Hungary 's Europe Minister Eniko Gyori told the BBC her country was willing to join with the consent of parliament - contradicting earlier reports .\n",
            "the BBC 's foreign minister says she is ready to meet a congressional agreement for the Congress - which has said previous reports .\n",
            "bộ_trưởng Châu_Âu của Hungary cho BBC biết nước bà sẵn_sàng tham_gia hiệp_ước vì quốc_hội đã cho_phép - phủ_nhận các tin_đồn trước_đây .\n",
            "the European Union 's European minister said she was ready to take part of the pact for the White House - which allowed the rumors .\n",
            "thủ tướng Ôn Gia Bảo đã nói với BBC rằng phóng viên quốc gia này đang chuẩn bị cho cuộc họp với các quan chức quốc gia láng giềng của quốc gia này - đã bắt đầu cuộc họp báo trước đó .\n",
            "bộ_trưởng Bộ ngoại_giao châu_Âu của Pa-ki-xtan - Hôm thứ_năm đã nói với BBC rằng đất_nước này sẵn_sàng phản_đối việc phản_đối các bản tin quốc_hội - trước đó các bản báo_cáo ban_đầu .\n",
            "bộ trưởng Bộ ngoại giao châu Âu của Pa-ki-xtan - Hôm thứ năm đã nói với BBC rằng đất nước này sẵn sàng phản đối việc phản đối các bản tin quốc hội - trước đó các bản báo cáo ban đầu .\n",
            "         \n",
            "----------9----------\n",
            "Foxconn đã tăng lương ở Trung Quốc từ vài tháng trước , vàlần tăng gần đây nhất được công bố là vào tháng hai với mức lương tăng lên đến 25% .\n",
            "Foxconn has already increased salaries in China over the past few months , with the latest rise announced in February resulting in an increase of up to 25% .\n",
            "Foxconn has increased food prices in China from just a few months ago , the latest increase in February by rising food prices .\n",
            "Foxconn đã tăng lương ở Trung_Quốc từ vài tháng trước , vàlần tăng gần đây nhất được công_bố là vào tháng hai với mức lương tăng lên đến 25% .\n",
            "Sweden has increased wages in China from a few months earlier , up to a recent increase in February .\n",
            "doanh số này đã tăng thêm nhiều tháng trong thập niên qua , với mức tăng trưởng mới nhất của Trung Quốc trong tháng bảy vừa qua .\n",
            "GSO đã tăng lương trong vài tháng qua , cùng với mức tăng cao nhất trong tháng hai , mức tăng cao nhất trong một con_số được công_bố vào tháng hai .\n",
            "GSO đã tăng lương trong vài tháng qua , cùng với mức tăng cao nhất trong tháng hai , mức tăng cao nhất trong một con số được công bố vào tháng hai .\n",
            "         \n",
            "----------10----------\n",
            "tuy nhiên Thủ tướng nói ông muốn dự thảo này phải được ban hành ngay phiên họp hiện tại của quốc hội , dự kiến kết thúc vào tháng 6 tới .\n",
            "but the prime minister says he wants the bill enacted in the current session of parliament , which ends in June .\n",
            "however , Prime Minister said he wanted to attend the current meeting at the end of the Congress , intended to end 6 months .\n",
            "tuy_nhiên Thủ_tướng nói ông muốn dự_thảo này phải được ban_hành ngay phiên họp hiện_tại của quốc_hội , dự_kiến kết_thúc vào tháng 6 tới .\n",
            "but the prime minister said he wanted to give this incident at the end of the Congress 's scheduled meeting , scheduled to end the end of June .\n",
            "nhưng thủ tướng này đã nói ông muốn rút lui kế hoạch tại cuộc họp thượng đỉnh vào tháng sáu .\n",
            "nhưng thủ_tướng cho biết ông muốn dự_luật này thay_đổi trong phiên họp hiện_tại của quốc_hội , chấm_dứt vào tháng sáu .\n",
            "nhưng thủ tướng cho biết ông muốn dự luật này thay đổi trong phiên họp hiện tại của quốc hội , chấm dứt vào tháng sáu .\n",
            "         \n",
            "----------11----------\n",
            "Remington , một nhà sản xuất dao cạo điện , lấy phần lớn tiền của mình trước , chứ không chờ lấy từ dòng doanh thu bán lưỡi dao thay thế .\n",
            "Remington , a manufacturer of electric shavers , makes most of its money upfront , rather than from a stream of Blade refill sales .\n",
            "a cheap maker , took part of your money , took off the larger part of your money , not getting rid of revenue .\n",
            "Remington , một nhà_sản_xuất dao_cạo điện , lấy phần_lớn tiền của mình trước , chứ không chờ lấy từ dòng doanh_thu bán lưỡi dao thay_thế .\n",
            "Stephen Zoellick , an extra power manufacturer , took off his money before its pocket , but did n't wait for the revenue from the revenue from the mainland .\n",
            "nhà sản xuất , một nhà sản xuất máy tính xách tay , hầu hết các dòng tiền mặt hàng đầu của nó là tiền mặt hàng không gian lận .\n",
            "Costa_Rica , một nhà_sản_xuất thiết_bị điện điện điện , làm cho hầu_hết các khoản tiền lương thấp nhất của nó , chứ không phải là từ dòng điện_thoại thông_minh mặt_trời .\n",
            "Costa Rica , một nhà sản xuất thiết bị điện điện điện , làm cho hầu hết các khoản tiền lương thấp nhất của nó , chứ không phải là từ dòng điện thoại thông minh mặt trời .\n",
            "         \n",
            "----------12----------\n",
            "Joan Magretta , cựu biên tập viên Tạp chí Kinh doanh của Đại học Harvard , nhấn mạnh hai tiêu chuẩn quan trọng để đánh giá các mô hình kinh doanh .\n",
            "Joan Magretta , former editor of the Harvard Business Review , highlights two critical tests for sizing up business models .\n",
            "former affairs editor L. L. Takeshita , who heads the Harvard School of Business School of Economics , stressed two important ways to assess business models .\n",
            "Joan_Magretta , cựu biên_tập_viên Tạp_chí Kinh_doanh của Đại_học Harvard , nhấn_mạnh hai tiêu_chuẩn quan_trọng để đánh_giá các mô_hình kinh_doanh .\n",
            "former Medical Research , former director of the Harvard School of the Harvard School of Pediatrics , highlights the two key standards of business models to assess business models .\n",
            "các nhà kinh tế học , cựu tổng giám đốc marketing tại Đại học Harvard , hai nhà nghiên cứu mô tả các cấu trúc mô hình kinh doanh phức tạp đối với các doanh nghiệp nhỏ .\n",
            "Julian_Assange , cựu giám_đốc bộ_phận kinh_doanh chính của Đại_học Harvard , đã nêu hai bài kiểm_tra quan_trọng cho các mô_hình kinh_doanh .\n",
            "Julian Assange , cựu giám đốc bộ phận kinh doanh chính của Đại học Harvard , đã nêu hai bài kiểm tra quan trọng cho các mô hình kinh doanh .\n",
            "         \n",
            "----------13----------\n",
            "đối với ví dụ về các mô hình kinh doanh không đạt chuẩn liên quan đến những con số , chúng ta có thể nhìn vào các hãng xe hơi Mỹ .\n",
            "for examples of business models that failed the numbers test , we can look at U.S. automakers .\n",
            "for example , business models are not tied to numbers of numbers , we can look at the United States .\n",
            "đối_với ví_dụ về các mô_hình kinh_doanh không đạt chuẩn liên_quan đến những con_số , chúng_ta có_thể nhìn vào các hãng xe_hơi Mỹ .\n",
            "for example , business models do not reach standard numbers , we can look at U.S. car companies .\n",
            "ví dụ về một số doanh nghiệp không rõ doanh nghiệp , chúng ta có thể nhìn thấy rằng chúng ta có thể nhìn thấy những số liệu thống nhất của Hoa Kỳ .\n",
            "ví_dụ về những mô_hình kinh_doanh không thất_bại , chúng_tôi có_thể nhìn thấy những con_số của các công_ty bảo_hiểm tại Mỹ .\n",
            "ví dụ về những mô hình kinh doanh không thất bại , chúng tôi có thể nhìn thấy những con số của các công ty bảo hiểm tại Mỹ .\n",
            "         \n",
            "----------14----------\n",
            "khi đánh giá một công ty với tư cách là một khoản đầu tư khả dĩ , hãy tìm hiểu chính xác xem công ty đó kiếm tiền bằng cách nào .\n",
            "when evaluating a company as a possible investment , learn exactly how it makes its money .\n",
            "when you evaluate a company as a investment is an investment , look at how to figure out how a company gets money .\n",
            "khi đánh_giá một công_ty với tư_cách là một khoản đầu_tư khả_dĩ , hãy tìm_hiểu chính_xác xem công_ty đó kiếm tiền bằng cách nào .\n",
            "when a company is evaluating the company as an investment investment , find out how much the company works by doing it .\n",
            "khi một công ty đầu tư có thể kiếm được một khoản đầu tư nào đó , nó kiếm được tiền .\n",
            "khi đánh_giá một công_ty nào đó như một công_ty có_thể đầu_tư , chính_xác cách kiếm tiền như_thế_nào .\n",
            "khi đánh giá một công ty nào đó như một công ty có thể đầu tư , chính xác cách kiếm tiền như thế nào .\n",
            "         \n",
            "----------15----------\n",
            "lúc đó , ông nổi tiếng với vai tù binh Cockney trong \" Những người hùng của Hogan \" , trình chiếu trên truyền hình CBS từ năm 1965 đến 1971 .\n",
            "by then , he was known as the Cockney prisoner of war in Hogan 's Heroes , which ran on CBS from 1965 to 1971 .\n",
            "at that time , he was famous for Mr Hogan 's Heroes 's Heroes , which CBS CBS CBS CBS CBS from 1971 to 1971 .\n",
            "lúc đó , ông nổi_tiếng với vai tù_binh Cockney trong \" Những người_hùng của Hogan \" , trình_chiếu trên truyền_hình CBS từ năm 1965 đến 1971 .\n",
            "at that time , he was famous for Mr Hogan 's Heroes in Hogan 's Heroes , which took off CBS from CBS in 1971 to 1971 .\n",
            "sau đó anh ấy được biết đến là \" cầu thủ \" của Thụy Sĩ \" trong bộ phim khiêu dâm , đã giành được trang bị từ lâu đài truyền hình Hàn Quốc .\n",
            "sau đó , ông được biết đến với tên gọi là \" người_hùng \" Những người_hùng của Hogan \" vào năm 1971 , chạy trốn khỏi CBS .\n",
            "sau đó , ông được biết đến với tên gọi là \" người hùng \" Những người hùng của Hogan \" vào năm 1971 , chạy trốn khỏi CBS .\n",
            "         \n",
            "----------16----------\n",
            "tuần trước tác phẩm \" The Scream \" của Edvard Munch đã lập kỷ lục thế giới cho một tác phẩm nghệ thuật - bán với giá 119.9 triệu đô la .\n",
            "last week Edvard Munch 's The scream set the world record for an artwork - selling for $ 119.9m .\n",
            "last week The The The \" The World Cup \" has set the world record record record - with $ 1bn million .\n",
            "tuần trước_tác phẩm \" The_Scream \" của Edvard_Munch đã lập kỷ_lục thế_giới cho một tác_phẩm nghệ_thuật - bán với giá 119.9 triệu đô_la .\n",
            "a week of The International Monetary Fund 's \" record record record for world record record , which is available to $ 1bn per year .\n",
            "tuần trước , cỗ xe hơi khổng lồ trên thế giới đã giành được một kỷ lục thế giới - một nhà xuất bản cho vay thế chấp có giá trị khoảng 15 triệu đô-la Mỹ .\n",
            "tuần trước , nhà vô_địch thế_giới đã đưa ra một kỷ_lục về thành_tích cho các nhà mạng thế_giới - bán được 13 triệu bảng Anh .\n",
            "tuần trước , nhà vô địch thế giới đã đưa ra một kỷ lục về thành tích cho các nhà mạng thế giới - bán được 13 triệu bảng Anh .\n",
            "         \n",
            "----------17----------\n",
            "dân buôn lậu ma túy ban đầu bị tình nghi là thủ phạm của vụ tấn công này , thế nhưng sau đó chín tên lính Thái đã bị bắt giam .\n",
            "drug smugglers were initially suspected of the attack , but nine Thai soldiers were subsequently detained .\n",
            "critics of the forget that the incident was convicted of this attack , but nine names were arrested .\n",
            "dân buôn_lậu ma_tuý ban_đầu bị tình_nghi là thủ_phạm của vụ tấn_công này , thế nhưng sau đó chín tên lính Thái đã bị bắt giam .\n",
            "critics have initially suspected the suspect of the attack on the attack , but then nine soldiers were arrested .\n",
            "những kẻ giết người đầu tiên bị tấn công , nhưng bốn binh lính Thái Lan bị giam giữ tại Thái Lan .\n",
            "những loại thuốc giả ban_đầu ban_đầu bị nghi_ngờ là tấn_công , nhưng chín binh_sĩ Thái_Lan bị bắt .\n",
            "những loại thuốc giả ban đầu ban đầu bị nghi ngờ là tấn công , nhưng chín binh sĩ Thái Lan bị bắt .\n",
            "         \n",
            "----------18----------\n",
            "ở giai đoạn này , nhà nghiên cứu cố tình không yêu cầu họ hoàn thành bất kỳ nhiệm vụ nào mà làm tăng cao mức độ căng thẳng của họ .\n",
            "at this stage , the researchers deliberately did not ask them to complete any tasks that would raise their stress levels .\n",
            "at this stage , the researchers attempted not to ask them any task to increase their stress levels .\n",
            "ở giai_đoạn này , nhà_nghiên_cứu cố_tình không yêu_cầu họ hoàn_thành bất_kỳ nhiệm_vụ nào mà làm tăng cao mức_độ căng_thẳng của họ .\n",
            "at this stage , the researchers did not ask them to complete any task which increases their stress .\n",
            "vào giai đoạn này , các nhà nghiên cứu đã yêu cầu họ không giải quyết bất kỳ hoạt động nào mà họ sẽ cố gắng đẩy mạnh mức độ căng thẳng của họ .\n",
            "ở giai_đoạn này , các nhà_nghiên_cứu cũng không yêu_cầu họ làm bất_kỳ hoạt_động nào làm tăng mức_độ căng_thẳng của họ .\n",
            "ở giai đoạn này , các nhà nghiên cứu cũng không yêu cầu họ làm bất kỳ hoạt động nào làm tăng mức độ căng thẳng của họ .\n",
            "         \n",
            "----------19----------\n",
            "ông Paul Thurrott cho biết : \" mặc định , các email trả lời sẽ xuất hiện cùng mạch với email gốc và nằm trong phần đọc email dành riêng \" .\n",
            "Thurrott says , \" Now , replies happen inline by default , and in the area normally reserved for the reading pane . \"\n",
            "\" despite the email , email makers will be able to appear with the email and lie in a separate email email email , \" said Paul Stephenson .\n",
            "ông Paul_Thurrott cho biết : \" mặc_định , các email trả_lời sẽ xuất_hiện cùng mạch với email gốc và nằm trong phần đọc email dành riêng \" .\n",
            "\" by default , email and email sites will appear to be shut down with the email and read in private email , \" he said .\n",
            "theo nhà kinh tế học , \" Tôi thích bất cứ thứ gì xảy ra bằng cách đánh giá tín nhiệm , và không gian lận trong khu vực này . \"\n",
            "Liu cho biết , \" Điều đó là điều không_thể trả được bằng việc trả tự_do không đúng cách , và trong khu_vực thường có cả hai tay đọc sách . \"\n",
            "Liu cho biết , \" Điều đó là điều không thể trả được bằng việc trả tự do không đúng cách , và trong khu vực thường có cả hai tay đọc sách . \"\n",
            "         \n",
            "----------20----------\n",
            "Skype cho Windows Phone đòi hỏi bạn phải sử dụng Nokia Lumia 710 , 800 , hoặc 900 ; HTC Titan hoặc Radar ; hoặc Samsung Focus S hoặc Focus Flash .\n",
            "Skype for Windows Phone requires use of a Nokia Lumia 710 , 800 , or 900 ; an HTC Titan or radar ; or a Samsung Focus S or Focus Flash .\n",
            "Windows Phone Phone Phone has to ask you to use Nokia 's Lumia 900 , 800 , or Lumia 900 ; Samsung or Samsung , or Samsung Electronics .\n",
            "Skype cho Windows_Phone đòi_hỏi bạn phải sử_dụng Nokia Lumia 710 , 800 , hoặc 900 ; HTC Titan hoặc Radar ; hoặc Samsung Focus_S hoặc Focus_Flash .\n",
            "Windows Phone RT requires you to use Nokia 's Lumia 900 , 800 , or the HTC One , or the Samsung Galaxy S S S S , or Samsung Galaxy S S S S , or Samsung , as well .\n",
            "hãy sử dụng Windows Phone 7 bao gồm một máy tính xách tay Lumia 900 , Lumia 900 , Lumia 900 , hoặc HTC One X hoặc HTC One S .\n",
            "phiên_bản Windows_Phone 7 đòi_hỏi phải sử_dụng một thiết_bị chạy Lumia 900 , 800 , hoặc 900 inch ; hoặc là một phiên_bản 16 inch hoặc là màn_hình chụp đẹp nhất mà Samsung ghi lại .\n",
            "phiên bản Windows Phone 7 đòi hỏi phải sử dụng một thiết bị chạy Lumia 900 , 800 , hoặc 900 inch ; hoặc là một phiên bản 16 inch hoặc là màn hình chụp đẹp nhất mà Samsung ghi lại .\n",
            "         \n",
            "----------21----------\n",
            "nếu bạn không để Skype trên nền chính , nói cách khác là nếu bạn đang sử dụng ứng dụng khác , bạn sẽ không thể nhận cuộc gọi qua Skype .\n",
            "if the Skype app is not in the foreground , or if you 're in another app , you wo n't be able to receive calls via the Skype app .\n",
            "if you do n't do n't have any other major thing on the platform , you 're using another app , you wo n't get past it .\n",
            "nếu bạn không để Skype trên nền chính , nói cách khác là nếu bạn đang sử_dụng ứng_dụng khác , bạn sẽ không_thể nhận cuộc_gọi qua Skype .\n",
            "if you do n't let your own economy , it 's another way to tell if you 're using other apps , you 'll be able to get a call through a call .\n",
            "nếu ứng dụng không có ứng dụng gì , hoặc nếu bạn có thể cài đặt trong một ứng dụng nào đó , thì bạn sẽ không thể kiểm tra ứng dụng video .\n",
            "nếu ứng_dụng không được sao_chép trên ứng_dụng , hoặc nếu bạn ở một ứng_dụng khác , thì bạn sẽ không_thể nhận được ứng_dụng thông_qua thông_qua ứng_dụng video .\n",
            "nếu ứng dụng không được sao chép trên ứng dụng , hoặc nếu bạn ở một ứng dụng khác , thì bạn sẽ không thể nhận được ứng dụng thông qua thông qua ứng dụng video .\n",
            "         \n",
            "----------22----------\n",
            "không có bằng chứng đáng tin cậy nào – quan sát được bằng kính thiên văn hoặc các hình thức khác – cho thấy sự tồn tại của hành tinh này .\n",
            "there is no credible evidence - telescopic or otherwise - for this object 's existence .\n",
            "there is no credible evidence - with cultural or color - evidence of this planet .\n",
            "không có bằng_chứng đáng tin_cậy nào – quan_sát được bằng kính_thiên_văn hoặc các hình_thức khác – cho thấy sự tồn_tại của hành_tinh này .\n",
            "there is no credible evidence - either - or in other forms - shows the planet of this planet .\n",
            "không có bằng chứng đáng tin cậy nào đáng tin cậy - hoặc các tác giả của vật liệu này .\n",
            "không có bằng_chứng đáng tin_cậy nào đáng tin_cậy - hoặc nếu không có sự tồn_tại của vật_thể này .\n",
            "không có bằng chứng đáng tin cậy nào đáng tin cậy - hoặc nếu không có sự tồn tại của vật thể này .\n",
            "         \n",
            "----------23----------\n",
            "Iran cho biết chương trình hạt nhân này vì mục đích hoà bình , nhưng Israel và phương Tây cho là Iran đang phát triển chương trình vũ khí hạt nhân .\n",
            "Iran says its nuclear program is for peaceful purposes , but Israel and the West believe Iran is building nuclear weapons .\n",
            "Iran says the nuclear program is essential for peace , but Israel and Iran say Iran is developing nuclear weapons .\n",
            "Iran cho biết chương_trình hạt_nhân này vì mục_đích hoà_bình , nhưng Israel và phương Tây cho là Iran đang phát_triển chương_trình vũ_khí_hạt_nhân .\n",
            "Iran said the nuclear program was for peace , but Israel and Western Iran are developing Iran for nuclear weapons .\n",
            "Iran cho biết chương trình hạt nhân của chương trình hạt nhân đang nhắm đến Israel , nhưng các nhà lãnh đạo Israel và Iran tuyên bố rằng vũ khí hạt nhân đang xây dựng vũ khí hạt nhân .\n",
            "Iran nói chương_trình hạt_nhân của họ là dành cho các mục_đích hoà_bình , nhưng Israel và phương Tây tin rằng Iran đang xây_dựng vũ_khí hạt_nhân .\n",
            "Iran nói chương trình hạt nhân của họ là dành cho các mục đích hoà bình , nhưng Israel và phương Tây tin rằng Iran đang xây dựng vũ khí hạt nhân .\n",
            "         \n",
            "----------24----------\n",
            "tôi đã định sẽ ngủ dậy thật muộn để tận hưởng vẻ đẹp nguyên sơ tiềm ẩn của các bãi biển và khu rừng nơi đây mà tôi đã bắt gặp .\n",
            "I planned to enjoy staying in bed late to enjoy the pristine hide-away I had found combing the beauty of beach and forest .\n",
            "I 've decided to fall asleep so late that the great cause of sea and sea areas where I had arrested .\n",
            "tôi đã định sẽ ngủ dậy thật muộn để tận_hưởng vẻ đẹp nguyên_sơ tiềm_ẩn của các bãi biển và khu rừng nơi đây mà tôi đã bắt_gặp .\n",
            "I 've scheduled to wake up at the end of the evening to enjoy a beautiful dress of the sea and the sea places where I was caught .\n",
            "tôi muốn cảm thấy vui sướng khi đi ngủ trên giường với tôi , tôi thấy mình đã nhìn thấy một thiên thần tuyệt vời và xinh xắn .\n",
            "tôi sẵn_sàng đi ngủ một_mình vào cuối buổi tối để tận_hưởng sự yêu thích , tôi thấy tôi đã thấy một cái áo_khoác đẹp và cánh cửa .\n",
            "tôi sẵn sàng đi ngủ một mình vào cuối buổi tối để tận hưởng sự yêu thích , tôi thấy tôi đã thấy một cái áo khoác đẹp và cánh cửa .\n",
            "         \n",
            "----------25----------\n",
            "anh cũng cho biết thêm \" Chúng tôi sẽ đón tiếp rất nhiều khách du lịch nước ngoài , đặc biệt là các bạn đến từ Đức , Úc và Nga \"\n",
            "\" we receive many foreign tourists , especially from Germany , Australia and Russia , \" he said .\n",
            "\" we will take a lot of foreign visitors outside the country , especially friends from Germany , Australia and Russia , \" he added .\n",
            "anh cũng cho biết thêm \" Chúng_tôi sẽ đón_tiếp rất nhiều khách du_lịch nước_ngoài , đặc_biệt là các bạn đến từ Đức , Úc và Nga \"\n",
            "he added , \" we will look at a lot of foreign abroad , particularly from Germany , and Russia , \" he added .\n",
            "\" chúng tôi có nhiều khách du lịch từ nước Anh , đặc biệt là Đức , Nga và Nga , \" ông nói .\n",
            "\" chúng_tôi nhận được nhiều khách du_lịch nước_ngoài , đặc_biệt là Đức , Úc và Nga , \" ông nói .\n",
            "\" chúng tôi nhận được nhiều khách du lịch nước ngoài , đặc biệt là Đức , Úc và Nga , \" ông nói .\n",
            "         \n",
            "----------26----------\n",
            "tập 5 và cũng là tập cuối bắt đầu với từ slab , đường bê-tông và kết thúc với từ Zydecom - một thể loại âm nhạc thịnh hành ở Louisiana .\n",
            "the fifth and final volume starts with slab , a concrete road , and ends with zydeco , a kind of music popular in Louisiana .\n",
            "5 and the last episode is the last episode of getting along with sugar , depending on the streets and ends with a variety of music - a type of music in Louisiana .\n",
            "tập 5 và cũng là tập cuối bắt_đầu với từ slab , đường bê-tông và kết_thúc với từ Zydecom - một thể loại âm_nhạc thịnh_hành ở Louisiana .\n",
            "in addition to five and five is also the end of the end of the year to the urinary tract , which is also the end of the urinary tract - and is also the end of a music virus in Louisiana .\n",
            "năm thứ 5 và bắt đầu phát triển các bóng bán dẫn bắt đầu , với một con đực cùng một loạt các bóng bán dẫn , cùng với âm nhạc xuất hiện trên âm nhạc .\n",
            "vị_trí thứ năm và bắt_đầu bằng cách đạt được một con đường dài , một con đường dài , một con đường ngắn và đạt được các dạng nhạc nổi_tiếng , cùng với một loại nhạc phổ_biến phổ_biến phổ_biến phổ_biến ở Louisiana .\n",
            "vị trí thứ năm và bắt đầu bằng cách đạt được một con đường dài , một con đường dài , một con đường ngắn và đạt được các dạng nhạc nổi tiếng , cùng với một loại nhạc phổ biến phổ biến phổ biến phổ biến ở Louisiana .\n",
            "         \n",
            "----------27----------\n",
            "ví dụ , ở một số vùng của nước Mĩ , đồ uống có ga được gọi là \" soda \" nhưng ở nơi khác được gọi là \" pop \" .\n",
            "for example , in some parts of the country , a carbonated drink is a soda ; in others , it 's called pop .\n",
            "for example , in some parts of the U.S. , drinks are called \" soda \" but in other places where the pop is called \" .\n",
            "ví_dụ , ở một_số vùng của nước Mĩ , đồ uống có ga được gọi là \" soda \" nhưng ở nơi khác được gọi là \" pop \" .\n",
            "for instance , in some parts of the US , the drink is called \" bitter \" but elsewhere called \" .\n",
            "chẳng hạn như ở một số quốc gia , một số quốc gia uống trà đóng chai , đây là một trong những người khác .\n",
            "chẳng_hạn như , ở một_số vùng của đất_nước này , thức uống có ga và thức uống trong viên khác ; nó được gọi là Vua nhạc pop .\n",
            "chẳng hạn như , ở một số vùng của đất nước này , thức uống có ga và thức uống trong viên khác ; nó được gọi là Vua nhạc pop .\n",
            "         \n",
            "----------28----------\n",
            "các nhà khoa học cho biết phương pháp định tuổi bằng đồng vị nguyên tố carbon đã xác định các hạt giống có 31.800 năm tuổi với sai số 300 năm .\n",
            "researchers said radiocarbon dating has confirmed the tissue to be 31,800 years old , give or take 300 years .\n",
            "scientists say the age of the current age was accurate by the current current size of the year , which has been identified with the number of five years .\n",
            "các nhà_khoa_học cho biết phương_pháp định tuổi bằng đồng_vị nguyên_tố carbon đã xác_định các hạt_giống có 31.800 năm_tuổi với sai_số 300 năm .\n",
            "scientists say the approach is expected to be able to determine the carbon Academy of Medicine , which is now known to determine whether the year of approximately 300 years old , according to scientists .\n",
            "các nhà nghiên cứu đã công bố kế hoạch sinh nở đã mô tả xét nghiệm siêu âm trong vòng 12 năm , hoặc hơn 300 năm .\n",
            "các nhà_nghiên_cứu cho biết , các nhà_nghiên_cứu đã xác_nhận các mô đã được xác_nhận để khai_thác mô thần_kinh từ năm_tuổi đến 300 năm .\n",
            "các nhà nghiên cứu cho biết , các nhà nghiên cứu đã xác nhận các mô đã được xác nhận để khai thác mô thần kinh từ năm tuổi đến 300 năm .\n",
            "         \n",
            "----------29----------\n",
            "Wendy Baldwin của Viện này cho biết điểm khởi đầu thường được tính là khi người hiện đại bước những bước đầu tiên trên Trái Đất khoảng 50,000 năm về trước .\n",
            "Wendy Baldwin from the Bureau says that the normal starting point is when Homo sapiens first walked the earth , about 50,000 years ago .\n",
            "Norwegian Institute of the Institute said it was often the first time to be the first step of the Earth on Earth by about 2.5 years ago .\n",
            "Wendy_Baldwin của Viện này cho biết điểm khởi_đầu thường được tính là khi người hiện_đại bước những bước đầu_tiên trên Trái_Đất khoảng 50,000 năm về trước .\n",
            "the Institute of the Institute of the Institute of the Institute of the Institute of the Institute said it was often started to be the modern modern modern people first step back on Earth by the Earth .\n",
            "cựu giám đốc Quỹ tiền tệ thế giới nói rằng khi bắt đầu bắt đầu từ phía nam giới bắt đầu trở về phía trước , thì khoảng cách đây khoảng cách đây mười năm .\n",
            "Peter_Petri , thuộc Cục Dự_trữ Liên_bang nói rằng điểm bắt_đầu bình_thường khi người ta đi bộ trên mặt_đất , cách đây khoảng 50.000 năm trước .\n",
            "Peter Petri , thuộc Cục Dự trữ Liên bang nói rằng điểm bắt đầu bình thường khi người ta đi bộ trên mặt đất , cách đây khoảng 50.000 năm trước .\n",
            "         \n",
            "----------30----------\n",
            "ngoài ra , mỗi năm hàng ngàn sinh viên Việt Nam được đi du học ở nước ngoài tại các quốc gia như Hoa Kỳ , Pháp , Đức và Úc .\n",
            "in addition , each year thousands of Vietnamese students studied abroad in countries such as the United States , France , Germany and Australia .\n",
            "in addition , each year the Vietnamese students were visited by foreign students in countries such as the United States , France , France , Germany and Australia .\n",
            "ngoài_ra , mỗi năm hàng ngàn sinh_viên Việt_Nam được đi du_học ở nước_ngoài tại các quốc_gia như Hoa_Kỳ , Pháp , Đức và Úc .\n",
            "in addition , every year of thousands of Vietnamese students traveled to foreign countries in countries such as France , France , Germany , Germany and Australia .\n",
            "ngoài ra , mỗi năm hàng ngàn người Việt Nam đã học sinh ở Anh như Pháp , Pháp , Pháp và Đức .\n",
            "ngoài_ra , mỗi năm hàng ngàn sinh_viên Việt_Nam học ở các nước láng_giềng như Hoa_Kỳ , Pháp , Pháp , Pháp và Úc .\n",
            "ngoài ra , mỗi năm hàng ngàn sinh viên Việt Nam học ở các nước láng giềng như Hoa Kỳ , Pháp , Pháp , Pháp và Úc .\n",
            "         \n",
            "----------31----------\n",
            "phụ nữ cũng cảm nhận nhiệm vụ khó khăn hơn khi bị mất nước nhẹ , mặc dù không có sự giảm đáng kể trong khả năng nhận thức của họ .\n",
            "the female subjects also perceived tasks as more difficult when slightly dehydrated , although there was no substantive reduction in their cognitive abilities .\n",
            "women also feel harder when it is lost , although there is no significant decline in their abilities .\n",
            "phụ_nữ cũng cảm_nhận nhiệm_vụ khó_khăn hơn khi bị mất nước nhẹ , mặc_dù không có sự giảm đáng_kể trong khả_năng nhận_thức của họ .\n",
            "women also feel more difficult when they lose water , although there is no significant decline in their ability .\n",
            "những người phụ nữ cũng nhận ra rằng việc thu nhập ít khó chịu hơn khi bị thất vọng , mặc dù không có khả năng suy giảm mức thu nhập của họ .\n",
            "các vấn_đề ở phụ_nữ cũng cảm_nhận được nhiều công_việc tích_cực hơn khi bị mất nước , mặc_dù không có sự giảm bớt bớt khả_năng nhận_thức về khả_năng nhận_thức của họ .\n",
            "các vấn đề ở phụ nữ cũng cảm nhận được nhiều công việc tích cực hơn khi bị mất nước , mặc dù không có sự giảm bớt bớt khả năng nhận thức về khả năng nhận thức của họ .\n",
            "         \n",
            "----------32----------\n",
            "tình trạng mất nước nhẹ cũng có thể cản trở các hoạt động thường nhật khác , ngay cả khi không có nhu cầu thực hiện các hoạt động thể chất .\n",
            "mild dehydration may also interfere with other daily activities , even when there is no physical demand component present . \"\n",
            "mild dehydration can also interfere with other normal activities , even if there are no need to work on physical activity .\n",
            "tình_trạng mất nước nhẹ cũng có_thể cản_trở các hoạt_động thường_nhật khác , ngay cả khi không có nhu_cầu thực_hiện các hoạt_động thể_chất .\n",
            "mild dehydration can also interfere with other daily activities , even if there is no demand for physical activity .\n",
            "nôn ói cũng có thể làm cho hoạt động hàng ngày , thậm chí khi các hoạt động khác không có nhu cầu trên toàn cầu . \"\n",
            "tình_trạng mất nước nhẹ cũng có_thể làm cản_trở các hoạt_động hàng ngày của mình , thậm_chí khi không có nhu_cầu về thể_chất .\n",
            "tình trạng mất nước nhẹ cũng có thể làm cản trở các hoạt động hàng ngày của mình , thậm chí khi không có nhu cầu về thể chất .\n",
            "         \n",
            "----------33----------\n",
            "ý tưởng này chính là tế bào gan do tế bào gốc tạo ra có thể được sử dụng để chữa trị cho bộ phận này nếu nó bị tổn hại .\n",
            "the idea is that liver cells produced from stem cells could be used to repair the organ if it was damaged .\n",
            "this idea is that the cells stem cell cells can be used to treat this part if it is damaged .\n",
            "ý_tưởng này chính là tế_bào gan do tế_bào gốc tạo ra có_thể được sử_dụng để chữa_trị cho bộ_phận này nếu nó bị tổn_hại .\n",
            "this idea is that the liver cells from stem cells can be used to treat this part if it is damage .\n",
            "ý kiến là tế bào gan được tạo ra từ tế bào miễn dịch có thể được sử dụng rộng rãi nếu nó bị hư hại .\n",
            "ý_tưởng là tế_bào gan tạo ra tế_bào gốc từ tế_bào gốc có_thể được sử_dụng để sửa_chữa nếu cơ_quan này bị hư_hại .\n",
            "ý tưởng là tế bào gan tạo ra tế bào gốc từ tế bào gốc có thể được sử dụng để sửa chữa nếu cơ quan này bị hư hại .\n",
            "         \n",
            "----------34----------\n",
            "các tế bào này cũng có thể di chuyển đến vùng chậu trong khi hành kinh , qua máu , hoặc trong khi phẫu thuật chẳng hạn như mổ bắt con .\n",
            "the cells also might migrate to the pelvic area during menstruation , through the bloodstream , or during surgery such as caesarian delivery .\n",
            "these cells can also move to the area during the economy , through blood , or during surgery such as surgery .\n",
            "các tế_bào này cũng có_thể di_chuyển đến vùng chậu trong khi hành_kinh , qua máu , hoặc trong khi phẫu_thuật chẳng_hạn như mổ bắt con .\n",
            "these cells can also travel to the pelvic area while the blood , blood , or during surgery , such as a cesarean section of a cesarean section .\n",
            "tế bào thần kinh cũng có thể di chuyển qua vùng chậu chậu chậu chậu , trong suốt quá trình sinh nở , chẳng hạn như phẫu thuật hoặc phẫu thuật .\n",
            "các tế_bào cũng có_thể quét qua vùng chậu hông , trong suốt quá_trình truyền máu , hoặc trong suốt quá_trình phẫu_thuật chẳng_hạn như sinh_nở .\n",
            "các tế bào cũng có thể quét qua vùng chậu hông , trong suốt quá trình truyền máu , hoặc trong suốt quá trình phẫu thuật chẳng hạn như sinh nở .\n",
            "         \n",
            "----------35----------\n",
            "không có những dòng tiền béo bở thu về từ việc chơi ở Champions League , câu lạc bộ sẽ phải chứng kiến thu nhập giảm sút trong nửa năm tới .\n",
            "without the lucrative revenue streams garnered from competing in the Champions League , the club is expected to see a decline in income over the next half-year .\n",
            "there are no significant cash flow from the Champions League in the Champions League , where the club will have to fall in a half over the next half .\n",
            "không có những dòng tiền béo_bở thu về từ việc chơi ở Champions_League , câu_lạc_bộ sẽ phải chứng_kiến thu_nhập giảm_sút trong nửa năm tới .\n",
            "there were no lucrative lucrative lucrative cash from playing in the Champions League , which would have to see a fall in the next half of the year .\n",
            "không có lợi nhuận từ các nhà đầu tư có quyền sở hữu sân vận động , MU nhận được số tiền này sẽ tiếp tục giảm xuống mức thu nhập vào năm 2009 .\n",
            "nếu không thu được doanh_thu từ việc sở_hữu câu_lạc_bộ Champions_League , câu_lạc_bộ sẽ phải chứng_kiến sự sụt_giảm thu_nhập trong bảng xếp_hạng năm tới .\n",
            "nếu không thu được doanh thu từ việc sở hữu câu lạc bộ Champions League , câu lạc bộ sẽ phải chứng kiến sự sụt giảm thu nhập trong bảng xếp hạng năm tới .\n",
            "         \n",
            "----------36----------\n",
            "quá trình này khiến chính bạn hoặc bác sĩ mắt và những người khác thấy rõ mạch máu bị sưng và đôi mắt đỏ , ngứa và mọng nước của bạn .\n",
            "the process can lead to swollen ocular surface blood vessels and red , itchy , watery eyes visible to you , your eye doctor , and everyone else .\n",
            "this process allows you to see or other blood vessels and people with swollen blood vessels , swelling and redness .\n",
            "quá_trình này khiến chính bạn hoặc bác_sĩ mắt và những người khác thấy rõ mạch_máu bị sưng và đôi mắt đỏ , ngứa và mọng nước của bạn .\n",
            "this process makes you or your eyes and other people with swelling of blood vessels and redness in the red eyes and redness , and the swelling of your water .\n",
            "quá trình này có thể làm sưng phồng lên các mạch máu và mắt đỏ , mắt đỏ , mắt đỏ , và bác sĩ của bạn .\n",
            "quá_trình này có_thể làm cho bề_mặt bề_mặt da bị sưng phồng lên và sưng đỏ , ngứa , mắt , mắt , và bất_kỳ ai khác .\n",
            "quá trình này có thể làm cho bề mặt bề mặt da bị sưng phồng lên và sưng đỏ , ngứa , mắt , mắt , và bất kỳ ai khác .\n",
            "         \n",
            "----------37----------\n",
            "chở nặng sẽ tiêu tốn thêm nhiên liệu , vì thế hãy mang những đồ nhỏ tới câu lạc bộ golf và bỏ những chiếc hộp ra khỏi xe của mình .\n",
            "extra weight costs fuel , so take those golf clubs and moving boxes out of your car .\n",
            "the burden will cost further fuel , so bring the small toys to the club and leave your car out .\n",
            "chở nặng sẽ tiêu tốn thêm nhiên_liệu , vì_thế hãy mang những đồ nhỏ tới câu_lạc_bộ golf và bỏ những chiếc hộp ra khỏi xe của mình .\n",
            "it is going to spend extra fuel , so bring small vehicles to the club and remove the box from its car .\n",
            "bổ sung thêm giá nhiên liệu , làm cho những chiếc xe tải đi đến cửa hàng tạp hoá và chạy khỏi xe hơi của bạn .\n",
            "giá nhiên_liệu tăng , vì_vậy những chiếc xe chở những chiếc xe chở những chiếc xe và đường đi ra khỏi xe của bạn .\n",
            "giá nhiên liệu tăng , vì vậy những chiếc xe chở những chiếc xe chở những chiếc xe và đường đi ra khỏi xe của bạn .\n",
            "         \n",
            "----------38----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b23235b31a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'         '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcatl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-b23235b31a13>\u001b[0m in \u001b[0;36mcatl\u001b[0;34m(stop, start, length, ps)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'----------{i}----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'         '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_x0cMhGdrK0",
        "outputId": "5e378891-1c9c-4579-f0b3-dc13a170ce58"
      },
      "source": [
        "x = 'when evaluating a company as a possible investment , learn exactly how it makes its money .'\n",
        "print(len(x.split(' ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}